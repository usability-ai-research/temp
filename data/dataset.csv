key,title,author,year,type,volume,issue,pages,journal,journalShort,publisher,doi,url,abstract,keywords,citedBy,quality,area,subArea,approach,AI,AIsub,data,participants,note
akinyelu2022,Convolutional Neural Network-Based Technique for Gaze Estimation on Mobile Devices,"Akinyelu, Andronicus A.; Blignaut, Pieter",2022,journalArticle,4,,,Frontiers in Artificial Intelligence,,Frontiers Media SA,10.3389/frai.2021.796825,https://doi.org/10.3389/frai.2021.796825,"Eye tracking is becoming a very popular, useful, and important technology. Many eye tracking technologies are currently expensive and only available to large corporations. Some of them necessitate explicit personal calibration, which makes them unsuitable for use in real-world or uncontrolled environments. Explicit personal calibration can also be cumbersome and degrades the user experience. To address these issues, this study proposes a Convolutional Neural Network (CNN) based calibration-free technique for improved gaze estimation in unconstrained environments. The proposed technique consists of two components, namely a face component and a 39-point facial landmark component. The face component is used to extract the gaze estimation features from the eyes, while the 39-point facial landmark component is used to encode the shape and location of the eyes (within the face) into the network. Adding this information can make the network learn free-head and eye movements. Another CNN model was designed in this study primarily for the sake of comparison. The CNN model accepts only the face images as input. Different experiments were performed, and the experimental result reveals that the proposed technique outperforms the second model. Fine-tuning was also performed using the VGG16 pre-trained model. Experimental results show that the fine-tuned results of the proposed technique perform better than the fine-tuned results of the second model. Overall, the results show that 39-point facial landmarks can be used to improve the performance of CNN-based gaze estimation models.",computer vision; Convolutional Neural Network; eye tracking; gaze estimation; mobile device,12,Q3,Saliency/visual importance,,Computer vision,deep learning,,Video/facial expressions,yes,gaze estimation; computer vision aproach; images with metadata; model training; estimate gaze patterns; CNN based calibration-free technique for improved gaze estimation in unconstrained environments; mobile
al-sakran2021,Usability and Accessibility Assessment of Saudi Arabia Mobile E-Government Websites,"Al-Sakran, Hasan O.; Alsudairi, Mohammed A.",2021,journalArticle,9,,48254 – 48275,IEEE Access,,IEEE,10.1109/ACCESS.2021.3068917,https://doi.org/10.1109/ACCESS.2021.3068917,"The rapid spread of smart mobile technology is transforming the way how governments provide information and services to their citizens. We all are rely more on our devices, namely smartphones, laptops, or desktop to get information and a wide range of services from government websites. Such heavy usage of government websites results in an increased need for efficient and effective delivery of government services. Therefore, mobile government websites' usability and accessibility are essential dimensions that determine the quality and accessibility of mobile e-government. The main objective of this research is to analyze the accessibility and usability aspects of selected public sector websites in Saudi Arabia. This study investigates how well the Saudi mobile e-government websites comply with usability standards and accessibility guidelines recommended in the Web Content Accessibility Guidelines (WCAG). Websites assessments were conducted using manual evaluation and complemented by different automated analysis tools. This study applies a number of evaluation techniques to assess Saudi government websites accessed from desktop or mobile devices, such as an automated website testing technique, mobile-friendliness testing, and content observation technique. Various tools have been used for site evaluation, such as GTmetrix (PageSpeed Score, YSlow Score), WAVE, Google mobile-friendly test, and Dareboost for mobile websites. The study uncovers shortcomings regarding non-compliance to international web standards recommendations. The findings revealed usability and accessibility problems that affect the performance of government websites. In order to improve these websites within these aspects, several recommendations were suggested for improving the usability and accessibility of websites in Saudi Arabia that will make sure different groups of citizens are satisfied with the website features and services provided by them. © 2013 IEEE.",Accessibility guidelines; Accessibility problems; Automated analysis; e-government; Government services; Government websites; Mobile e governments; Observation techniques; Regulatory compliance; Testing; Usability engineering; Web content accessibility guidelines; Web Design; Websites,37,Q2,UX/usability attributes evaluation,"usability metrics (efficiency,...)",Source code analysis (web/app crawling),none,descriptive metrics/visualizations,Source code,no,automated website qualities evaluation; web crawling data analysis; static web metrics; existing automated website testing tools usage; nan; methodology of Saudi governmental websites evaluation; mobile
aleksander2018,Method for semi-automated evaluation of user experience using brain activity,"Aleksander, B.A.I.; Fuglerud, Kristin S.",2018,journalArticle,256,,811 – 820,Studies in Health Technology and Informatics,,IOS Press,10.3233/978-1-61499-923-2-811,https://doi.org/10.3233/978-1-61499-923-2-811,"There is a large interest in user experience today, both from a usability and accessibility point of view. However, in order to verify what the users actually like and don't like, user testing must be conducted. Traditionally, user experience is measured retrospective with surveys and interviews, but this is not the most optimal approach since it does not measure user experience in the moment and it is prone for human error because of our inaccurate memory recollection. Here we propose a method that does semi-automated evaluation of user experience by utilizing electrophysiological equipment that monitors electrical activity of the brain. We describe an approach that together with brain activity monitoring will collect and quantify user experience in a non-intrusive manner. We demonstrate the method by showing how a low cost device can record brain activity during a user test, and auto-detect where the user has difficulties understand or navigating a solution. All this is done in an unsupervised manner, but an observer must still verify the feedback with the actual user to remove false positives. Our method is not limited to digital solutions and can also be used for evaluating user experience of physical installations. © 2018 The authors and IOS Press.",Usability; User experience; automation; Automation; Surveys; Brain; Electroencephalography; Design; human; Humans; User-Computer Interface; human experiment; quantitative analysis; questionnaire; computer interface; Surveys and Questionnaires; Universal Design; Retrospective Studies; retrospective study; brain function; false positive result; conference paper; monitoring; Neurophysiology; Electrophysiology; brain; EEG; Digital solutions; Electrical activities; electroencephalogram; Low-cost devices; Optimal approaches; Semi-automated; Universal design,6,Q3,UX/usability attributes evaluation,difficulty/demand,Physiological signal analysis,none,descriptive metrics/visualizations,physiological signals,yes,"automated usability testing; brain activity analysis; EEG data, thinkaloud data, questionnaire; lab usability test; nan; semi-automated method for evaluation of user experience vie EEG; desktop"
alshahrani2024,Enhancing Online Food Service User Experience Through Advanced Analytics and Hybrid Deep Learning for Comprehensive Evaluation,"Alshahrani, Hussain; Abdullah Mengash, Hanan; Maashi, Mashael; Kouki, Fadoua; Mahmud, Ahmed; Duhayyim, Mesfer Al",2024,journalArticle,12,,70999-71009,IEEE Access,,IEEE,10.1109/ACCESS.2024.3402100,https://doi.org/10.1109/ACCESS.2024.3402100,"User experience (UX) analysis of Online Food Delivery Services (OFDS) involves features like order placement efficacy, delivery tracking reliability, ease of navigation, menu visibility, and payment process simplicity. By examining these factors, OFDS offers can optimize its platforms to improve user satisfaction, streamline ordering procedures, minimize friction points, and improve customer retention. We can gain valued visions into customer opinions and preferences by connecting sentiment analysis, recommendation systems, feature extractors, and XAI platforms. Then, this information can be employed to develop the superiority of service, personalize UX, and finally develop customer fulfilment and platform victory. This paper presents a Reptile Search Algorithm with a Hybrid DL-based UX Detection (RSAHDL-UXD) approach on OFDSs. The RSAHDL-UXD approach utilizes data preprocessing and a word2vec-based word embedding process. For UX recognition, sliced multi-head self-attention slice recurrent neural network (SMH-SASRNN) methodology is employed. Finally, the hyperparameter tuning procedure was executed using RSA. To validate the upgraded performance of the RSAHDL-UXD methodology, a wide array of models was executed on manifold online food services datasets. The experimental outcomes stated that the RSAHDL-UXD model highlighted the superior accuracy of 98.57% and 93.33% on the Swiggy and Zomato datasets, respectively.",artificial intelligence; Business; deep learning; Deep learning; Food products; Hyperparameter optimization; hyperparameter tuning; Long short term memory; Online food delivery services; Online services; Predictive models; Product delivery; Recurrent neural networks; reptile search algorithm; Search methods; Tuning,0,Q2,UX/usability attributes evaluation,sentiment,Text analysis,deep learning,,User feedback/reviews/text,yes,"user experience evaluation; text analysis approach; tweets; web crawling, model training; classifying user experience from tweet data; neural network that analyses text; -"
amrehn2019,A semi-automated usability evaluation framework for interactive image segmentation systems,"Amrehn, Mario; Steidl, Stefan; Kortekaas, Reinier; Strumia, Maddalena; Weingarten, Markus; Kowarschik, Markus; Maier, Andreas",2019,journalArticle,2019,,,International Journal of Biomedical Imaging,,Wiley,10.1155/2019/1464592,https://doi.org/10.1155/2019/1464592,"For complex segmentation tasks, the achievable accuracy of fully automated systems is inherently limited. Specifically, when a precise segmentation result is desired for a small amount of given data sets, semi-automatic methods exhibit a clear benefit for the user. The optimization of human computer interaction (HCI) is an essential part of interactive image segmentation. Nevertheless, publications introducing novel interactive segmentation systems (ISS) often lack an objective comparison of HCI aspects. It is demonstrated that even when the underlying segmentation algorithm is the same throughout interactive prototypes, their user experience may vary substantially. As a result, users prefer simple interfaces as well as a considerable degree of freedom to control each iterative step of the segmentation. In this article, an objective method for the comparison of ISS is proposed, based on extensive user studies. A summative qualitative content analysis is conducted via abstraction of visual and verbal feedback given by the participants. A direct assessment of the segmentation system is executed by the users via the system usability scale (SUS) and AttrakDiff-2 questionnaires. Furthermore, an approximation of the findings regarding usability aspects in those studies is introduced, conducted solely from the system-measurable user actions during their usage of interactive segmentation prototypes. The prediction of all questionnaire results has an average relative error of 8.9%, which is close to the expected precision of the questionnaire results themselves. This automated evaluation scheme may significantly reduce the resources necessary to investigate each variation of a prototype's user interface (UI) features and segmentation methodologies. © 2019 Mario Amrehn et al.",Article; Automated usability evaluation; automation; Automation; Average relative error; cognitive model; content analysis; Degrees of freedom (mechanics); human; Human computer interaction; Human computer interaction (HCI); image segmentation; Image segmentation; Interactive image segmentation; Interactive segmentation; Iterative methods; prediction; qualitative analysis; questionnaire; Segmentation algorithms; Semiautomatic methods; Surveys; System Usability Scale (SUS); User interfaces,11,Q3,UX/usability attributes evaluation,"usability metrics (efficiency,...)",User interaction analysis,machine learning,,User interactions,yes,"automated usability testing; participant's log data analysis; questionnaire, action logs, task results; lab usability test with data logging; predicting questionnaire data from logs; method for the comparison of interactive segmentation systems's usability; desktop"
asnawi2023,The Combination of Contextualized Topic Model and MPNet for User Feedback Topic Modeling,"Asnawi, Mohammad Hamid; Pravitasari, Anindya Apriliyanti; Herawan, Tutut; Hendrawati, Triyani",2023,journalArticle,11,,130272-130286,IEEE Access,,IEEE,10.1109/ACCESS.2023.3332644,https://doi.org/10.1109/ACCESS.2023.3332644,"In the era of big data and ubiquitous internet connectivity, user feedback data plays a crucial role in product development and improvement. However, extracting valuable insights from the vast pool of unstructured text data found in user feedback presents significant challenges. In this paper, we propose an innovative approach to tackle this challenge by combining the Contextualized Topic Model (CTM) and the Masked and Permuted Pre-training for Language Understanding (MPNet) model. Our approach aims to create a more accurate and context-aware topic model that enhances the understanding of user experiences and opinions. To achieve this, we first search for the optimal number of topics, focusing on generating distinguishable, general, and unique topics. Next, we perform hyperparameter optimization to fine-tune the model and maximize coherence metrics. The result is an exceptionally effective model that outperforms established topic modeling methods, including LSI, NMF, LDA, HDP, NeuralLDA, ProdLDA, ETM, and the default CTM, achieving the highest coherence CV score of 0.7091. In this study, the combination of CTM and MPNet has proven highly effective in the context of user feedback topic modeling. This model excels in generating coherent, distinguishable, and highly relevant user feedback topics, capturing the nuanced nature of user feedback data. The topics generated from this model include ‘Music and Audio Streaming,’ ’Application Performance,’ ‘Banking, Financial Services, and Customer Support,’ ’User Experience,’ ‘Other Topics,’ ’Application Content,’ and ‘Application Features.’ Our contributions include a powerful tool for developers to gain deeper insights, prioritize actions, and enhance user satisfaction by incorporating feedback into future product iterations. Furthermore, we introduce a new dataset as an open-source resource for further exploration and validation of user feedback analysis techniques and general natural language processing applications. With our proposed approach, we strive to drive business success, improve user experiences, and inform data-driven decision-making processes, ultimately benefiting both developers and users alike.",Analytical models; Coherence; Context modeling; Contextualized topic model; Data mining; Data models; Feedback; Measurement; MPNet; natural language processing; Natural language processing; Predictive models; topic model; user feedback,1,Q2,UX/usability attributes evaluation,topic classification,Text analysis,New LMs,LLM,User feedback/reviews/text,yes,"user feedback generation; text analysis; user reviews; web scraping, model training; topic classification from feedback data; extracting valuable insights from the vast pool of unstructured text data found in user feedback - combining the Contextualized Topic Model (CTM) and the Masked and Permuted Pre-training for Language Understanding (MPNet) model; -"
bacikova2021,Domain Usability Evaluation,"Bacikova, Michaela; Poruban, Jaroslav; Sulir, Matus; Chodarev, Sergej; Steingartner, William; Madeja, Matej",2021,journalArticle,10,16,,Electronics,,MDPI,10.3390/electronics10161963,https://doi.org/10.3390/electronics10161963,"Contemporary software systems focus on usability and accessibility from the point of view of effectiveness and ergonomics. However, the correct usage of the domain dictionary and the description of domain relations and properties via their user interfaces are often neglected. We use the term domain usability (DU) to describe the aspects of the user interface related to the terminology and domain. Our experience showed that poor domain usability reduces the memorability and effectiveness of user interfaces. To address this problem, we describe a method called ADUE (Automatic Domain Usability Evaluation) for the automated evaluation of selected DU properties on existing user interfaces. As a prerequisite to the method, metrics for formal evaluation of domain usability, a form stereotype recognition algorithm, and general application terms filtering algorithm have been proposed. We executed ADUE on several real-world Java applications and report our findings. We also provide proposals to modify existing manual usability evaluation techniques for the purpose of domain usability evaluation.",User experience; Graphical user interfaces; user experience; human-computer interaction; graphical user interfaces; Usability evaluation methods; domain usability; domain-specific languages; usability evaluation methods; Domain usability; Domain-specific languages; Human–computer interaction,5,Q3,UX/usability attributes evaluation,domain usability (ontology),Source code analysis (web/app crawling),none,descriptive metrics/visualizations,Source code,no,automated domain (ontology) usability testing; application UI elements crawling; UI elements data; ontology analysis; nan; Automatic Domain Usability Evaluation method for the automated evaluation of selected domain usability properties on UIs; desktop
baek2016,Automated model-based android GUI testing using multi-level GUI comparison criteria,"Baek, Young-Min; Bae, Doo-Hwan",2016,conferencePaper,,,238 – 249,Proceedings of the 31st IEEE/ACM International Conference on Automated Software Engineering,ASE,ACM,10.1145/2970276.2970313,https://doi.org/10.1145/2970276.2970313,"Automated Graphical User Interface (GUI) testing is one of the most widely used techniques to detect faults in mobile applications (apps) and to test functionality and usability. GUI testing exercises behaviors of an application under test (AUT) by executing events on GUIs and checking whether the app behaves correctly. In particular, because Android leads in market share of mobile OS platforms, a lot of research on automated Android GUI testing techniques has been performed. Among various techniques, we focus on model-based Android GUI testing that utilizes a GUI model for systematic test generation and effective debugging support. Since test inputs are generated based on the underlying model, accurate GUI modeling of an AUT is the most crucial factor in order to generate effective test inputs. However, most modern Android apps contain a number of dynamically constructed GUIs that make accurate behavior modeling more challenging. To address this problem, we propose a set of multi-level GUI Comparison Criteria (GUICC) that provides the selection of multiple abstraction levels for GUI model generation. By using multilevel GUICC, we conducted empirical experiments to identify the inuence of GUICC on testing effectiveness. Results show that our approach, which performs model-based testing with multi-level GUICC, achieved higher effectiveness than activity-based GUI model generation. We also found that multi-level GUICC can alleviate the inherent state explosion problems of existing a single-level GUICC for behavior modeling of real-world Android apps by exibly manipulating GUICC. © 2016 ACM.",Graphical user interfaces; GUI testing; Testing; User interfaces; Software engineering; Automation; Embedded systems; Android (operating system); Model checking; Competition; Model-based test; Graphical user interfaces (GUI); Testing effectiveness; Android applications; Application under tests; Comparison criterion; State explosion problems; Android application testing; GUI comparison criteria; GUI model generation; Model-based test input generation,193,A,UI issue detection,,Source code analysis (web/app crawling),none,test execution,Source code,no,automated GUI testing; application UI elements crawling; extracted GUI information; model-based GUI testing; nan; multi-level GUI Comparison Criteria - model-based testing framework; mobile
bakiu2017,Which feature is unusable? Detecting usability and user experience issues from user reviews,"Bakiu, Elsa; Guzman, Emitza",2017,conferencePaper,,,182 – 187,"Proceedings - 2017 IEEE 25th International Requirements Engineering Conference Workshops, REW 2017",REW,IEEE,10.1109/REW.2017.76,https://doi.org/10.1109/REW.2017.76,"Usability and user experience (UUX) strongly affect software quality and success. User reviews allow software users to report UUX issues. However, this information can be difficult to access due to the varying quality of the reviews, its large numbers and unstructured nature. In this work we propose an approach to automatically detect the UUX strengths and issues of software features according to user reviews. We use a collocation algorithm for extracting the features, lexical sentiment analysis for uncovering users' satisfaction about a particular feature and machine learning for detecting the specific UUX issues affecting the software application. Additionally, we present two visualizations of the results. An initial evaluation of the approach against human judgement obtained mixed results. © 2017 IEEE.",Application programs; Computer software selection and evaluation; Data mining; Feature extraction; Learning systems; Natural language processing systems; Requirements engineering; Software Evolution; Text mining; Usability; User experience; User feedback,64,,UX/usability issue detection,topic classification,Text analysis,"NLP, machine learning",,User feedback/reviews/text,yes,automated usability issue detection; text analysis (NTLK and sentiment); user reviews; web crawling; classification of found issues into categories; approach to automatically detect the UUX strengths and issues of software features according to user reviews; -
batch2024,uxSense: Supporting User Experience Analysis with Visualization and Computer Vision,"Batch, Andrea; Ji, Yipeng; Fan, Mingming; Zhao, Jian; Elmqvist, Niklas",2024,journalArticle,30,7,3841-3856,IEEE Transactions on Visualization and Computer Graphics,,IEEE,10.1109/TVCG.2023.3241581,https://doi.org/10.1109/TVCG.2023.3241581,"Analyzing user behavior from usability evaluation can be a challenging and time-consuming task, especially as the number of participants and the scale and complexity of the evaluation grows. We propose uxSense, a visual analytics system using machine learning methods to extract user behavior from audio and video recordings as parallel time-stamped data streams. Our implementation draws on pattern recognition, computer vision, natural language processing, and machine learning to extract user sentiment, actions, posture, spoken words, and other features from such recordings. These streams are visualized as parallel timelines in a web-based front-end, enabling the researcher to search, filter, and annotate data across time and space. We present the results of a user study involving professional UX researchers evaluating user data using uxSense. In fact, we used uxSense itself to evaluate their sessions.",Annotations; computer vision; Data visualization; deep learning; evaluation; Feature extraction; Gaze tracking; machine learning; Measurement; Usability; video analytics; visual analytics; Visual analytics; Visualization,8,Q1,UX/usability attributes evaluation,sentiment,"User interaction analysis, Text analysis, Audio analysis, Computer vision","NLP, machine learning",,"User interactions, Audio/speech, Video/facial expressions",yes,"automated usabilty testing; computer vision, signal processing, text analysis, participant's log data analysis; video, audio recordings, user interactions, task results; usability test; nan; visual analytics system using machine learning methods to extract user behavior from audio and video recordings; desktop"
bisante2024,Enhancing Interface Design with AI: An Exploratory Study on a ChatGPT-4-Based Tool for Cognitive Walkthrough Inspired Evaluations,"Bisante, Alba; Datla, Venkata Srikanth Varma; Panizzi, Emanuele; Trasciatti, Gabriella; Zeppieri, Stefano",2024,conferencePaper,,,,Proceedings of the 2024 International Conference on Advanced Visual Interfaces,AVI,ACM,10.1145/3656650.3656676,https://doi.org/10.1145/3656650.3656676,"This paper introduces CWGPT, a ChatGPT-4-based tool designed for Cognitive Walkthrough (CW) inspired evaluations of web interfaces. The primary goal is to assist users, particularly students and inexperienced designers, in evaluating web interfaces. Our tool, operating as a conversational agent, provides detailed evaluations of a user-specified task by intelligently guessing the subtasks and actions required to accomplish them, answering the standard CW questions, and providing helpful feedback and practical suggestions to improve the usability of the analyzed interface. For our study, we selected a group of web applications designed by students from a Web and Software Architecture course. We compare the outcome of the CWs we executed on ten web apps against the corresponding CWGPT analyses. We then describe the study we conducted involving five author-students to assess the tool's efficacy in helping them recognize and solve usability issues. In addition to introducing a novel adaptation of ChatGPT, the outcomes of the described experience underscore the promising potential of AI in usability evaluations. © 2024 Owner/Author.",Application programs; ChatGPT; Cognitive walkthrough; Conversational agents; Exploratory studies; GPT; Interface designs; Students; Subtask; Usability engineering; WEB application; Web applications; Web interface,0,B,UI understanding/feedback,,Computer vision,New LMs,LLM,Screenshots/images,no,"assisted cognitive walkthrough; computer vision; UI screenshot; chatbot usage; UI screenshot assessment; conversational agent, provides detailed evaluations of a user-specified task by intelligently guessing the subtasks and actions required to accomplish them, answering the standard CW questions, and providing helpful feedback and practical suggestions to improve the usability of the analyzed interface.; desktop"
blanco-gonzalo2014,Automatic usability and stress analysis in mobile biometrics,"Blanco-Gonzalo, Ramon; Sanchez-Reillo, Raul; Miguel-Hurtado, Oscar; Bella-Pulgarin, Eric",2014,journalArticle,32,12,1173 – 1180,Image and Vision Computing,,Elsevier,10.1016/j.imavis.2014.09.003,https://doi.org/10.1016/j.imavis.2014.09.003,"This article focuses on the usability evaluation of biometric recognition systems in mobile devices. In particular, a behavioural modality has been used: the dynamic handwritten signature. Testing usability in behavioural modalities involves a big challenge due to the number of degrees of freedom that users have in interacting with sensors, as well as the variety of capture devices to be used. In this context we propose a usability evaluation that allows users to interact freely with the system while minimizing errors at the same time. The participants signed in a smartphone with a stylus through the different phases in the use of a biometric system: training, enrolment and verification. In addition, a profound study on the automation of the evaluation processes has been done, so as to reduce the resources employed. The influence of the users' stress has also been studied, to obtain conclusions on its impact on both the usability systems in scenarios where the user may suffer a certain level of stress, such as in courts, banks or even shopping. In brief, the results shown in this paper prove not only that a dynamic handwritten signature is a trustable solution for a large number of applications in the real world, but also that the evaluation of the usability of biometric systems can be carried out at lower costs and shorter duration. © 2014 Elsevier B.V.",Biometric recognition system; Biometric systems; Biometrics; Character recognition; Degrees of freedom (mechanics); Handwritten signatures; Mobile biometrics; Mobile computing; Number of degrees of freedom; Real-world; Stress analysis; Stresses; Usability; Usability engineering; Usability evaluation; Well testing,24,Q1,Emotion detection,,User interaction analysis,none,descriptive metrics/visualizations,User interactions,yes,"usability evaluation; signature processing, participant's log data analysis; user interactions, task results; usability test; nan; usability evaluation that allows users to interact freely with the system while minimizing errors at the same time; mobile"
bures2020,Testing the Usability and Accessibility of Smart TV Applications Using an Automated Model-Based Approach,"Bures, Miroslav; MacIk, Miroslav; Ahmed, Bestoun S.; Rechtberger, Vaclav; Slavik, Pavel",2020,journalArticle,66,2,134 – 143,IEEE Transactions on Consumer Electronics,,IEEE,10.1109/TCE.2020.2986049,https://doi.org/10.1109/TCE.2020.2986049,"As the popularity of Smart Televisions (TVs) and interactive Smart TV applications (apps) has recently grown, the usability of these apps has become an important quality characteristic. Previous studies examined Smart TV apps from a usability perspective. However, these methods are mainly manual, and the potential of automated model-based testing methods for usability testing purposes has not yet been fully explored. In this article, we propose an approach to test the usability of Smart TV apps based on the automated generation of a Smart TV user interaction model from an existing app by a specialized automated crawler. By means of this model, defined user tasks in the Smart TV app can be evaluated automatically in terms of their feasibility and estimated user effort, which reflects the usability of the analyzed app. This analysis can be applied in the context of regular users and users with various specific needs. The findings from this model-based automated analysis approach can be used to optimize the user interface of a Smart TV app to increase its usability, accessibility, and quality. © 1975-2011 IEEE.",Usability; Task analysis; Testing; User interfaces; Computational modeling; Usability engineering; Automation; Analytical models; Crawlers; model-based testing; Smart TV; smart TV application; Usability testing; user interface quality; Model-based OPC; Quality control; Model checking; Automated generation; Automated analysis; Automated modeling; Interactive television; Quality characteristic; Smart-TV; User Interaction Modeling,20,Q2,UX/usability attributes evaluation,user effort/time,"Source code analysis (web/app crawling), User interaction analysis",none,metrics + rule-based,"Source code, User interactions",yes,"model-based usability evaluation; participant's log data analysis, application UI crawling; user interactions, UI data; usability test; nan; approach to test the usability of Smart TV apps based on the automated generation of a Smart TV user interaction model from an existing app by a specialized automated crawler; TV"
bylinskii2017,Learning Visual Importance for Graphic Designs and Data Visualizations,"Bylinskii, Zoya; Kim, Nam Wook; O'Donovan, Peter; Alsheikh, Sami; Madan, Spandan; Pfister, Hanspeter; Durand, Fredo; Russell, Bryan; Hertzmann, Aaron",2017,conferencePaper,,,57–69,Proceedings of the 30th Annual ACM Symposium on User Interface Software and Technology,UIST,ACM,10.1145/3126594.3126653,https://doi.org/10.1145/3126594.3126653,"Knowing where people look and click on visual designs can provide clues about how the designs are perceived, and where the most important or relevant content lies. The most important content of a visual design can be used for effective summarization or to facilitate retrieval from a database. We present automated models that predict the relative importance of different elements in data visualizations and graphic designs. Our models are neural networks trained on human clicks and importance annotations on hundreds of designs. We collected a new dataset of crowdsourced importance, and analyzed the predictions of our models with respect to ground truth importance and human eye movements. We demonstrate how such predictions of importance can be used for automatic design retargeting and thumbnailing. User studies with hundreds of MTurk participants validate that, with limited post-processing, our importance-driven applications are on par with, or outperform, current state-of-the-art methods, including natural image saliency. We also provide a demonstration of how our importance predictions can be built into interactive design tools to offer immediate feedback during the design process.",Saliency; Computer Vision; Machine Learning; Eye Tracking; Visualization; Graphic Design; Deep Learning; Retargeting,173,A,Saliency/visual importance,,Computer vision,deep learning,,Screenshots/images,no,visual importance; computer vision; UI screenshot; model training; estimate importatnt elements; automated models that predict the relative importance of different elements in data visualizations and graphic designs; -
cassino2015,Empirical validation of an automatic usability evaluation method,"Cassino, Rosanna; Tucci, Maurizio; Vitiello, Giuliana; Francese, Rita",2015,journalArticle,28,,1 – 22,Journal of Visual Languages and Computing,,Elsevier,10.1016/j.jvlc.2014.12.002,https://doi.org/10.1016/j.jvlc.2014.12.002,"Today, the success of a software application strongly depends on the usability of its interface, so the evaluation of interfaces has become a crucial aspect of software engineering. It is recognized that automatic tools for graphical user interface evaluation may greatly reduce the costs of traditional activities performed during expert evaluation or user testing in order to estimate the success probability of an application. However, automatic methods need to be empirically validated in order to prove their effectiveness with respect to the attributes they are supposed to evaluate.In this work, we empirically validate a usability evaluation method conceived to assess consistency aspects of a GUI with no need to analyze the back-end. We demonstrate the validity of the approach by means of a comparative experimental study, where four web sites and a stand-alone interactive application are analyzed and the results compared to those of a human-based usability evaluation. The analysis of the results and the statistical correlation between the tool's rating and humans' average ratings show that the proposed methodology can indeed be a useful complement to standard techniques of usability evaluation. © 2014 Elsevier Ltd.",Interactive systems evaluation; Summative usability evaluation; Usability engineering,30,Q3,UX/usability issue detection,guidelines evaluation,Source code analysis (web/app crawling),none,metrics + rule-based,Source code,no,automated website qualities assessment; web crawling data analysis; UI elements; existing automated website testing tools usage; nan; validate a usability evaluation method conceived to assess consistency aspects of a GUI with no need to analyze the back-end; desktop
chen2021,A Novel User Emotional Interaction Design Model Using Long and Short-Term Memory Networks and Deep Learning,"Chen, Xiang; Huang, Rubing; Li, Xin; Xiao, Lei; Zhou, Ming; Zhang, Linghao",2021,journalArticle,12,,,Frontiers in Psychology,,Frontiers Media SA,10.3389/fpsyg.2021.674853,https://doi.org/10.3389/fpsyg.2021.674853,"Emotional design is an important development trend of interaction design. Emotional design in products plays a key role in enhancing user experience and inducing user emotional resonance. In recent years, based on the user's emotional experience, the design concept of strengthening product emotional design has become a new direction for most designers to improve their design thinking. In the emotional interaction design, the machine needs to capture the user's key information in real time, recognize the user's emotional state, and use a variety of clues to finally determine the appropriate user model. Based on this background, this research uses a deep learning mechanism for more accurate and effective emotion recognition, thereby optimizing the design of the interactive system and improving the user experience. First of all, this research discusses how to use user characteristics such as speech, facial expression, video, heartbeat, etc., to make machines more accurately recognize human emotions. Through the analysis of various characteristics, the speech is selected as the experimental material. Second, a speech-based emotion recognition method is proposed. The mel-Frequency cepstral coefficient (MFCC) of the speech signal is used as the input of the improved long and short-term memory network (ILSTM). To ensure the integrity of the information and the accuracy of the output at the next moment, ILSTM makes peephole connections in the forget gate and input gate of LSTM, and adds the unit state as input data to the threshold layer. The emotional features obtained by ILSTM are input into the attention layer, and the self-attention mechanism is used to calculate the weight of each frame of speech signal. The speech features with higher weights are used to distinguish different emotions and complete the emotion recognition of the speech signal. Experiments on the EMO-DB and CASIA datasets verify the effectiveness of the model for emotion recognition. Finally, the feasibility of emotional interaction system design is discussed. © Copyright © 2021 Chen, Huang, Li, Xiao, Zhou and Zhang.",emotion recognition; interaction design; LSTM; self-attention mechanism; speech,9,Q1,Emotion detection,,Audio analysis,deep learning,,Audio/speech,yes,"emotion recognition; speech data analysis; speech data; model training; speech-based emotion recognition; deep learning mechanism for more accurate and effective emotion recognition, thereby optimizing the design of the interactive system and improving the user experience; -"
cheng2023,Visual saliency model based on crowdsourcing eye tracking data and its application in visual design,"Cheng, Shiwei; Fan, Jing; Hu, Yilin",2023,journalArticle,27,3,613-630,Personal and Ubiquitous Computing,,Springer Nature,10.1007/s00779-020-01463-7,https://doi.org/10.1007/s00779-020-01463-7,"The visual saliency models based on low-level features of an image have the problem of low accuracy and scalability, while the visual saliency models based on deep neural networks can effectively improve the prediction performance, but require a large amount of training data, e.g., eye tracking data, to achieve good results. However, the traditional eye tracking method is limited by high equipment and time cost, complex operation process, low user experience, etc. Therefore, this paper proposed a visual saliency model based on crowdsourcing eye tracking data, which was collected by gaze recall with self-reporting from crowd workers. Parameter optimization on our crowdsourcing method was explored, and it came out that the accuracy of gaze data reached 1° of visual angle, which was 3.6% higher than other existed crowdsourcing methods. On this basis, we collected a webpage dataset of crowdsourcing gaze data and constructed a visual saliency model based on a fully convolutional neural network (FCN). The evaluation results showed that after trained by crowdsourcing gaze data, the model performed better, such as prediction accuracy increased by 44.8%. Also, our model outperformed the existing visual saliency models. We also applied our model to help webpage designers evaluate and revise their visual designs, and the experimental results showed that the revised design obtained improved ratings by 8.2% compared to the initial design.",Visual saliency model; Visual attention; Crowd computing; Eye tracking; Human-computer interaction,10,Q2,Saliency/visual importance,,Computer vision,deep learning,,Screenshots/images,no,visual saliency estimation; computer vision; UI screenshot; model training; predicting visual saliency from website screenshots; visual saliency model based on crowdsourcing eye tracking dat; desktop
cruzgardey2020,User Experience Evaluation through Automatic A/B Testing,"Cruz Gardey, Juan; Garrido, Alejandra",2020,conferencePaper,,,25-26,Companion Proceedings of the 25th International Conference on Intelligent User Interfaces,IUI,ACM,10.1145/3379336.3381514,https://doi.org/10.1145/3379336.3381514,"The goal of this research is to develop an A/B testing method to automatically compare the user experience (UX) of alternative designs for a web application in a real context with a large number of users. The challenge that it poses is to find mechanisms to predict the UX with machine learning techniques. This submission outlines the motivation, research goal, current status and remaining work.",User experience; Machine learning; User interfaces; User Experience; Machine Learning; Learning systems; A/B testing; User Interaction; WEB application; Machine learning techniques; Alternative designs; User experience evaluations; User interaction; Research goals; User experiences (ux); A/b testing,18,A,UX/usability attributes evaluation,user effort/time,User interaction analysis,machine learning,,User interactions,yes,automated A/B testing; participant's log data analysis; user interactions; real use data logging; effort prediction from user interactions; A/B testing method to automatically compare the user experience (UX) of alternative designs for a web application; not mentioned
dahri2019,Usability evaluation of mobile health application from AI perspective in rural areas of Pakistan,"Dahri, Abdul Samad; Al-Athwari, Ahmaed; Hussain, Azham",2019,journalArticle,13,11,213 – 225,International Journal of Interactive Mobile Technologies,,International Association of Online Engineering,10.3991/ijim.v13i11.11513,https://doi.org/10.3991/ijim.v13i11.11513,"Purpose: This study endorses AI enabled Mobile Health application and investigates usability evaluation of the Mobile Health application by patients' task performance evaluation and satisfaction. Materials and Methods: International Organization for Standards (ISO) 9241-11 standard metrics were used and 15 patients performed tasks on task success rate, errors, efficiency (time spent), satisfaction (SUS scale). Results: Getting registered was a top easy task while finding a relevant doctor was the most difficult task for users. The satisfaction scores by SUS suggest good rather excellent application user experience. Male were successful task achievers, while educational level and mobile know-how influence the usability scores in terms of time consumed, task errors occurred, and task completed. Conclusion: Methods used in this study suggest future research from different contexts. Using ISO 9241-11 usability standards, the SUS instrument for satisfaction, and measuring user characteristics influence performance and can provide considerable Mobile Health design. © International Association of Online Engineering.",Artificial intelligence; Developing countries; Healthcare; Mobile health (mHealth); Usability,17,Q3,UX/usability attributes evaluation,"usability metrics (efficiency,...)",User interaction analysis,none,descriptive metrics/visualizations,User interactions,yes,metrics for UX evaluation (satisfaction); participant's log data analysis; user interactions; usability test; nan; usability evaluation of the Mobile Health application by patients’ task performance evaluation and satisfaction; mobile
desolda2021,Detecting Emotions Through Machine Learning for Automatic UX Evaluation,"Desolda, Giuseppe; Esposito, Andrea; Lanzilotti, Rosa; Costabile, Maria F.",2021,conferencePaper,12934,,270-279,Human-Computer Interaction–INTERACT 2021: 18th IFIP TC 13 International Conference,INTERACT,Springer Nature,10.1007/978-3-030-85613-7_19,https://doi.org/10.1007/978-3-030-85613-7_19,"Although User eXperience (UX) is widely acknowledged as an important aspect of software products, its evaluation is often neglected during the development of most software products, primarily because developers think that it is resource-demanding and complain about the fact that is scarcely automated. Various attempts have been made to develop tools that support and automate the execution of tests with users. This paper is about an ongoing research work that exploits Machine Learning (ML) for automatic UX evaluation, specifically for understanding users' emotions by analyzing the log data of the users' interactions with websites. The approach described aims at overcoming some limitations of existing proposals based on ML.",Automatic UX evaluation; Usability; User eXperience; User experience; Machine learning; Human computer interaction; Software products; ITS evaluation; User experiences (ux); Log data,12,B,Emotion detection,,User interaction analysis,machine learning,,User interactions,yes,emotion detection; participant's log data analysis; user interactions; real use data logging; emotion detection from interaction data; understanding users’ emotions by analyzing the log data of the users’ interactions with websites; desktop
dingli2014,An Intelligent Framework for Website Usability,"Dingli, Alexiei; Cassar, Sarah",2014,journalArticle,2014,,,Advances in Human-Computer Interaction,,Wiley,10.1155/2014/479286,https://doi.org/10.1155/2014/479286,"With the major advances of the Internet throughout the past couple of years, websites have come to play a central role in the modern marketing business program. However, simply owning a website is not enough for a business to prosper on the Web. Indeed, it is the level of usability of a website that determines if a user stays or abandons it for another competing one. It is therefore crucial to understand the importance of usability on the web, and consequently the need for its evaluation. Nonetheless, there exist a number of obstacles preventing software organizations from successfully applying sound website usability evaluation strategies in practice. From this point of view automation of the latter is extremely beneficial, which not only assists designers in creating more usable websites, but also enhances the Internet users' experience on the Web and increases their level of satisfaction. As a means of addressing this problem, an Intelligent Usability Evaluation (IUE) tool is proposed that automates the usability evaluation process by employing a Heuristic Evaluation technique in an intelligent manner through the adoption of several research-based AI methods. Experimental results show there exists a high correlation between the tool and human annotators when identifying the considered usability violations.",,65,Q3,UX/usability issue detection,guidelines evaluation,Source code analysis (web/app crawling),machine learning,,Source code,no,"usability violations detection - guidelines evaluation; application UI elements crawling; UI elements data; web scraping; nan;  main purpose of the IUE tool is to assist website designers, internet users at large indirectly benefit from its consequences as websites will eventually be created in a more usable manner thus enhancing their online experience; desktop"
dou2019,Webthetics: Quantifying webpage aesthetics with deep learning,"Dou, Qi; Zheng, Xianjun Sam; Sun, Tongfang; Heng, Pheng-Ann",2019,journalArticle,124,,56-66,International Journal of Human-Computer Studies,,Elsevier,10.1016/j.ijhcs.2018.11.006,https://doi.org/10.1016/j.ijhcs.2018.11.006,"As web has become the most popular media to attract users and customers worldwide, webpage aesthetics plays an increasingly important role for engaging users online and impacting their user experience. We present a novel method using deep learning to automatically compute and quantify webpage aesthetics. Our deep neural network, named as Webthetics, which is trained from the collected user rating data, can extract representative features from raw webpages and quantify their aesthetics. To improve the model performance, we propose to transfer the knowledge from image style recognition task into our network. We have validated that our method significantly outperforms previous method using hand-crafted features such as colorfulness and complexity. These promising results indicate that our method can serve as an effective and efficient means for providing objective aesthetics evaluation during the design process.",Webpage aesthetics; Deep learning; Web visual design; User experience,65,Q1,UX/usability attributes evaluation,Visual aesthetics,Computer vision,deep learning,,Screenshots/images,no,"aesthetics prediction; computer vision; UI screenshots; using existing dataset, deep learning model training; aesthetics evaluation from screenshots; using deep learning to automatically compute and quantify webpage aesthetics; desktop"
duan2023,Towards Generating UI Design Feedback with LLMs,"Duan, Peitong; Warner, Jeremy; Hartmann, Bjoern",2023,conferencePaper,,,,Adjunct Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology,UIST,ACM,10.1145/3586182.3615810,https://doi.org/10.1145/3586182.3615810,"Feedback on user interface (UI) mockups is crucial for the design process, and designers often seek and leverage feedback to improve their UIs. However, human feedback is not always readily available. Given the recent emergence of LLMs, which have been shown to be proficient in rule-based reasoning, we explore the potential of LLMs to provide feedback automatically. In particular, we investigate automating heuristic evaluation, which currently entails a human expert assessing how well a UI adheres to a given set of design guidelines. We build an LLM-based heuristic evaluation plugin for Figma, which designers can use to evaluate their UI mockups. The plugin queries the LLM with the guidelines and a JSON representation of the UI mockup and then renders the identified guideline violations as constructive suggestions for design improvements. Future work is needed to study what types of usability problems can be successfully identified by LLM-driven heuristic evaluation. © 2023 Owner/Author.",User interfaces; Heuristic evaluation; Mockups; Design-process; Product design; Petroleum reservoir evaluation; User interface designs; Human expert; Usability problems; Design improvements; Plug-ins; Computational user interface design; Design feedbacks; Rule-based reasoning; Computational UI Design; Design Feedback,1,A,UI understanding/feedback,,Source code analysis (web/app crawling),New LMs,LLM,Source code,no,"UI evaluation; LLM feedback; JSON UI representation; none; guidelines evaluation accorting to heuristics, from UI data; potential of LLMs to provide feedback automatically - in figma prototypes (mockups); desktop"
duan2020,Optimizing User Interface Layouts via Gradient Descent,"Duan, Peitong; Wierzynski, Casimir; Nachman, Lama",2020,conferencePaper,,,1–12,Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems,CHI,ACM,10.1145/3313831.3376589,https://doi.org/10.1145/3313831.3376589,"Automating parts of the user interface (UI) design process has been a longstanding challenge. We present an automated technique for optimizing the layouts of mobile UIs. Our method uses gradient descent on a neural network model of task performance with respect to the model's inputs to make layout modifications that result in improved predicted error rates and task completion times. We start by extending prior work on neural network based performance prediction to 2-dimensional mobile UIs with an expanded interaction space. We then apply our method to two UIs, including one that the model had not been trained on, to discover layout alternatives with significantly improved predicted performance. Finally, we confirm these predictions experimentally, showing improvements up to 9.2 percent in the optimized layouts. This demonstrates the algorithm's efficacy in improving the task performance of a layout, and its ability to generalize and improve layouts of new interfaces.",Optimization; data-driven design; gradient descent; deep learning; mobile interfaces; LSTM; performance modeling;,21,A,UX/usability attributes evaluation,user effort/time,User interaction analysis,deep learning,,User interactions,yes,task performance prediction (ux evaluation); participant's log data analysis; user interactions; data logging (usaibility test); task performance prediction from user interactions; gradient descent on a neural network model of task performance with respect to the model’s inputs to make layout modifications that result in improved predicted error rates and task completion times; mobile
eiband2019,When people and algorithms meet: user-reported problems in intelligent everyday applications,"Eiband, Malin; Völkel, Sarah Theres; Buschek, Daniel; Cook, Sophia; Hussmann, Heinrich",2019,conferencePaper,,,96–106,Proceedings of the 24th International Conference on Intelligent User Interfaces,IUI,ACM,10.1145/3301275.3302262,https://doi.org/10.1145/3301275.3302262,"The complex nature of intelligent systems motivates work on supporting users during interaction, for example through explanations. However, there is yet little empirical evidence on specific problems users face in such systems in everyday use. This paper investigates such problems as reported by users: We analysed 35,448 reviews of three apps on the Google Play Store (Facebook, Netflix and Google Maps) with sentiment analysis and topic modelling to reveal problems during interaction that can be attributed to the apps' algorithmic decision-making. We enriched this data with users' coping and support strategies through a follow-up online survey (N=286). In particular, we found problems and strategies related to content, algorithm, user choice, and feedback. We discuss corresponding implications for designing user support, highlighting the importance of user control and explanations of output, not processes. Our work thus contributes empirical evidence to facilitate understanding of users' everyday problems with intelligent systems.",algorithm; explanations; transparency; user control,70,A,UX/usability issue detection,topic classification,Text analysis,NLP,,User feedback/reviews/text,yes,"automated usability problem detection; text analysis; user reviews; web scraping; nan; We analysed 35,448 reviews of three apps on the Google Play Store (Facebook, Netflix and Google Maps) with sentiment analysis and topic modelling to reveal problems during interaction that can be attributed to the apps’ algorithmic decision-making.; -"
eiband2020,A Method and Analysis to Elicit User-Reported Problems in Intelligent Everyday Applications,"Eiband, Malin; Völkel, Sarah Theres; Buschek, Daniel; Cook, Sophia; Hussmann, Heinrich",2020,journalArticle,10,4,,ACM Transactions on Interactive Intelligent Systems,,ACM,10.1145/3370927,https://doi.org/10.1145/3370927,"The complex nature of intelligent systems motivates work on supporting users during interaction, for example, through explanations. However, as of yet, there is little empirical evidence in regard to specific problems users face when applying such systems in everyday situations. This article contributes a novel method and analysis to investigate such problems as reported by users: We analysed 45,448 reviews of four apps on the Google Play Store (Facebook, Netflix, Google Maps, and Google Assistant) with sentiment analysis and topic modelling to reveal problems during interaction that can be attributed to the apps’ algorithmic decision-making. We enriched this data with users’ coping and support strategies through a follow-up online survey (N = 286). In particular, we found problems and strategies related to content, algorithm, user choice, and feedback. We discuss corresponding implications for designing user support, highlighting the importance of user control and explanations of output rather than processes.",Algorithm; explanations; transparency; user control,5,Q3,UX/usability issue detection,topic classification,Text analysis,NLP,,User feedback/reviews/text,yes,"automated usability problem detection; text analysis; user reviews; web scraping; nan; We analysed 35,448 reviews of three apps on the Google Play Store (Facebook, Netflix and Google Maps) with sentiment analysis and topic modelling to reveal problems during interaction that can be attributed to the apps’ algorithmic decision-making.; -"
eskonen2020,Automating GUI testing with image-based deep reinforcement learning,"Eskonen, Juha; Kahles, Julen; Reijonen, Joel",2020,conferencePaper,,,160 – 167,"Proceedings - 2020 IEEE International Conference on Autonomic Computing and Self-Organizing Systems, ACSOS 2020",ACSOS,IEEE,10.1109/ACSOS49614.2020.00038,https://doi.org/10.1109/ACSOS49614.2020.00038,"Users interact with modern applications and devices through graphical user interfaces (GUIs). To ensure intuitive and easy usability, the GUIs need to be tested, where developers aim at finding possible bugs and inconsistent functionality. Manual GUI testing requires time and effort, and thus, its efficiency can be improved with automation. Conventional automation tools for GUI testing reduce the burden of manual testing but also introduce challenges in the maintenance of test cases. In order to overcome these issues, we propose a deep-reinforcement-learning-based (DRL) solution for automated and adaptive GUI testing. Specifically, we propose and evaluate the performance of an image-based DRL solution. We adapt the asynchronous advantage actor-critic (A3C) algorithm to GUI testing inspired by how a human uses a GUI. We feed screenshots of the GUI as the input and let the algorithm decide how to interact with GUI components. We observe that our solution can achieve up to six times higher exploration efficiency compared to selected baseline algorithms. Moreover, our solution is more efficient than inexperienced human users and almost as efficient as an experienced human user in our experimental GUI testing scenario. For these reasons, image-based DRL exploration can be considered as a viable GUI testing method. © 2020 IEEE.",Actor critic; Adaptive GUI; Automation; Automation tools; Deep learning; Efficiency; Graphical user interface (GUIs); Graphical user interfaces; Human users; Its efficiencies; Manual testing; Modern applications; Reinforcement learning; Testing,33,,UI issue detection,,Computer vision,deep learning,,Screenshots/images,no,"automated GUI testing; computer vision; UI screenshot; model training; GUI testing using NN; we propose a deep-reinforcementlearning-based (DRL) solution for automated and adaptive GUI testing. Specifically, we propose and evaluate the performance of an image-based DRL solution; desktop"
esposito2024,The fine line between automation and augmentation in website usability evaluation,"Esposito, Andrea; Desolda, Giuseppe; Lanzilotti, Rosa",2024,journalArticle,14,1,,Scientific Reports,,Springer Nature,10.1038/s41598-024-59616-0,https://doi.org/10.1038/s41598-024-59616-0,"Artificial Intelligence (AI) systems are becoming widespread in all aspects of society, bringing benefits to the whole economy. There is a growing understanding of the potential benefits and risks of this type of technology. While the benefits are more efficient decision processes and industrial productivity, the risks may include a potential progressive disengagement of human beings in crucial aspects of decision-making. In this respect, a new perspective is emerging that aims at reconsidering the centrality of human beings while reaping the benefits of AI systems to augment rather than replace professional skills: Human-Centred AI (HCAI) is a novel framework that posits that high levels of human control do not contradict high levels of computer automation. In this paper, we investigate the two antipodes, automation vs augmentation, in the context of website usability evaluation. Specifically, we have analyzed whether the level of automation provided by a tool for semi-automatic usability evaluation can support evaluators in identifying usability problems. Three different visualizations, each one corresponding to a different level of automation, ranging from a full-automation approach to an augmentation approach, were compared in an experimental study. We found that a fully automated approach could help evaluators detect a significant number of medium and high-severity usability problems, which are the most critical in a software system; however, it also emerged that it was possible to detect more low-severity usability problems using one of the augmented approaches proposed in this paper. © The Author(s) 2024.",artificial intelligence; Artificial Intelligence; automation; Automation; human; Humans; Internet; software; Software; User-Computer Interface,1,Q1,Emotion detection,,User interaction analysis,machine learning,,User interactions,yes,"emotion detection; participant's log data analysis; user interactions; existing automated website testing tools usage, usability test; detecting emotions from user interactions; we investigate the two antipodes, automation vs augmentation, in the context of website usability evaluation; desktop"
esposito2022,SERENE: a Web platform for the UX semi-automatic evaluation of website,"Esposito, Andrea; Desolda, Giuseppe; Lanzilotti, Rosa; Costabile, Maria Francesca",2022,conferencePaper,,,,Proceedings of the 2022 International Conference on Advanced Visual Interfaces,AVI,ACM,10.1145/3531073.3534464,https://doi.org/10.1145/3531073.3534464,"This demo presents SERENE, a Web platform for the UX semi-automatic evaluation of websites. It exploits Artificial Intelligence to predict visitors' emotions starting from their interaction logs. The predicted emotions are shown by interactive heatmaps overlapped to the webpage to be analyzed. The concentration of negative emotions in a specific area of the webpage can help the UX experts identify UX problems. © 2022 Owner/Author.",Automatic UX evaluation; User eXperience; Automatic evaluation; Users' experiences; Websites; Semi-automatics; Heatmaps; Web-page; Evaluation of websites; Specific areas; UX smell; automatic UX evaluation; UX Smells,3,B,Emotion detection,,User interaction analysis,machine learning,,User interactions,yes,"emotion detection; participant's log data analysis; user interactions; none; detecting emotions from user interactions; SERENE, a Web platform for the UX semiautomatic evaluation of websites. It exploits Artificial Intelligence to predict visitors’ emotions starting from their interaction logs.; desktop"
fan2020,Automatic Detection of Usability Problem Encounters in Think-aloud Sessions,"Fan, Mingming; Li, Yue; Truong, Khai N.",2020,journalArticle,10,2,,ACM Transactions on Interactive Intelligent Systems,,ACM,10.1145/3385732,https://doi.org/10.1145/3385732,"Think-aloud protocols are a highly valued usability testing method for identifying usability problems. Despite the value of conducting think-aloud usability test sessions, analyzing think-aloud sessions is often time-consuming and labor-intensive. Consequently, previous research has urged the community to develop techniques to support fast-paced analysis. In this work, we took the first step to design and evaluate machine learning (ML) models to automatically detect usability problem encounters based on users' verbalization and speech features in think-aloud sessions. Inspired by recent research that shows subtle patterns in users' verbalizations and speech features tend to occur when they encounter problems, we examined whether these patterns can be utilized to improve the automatic detection of usability problems. We first conducted and recorded think-aloud sessions and then examined the effect of different input features, ML models, test products, and users on usability problem encounters detection. Our work uncovers several technical and user interface design challenges and sets a baseline for automating usability problem detection and integrating such automation into UX practitioners' workflow. © 2020 ACM.",machine learning; Testing; AI-assisted UX analysis method; speech features; Think aloud; usability problem; user experience (UX); verbalization; User interfaces; Feature extraction; Usability engineering; Speech recognition; User interface designs; Usability problems; Automatic Detection; Labor intensive; Think-aloud protocol; Recent researches; Speech features; Usability testing methods,26,Q3,UX/usability issue detection,audio/visual usability problem encounter,"Audio analysis, Text analysis","NLP, deep learning, machine learning",,Audio/speech,yes,automated usability problem encounters detection; speech data analysis; speech data; lab usabiilty test; usability problem encounter detection from speech data; machine learning (ML) models to automatically detect usability problem encounters based on users’ verbalization and speech features in think-aloud sessions; not mentioned
fan2020a,VisTA: Integrating Machine Intelligence with Visualization to Support the Investigation of Think-Aloud Sessions,"Fan, Mingming; Wu, Ke; Zhao, Jian; Li, Yue; Wei, Winter; Truong, Khai N.",2020,journalArticle,26,1,343 – 352,IEEE Transactions on Visualization and Computer Graphics,,IEEE,10.1109/TVCG.2019.2934797,https://doi.org/10.1109/TVCG.2019.2934797,"Think-aloud protocols are widely used by user experience (UX) practitioners in usability testing to uncover issues in user interface design. It is often arduous to analyze large amounts of recorded think-aloud sessions and few UX practitioners have an opportunity to get a second perspective during their analysis due to time and resource constraints. Inspired by the recent research that shows subtle verbalization and speech patterns tend to occur when users encounter usability problems, we take the first step to design and evaluate an intelligent visual analytics tool that leverages such patterns to identify usability problem encounters and present them to UX practitioners to assist their analysis. We first conducted and recorded think-aloud sessions, and then extracted textual and acoustic features from the recordings and trained machine learning (ML) models to detect problem encounters. Next, we iteratively designed and developed a visual analytics tool, VisTA, which enables dynamic investigation of think-aloud sessions with a timeline visualization of ML predictions and input features. We conducted a between-subjects laboratory study to compare three conditions, i.e., VisTA, VisTASimple (no visualization of the ML's input features), and Baseline (no ML information at all), with 30 UX professionals. The findings show that UX professionals identified more problem encounters when using VisTA than Baseline by leveraging the problem visualization as an overview, anticipations, and anchors as well as the feature visualization as a means to understand what ML considers and omits. Our findings also provide insights into how they treated ML, dealt with (dis)agreement with ML, and reviewed the videos (i.e., play, pause, and rewind). © 2020 IEEE.",machine learning; Usability; Machine learning; Think aloud; User interfaces; Visualization; prediction; Feature extraction; user study; Usability engineering; Tools; Artificial intelligence; artificial intelligence; adult; female; human; article; human experiment; male; User study; videorecording; Usability problems; Visual analytics; Machine intelligence; usability problems; visual analytics; machine intelligence; session review behavior; Think-aloud; UX practices; anticipation,26,Q1,UX/usability issue detection,audio/visual usability problem encounter,"Audio analysis, Text analysis","NLP, deep learning, machine learning",,Audio/speech,yes,automated usability problem encounters detection; speech data analysis; speech data; lab usabiilty test; usability problem encounter detection from speech data; design and evaluate an intelligent visual analytics tool that leverages subtle verbalization and speech patterns to identify usability problem encounters and present them to UX practitioners to assist their analysis; desktop
fan2022,Human-AI Collaboration for UX Evaluation: Effects of Explanation and Synchronization,"Fan, Mingming; Yang, Xianyou; Yu, TszTung; Liao, Q. Vera; Zhao, Jian",2022,journalArticle,6,CSCW1,,Proceedings of the ACM on Human-Computer Interaction,,ACM,10.1145/3512943,https://doi.org/10.1145/3512943,"Analyzing usability test videos is arduous. Although recent research showed the promise of AI in assisting with such tasks, it remains largely unknown how AI should be designed to facilitate effective collaboration between user experience (UX) evaluators and AI. Inspired by the concepts of agency and work context in human and AI collaboration literature, we studied two corresponding design factors for AI-assisted UX evaluation: explanations and synchronization. Explanations allow AI to further inform humans how it identifies UX problems from a usability test session; synchronization refers to the two ways humans and AI collaborate: synchronously and asynchronously. We iteratively designed a tool-AI Assistant-with four versions of UIs corresponding to the two levels of explanations (with/without) and synchronization (sync/async). By adopting a hybrid wizard-of-oz approach to simulating an AI with reasonable performance, we conducted a mixed-method study with 24 UX evaluators identifying UX problems from usability test videos using AI Assistant. Our quantitative and qualitative results show that AI with explanations, regardless of being presented synchronously or asynchronously, provided better support for UX evaluators' analysis and was perceived more positively; when without explanations, synchronous AI better improved UX evaluators' performance and engagement compared to the asynchronous AI. Lastly, we present the design implications for AI-assisted UX evaluation and facilitating more effective human-AI collaboration.",Think aloud; user experience (UX); User interfaces; Usability engineering; Explainable AI; Synchronization; Collaboration; explanation; explainable AI; Iterative methods; Users' experiences; User interface designs; Usability tests; Explanation; User experience (UX); AI-assisted UX evaluation; Intelligent user interface design; Intelligent User Interfaces; Think-aloud usability test; collaboration; intelligent user interface (UI) design; synchronization; think-aloud usability test,38,,UX/usability issue detection,audio/visual usability problem encounter,"Audio analysis, User interaction analysis",machine learning,,"Audio/speech, User interactions",yes,automated usability problem encounters detection; speech data analysis; speech data; lab usabiilty test; AI annotations; AI Assistant for Usability Test Video Analysis; desktop
feijofilho2016,"Affective-Ready, Contextual and Automated Usability Test for Mobile Software","Feijo Filho, Jackson; Prata, Wilson; Oliveira, Juan",2016,conferencePaper,,,638-644,Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services Adjunct,MOBILEHCI,ACM,10.1145/2957265.2961834,https://doi.org/10.1145/2957265.2961834,"This work proposes the use of system to perform affective-ready, contextual and automated usability tests for mobile software. Our proposal augments the traditional methods of software usability evaluation by monitoring users' location, weather conditions, moving/stationary status, data connection availability and spontaneous facial expressions automatically. This aims to identify the moment of negative and positive events. Identifying those situations and systematically associating them to the context of interaction, assisted software creators to overcome design flaws and enhancing interfaces' strengths. The validation of our approach include post-test questionnaires with test subjects. The results indicate that the automated user-context logging can be a substantial supplement to mobile software usability tests.",Usability; Testing; Mobile Software; affective computing; usability; Usability engineering; Automation; Software testing; Surveys; Human computer interaction; Mobile devices; Computer software; Usability tests; Mobile softwares; Affective Computing; Facial Expressions; Data connection; Software usabilities; User context; mobile software,6,B,Emotion detection,,"User interaction analysis, Computer vision",machine learning,,"User movements, Video/facial expressions, User movements",yes,"emotion detection; participant's log data analysis, computer vision; user interactions, face expressions, user location; data logging; nan; augments the   traditional methods of software usability evaluation by   monitoring users’ location, weather conditions, moving/stationary status, data connection availability  and spontaneous facial expressions automaticall; mobile"
ferre2017,Extending Mobile App Analytics for Usability Test Logging,"Ferre, Xavier; Villalba, Elena; Julio, Hector; Zhu, Hongming",2017,conferencePaper,10515,III,114-131,Human-Computer Interaction–INTERACT 2017: 16th IFIP TC 13 International Conference,INTERACT,Springer Nature,10.1007/978-3-319-67687-6_9,https://doi.org/10.1007/978-3-319-67687-6_9,"Mobile application development is characterized by reduced development cycles and high time-to-market pressure. Usability evaluation in mobile applications calls for the application of cost-effective methods, specially adapted to such constraints. We propose extending the Google Analytics for Mobile Applications basic service to store specific low-level user actions of interest for usability evaluation purposes. The solution can serve both for lab usability testing, automating quantitative data gathering, and for logging real use after application release. It is based on identification of relevant user tasks and the detailed events worth gathering, instrumentation of specific code for data gathering, and subsequent data extraction for calculating relevant usability- related variables. We validated our application in a real usability test by comparing the automatically gathered data with the information gathered by the human observer. Results shows both measurements are statistically exchangeable, opening promising new ways to perform usability testing cost-effectively and at greater scale.",Usability engineering; Mobile applications; Human computer interaction; Mobile computing; Usability testing; Automated usability evaluation; Usability evaluation; Cost effectiveness; Mobile application development; Mobile telecommunication systems; Cost-effective methods; Log-file analysis; Related variables; Log file analysis; Usability evaluation of mobile applications,41,A,UX/usability attributes evaluation,user effort/time,User interaction analysis,none,descriptive metrics/visualizations,User interactions,yes,automated usability testing; participant's log data analysis; user interactions; data logging; nan; Google Analytics for Mobile Applications basic service to store specific low-level user actions of interest for usability evaluation purposes; mobile
filho2015,Automated Usability Tests for Mobile Devices through Live Emotions Logging,"Filho, Jackson Feijó; Valle, Thiago; Prata, Wilson",2015,conferencePaper,,,636–643,Proceedings of the 17th International Conference on Human-Computer Interaction with Mobile Devices and Services Adjunct,MOBILEHCI,ACM,10.1145/2786567.2792902,https://doi.org/10.1145/2786567.2792902,"Usability tests for mobile phones software present several consistent obstacles. The issue of observing users' interaction within a controlled and supervised environment versus unsupervised field tests is a significant one. Additionally, the burden of frequently time-consuming and expensive usability tests for small software development teams are evident. This work proposes the use of a system to implement emotions logging in automated usability tests for mobile phones. We do this by efficiently, easily and cost-effectively assessing the users' affective state by evaluating their expressive reactions during a mobile software usability evaluation process. These reactions are collected using the front camera on mobile devices.",Usability; Testing; Emotion Feedback; Mobile Software; Usability engineering; Software testing; Human computer interaction; Mobile devices; Computer software; Software design; Usability tests; Field test; Mobile softwares; Cellular telephone systems; Cellular telephones; Mobile phones; Software development teams; Telephone sets; Users' affective state,17,B,Emotion detection,,"User interaction analysis, Computer vision",machine learning,,"User interactions, Video/facial expressions",yes,"emotion detection; participant's log data analysis; user interactions, face expressions; data logging; nan; use of a system to implement emotions logging in automated usability tests for mobile phones; mobile"
fosco2020,Predicting Visual Importance Across Graphic Design Types,"Fosco, Camilo; Casser, Vincent; Bedi, Amish Kumar; O'Donovan, Peter; Hertzmann, Aaron; Bylinskii, Zoya",2020,conferencePaper,,,249–260,Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology,UIST,ACM,10.1145/3379337.3415825,https://doi.org/10.1145/3379337.3415825,"This paper introduces a Unified Model of Saliency and Importance (UMSI), which learns to predict visual importance in input graphic designs, and saliency in natural images, along with a new dataset and applications. Previous methods for predicting saliency or visual importance are trained individually on specialized datasets, making them limited in application and leading to poor generalization on novel image classes, while requiring a user to know which model to apply to which input. UMSI is a deep learning-based model simultaneously trained on images from different design classes, including posters, infographics, mobile UIs, as well as natural images, and includes an automatic classification module to classify the input. This allows the model to work more effectively without requiring a user to label the input. We also introduce Imp1k, a new dataset of designs annotated with importance information. We demonstrate two new design interfaces that use importance prediction, including a tool for adjusting the relative importance of design elements, and a tool for reflowing designs to new aspect ratios while preserving visual importance.",Graphic Designs; Importance; Saliency; Human Attention; Automated Design; User Interface for Design; Deep Learning,50,A,Saliency/visual importance,,Computer vision,deep learning,,Screenshots/images,no,"visual importance; computer vision; UI screenshot; deep learning mdel training; saliency heatmap generation; deep learning-based model simultaneously trained on images from different design classes, including posters, infographics, mobile UIs, as well as natural images, and includes an automatic classification module to classify the input; mobile, desktop"
gardey2022,Predicting interaction effort in web interface widgets,"Gardey, Juan Cruz; Grigera, Julián; Rodríguez, Andrés; Rossi, Gustavo; Garrido, Alejandra",2022,journalArticle,168,,102919,International Journal of Human-Computer Studies,,Elsevier,10.1016/j.ijhcs.2022.102919,https://doi.org/10.1016/j.ijhcs.2022.102919,"The product of good design should render a tool invisible for a user who is executing a task. Unfortunately, web applications are often far from invisible to users, who struggle with poor design of websites and processes in them. We are particularly interested in web processes that involve form filling, so we have been studying how people interact with web forms. Besides cataloguing user interaction problems that are common in web forms, we have noticed that, in many cases, there is a single form element or widget to blame for a certain interaction problem, because such widget is not the most appropriate one for the required input in that particular context. This unfitness of the widget causes an extra burden to the user, which we call interaction effort. In this work we propose measuring the interaction effort of a widget with a unified score based on micro-measures automatically captured from interaction logs. We present the micro-measures that were found relevant to predict the interaction effort in 6 different types of web forms widgets. We describe a large data collection process and prediction models, showing that it is indeed possible to automatically predict a widget interaction effort score by learning from expert human ratings. We consequently believe that the interaction effort could be used as an effective metric to compare small variations in a design in terms of user experience.",Interactivity; User interaction metrics; User experience; Web usability; UX refactoring,10,Q1,UX/usability attributes evaluation,user effort/time,User interaction analysis,machine learning,,User interactions,yes,interaction effort detection; participant's log data analysis; user interactions; usability test; nan; measuring the interaction effort of a widget with a unified score based on micro-measures automatically captured from interaction logs; desktop
georges2016,UX Heatmaps: Mapping User Experience on Visual Interfaces,"Georges, Vanessa; Courtemanche, François; Senecal, Sylvain; Baccino, Thierry; Fredette, Marc; Leger, Pierre-Majorique",2016,conferencePaper,,,4850–4860,Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems,CHI,ACM,10.1145/2858036.2858271,https://doi.org/10.1145/2858036.2858271,"In this paper, we present an off-the-shelf UX evaluation tool which contextualizes users' physiological and behavioral signals while interacting with a system. The proposed tool triangulates users' gaze data with inferred users' cognitive and emotional states to produce user experience (UX) heatmaps, which show where users were looking when they experienced specific cognitive and emotional states. Results show that for a given cognitive state (i.e., cognitive load), the proposed UX heatmap was able to effectively highlight the areas where users experienced different levels of cognitive load on an interface. The proposed tool enables the visual analysis of users' various emotional and cognitive states for specific areas on a given interface, and also to compare users' states across multiple interfaces, which should be useful for both UX researchers and practitioners.",User experience; interface design; heatmaps; eye tracking; physiological computing; cognitive load; affective computing,38,A,Emotion detection,,"Eyetracking, Computer vision, Physiological signal analysis",machine learning,,"Video/facial expressions, gaze data, physiological signals",yes,"cognitive load detection; gaze analysis, expressions data analysis; gaze data, face expressions; usability test; nan; UX evaluation tool which contextualizes users’ physiological and behavioral signals while interacting with a system, cognitive load heatmaps generation from gaze and expressions; desktop"
giroux2021,Guidelines for Collecting Automatic Facial Expression Detection Data Synchronized with a Dynamic Stimulus in Remote Moderated User Tests,"Giroux, Felix; Leger, Pierre-Majorique; Brieugne, David; Courtemanche, Francois; Bouvier, Frederique; Chen, Shang-Lin; Tazi, Salima; Rucco, Emma; Fredette, Marc; Coursaris, Constantinos; Senecal, Sylvain",2021,conferencePaper,12762,,243-254,"Human-Computer Interaction. Theory, Methods and Tools: Thematic Area, HCI 2021",HCII,Springer Nature,10.1007/978-3-030-78462-1_18,https://doi.org/10.1007/978-3-030-78462-1_18,"Because of the COVID-19 pandemic, telework policies have required many user experience (UX) labs to restrict their research activities to remote user testing. Automatic Facial Expression Analysis (AFEA) is an accessible psychophysiological measurement that can be easily implemented in remote user tests. However, to date, the literature on Human Computer Interaction (HCI) has provided no guidelines for remote moderated user tests that collect facial expression data and synchronize them with the state of a dynamic stimulus such as a webpage. To address this research gap, this article offers guidelines for effective AFEA data collection that are based on a methodology developed in a concrete research context and on the lessons learned from applying it in four remote moderated user testing projects. Since researchers have less control over test environment settings, we maintain that they should pay greater attention to factors that can affect face detection and \textbackslashor emotion classification prior, during, and after remote moderated user tests. Our study contributes to the development of methods for including psychophysiological and neurophysiological measurements in remote user tests that offer promising opportunities for information systems (IS) research, UX design, and even digital health research.",Human-computer interaction; User experience; Human computer interaction (HCI); Data acquisition; Human computer interaction; Face recognition; User experiences (ux); Research activities; Automatic facial expression analysis; NeurolS; Psychophysiological data; Remote user test; Concrete research; Emotion classification; Facial expression data; Facial expression detections,15,B,Emotion detection,,Computer vision,deep learning,,Video/facial expressions,yes,"emotion detection from facial expressions; facial expression data analysis, eye-tracking; face expressions; remote moderated user testing; nan; guidelines for remote moderated user tests that collect facial expression data and synchronize them with the state of a dynamic stimulus such as a webpage; desktop, mobile"
gonçalves2016,Supporting adaptation of web applications to the mobile environment with automated usability evaluation,"Gonçalves, Luiz F.; Vasconcelos, Leandro G.; Munson, Ethan V.; Baldochi, Laércio A.",2016,conferencePaper,,,787–794,Proceedings of the ACM Symposium on Applied Computing,SAC,ACM,10.1145/2851613.2851863,https://doi.org/10.1145/2851613.2851863,"The year 2014 marked an important shift in theWeb's history, as users started to spend more time surfing the web using mobile devices than using desktop computers. However, today, a large proportion of websites are still not designed for good mobile device access. To tackle this problem, this paper proposes a new approach to adapting desktop-based web applications to the mobile environment. Our approach is supported by a task-based usability evaluation tool called MOBILICS, which is able to evaluate desktop-based web applications used in mobile devices. Based on the evaluation results, our tool provides detailed recommendations for fixing the detected usability problems. We conducted an experiment which demonstrates that our approach provides useful support for the adaptation of a desktop-based web application into a mobile version.",Usability engineering; usability evaluation; Automated usability evaluation; Usability evaluation; Mobile devices; WEB application; Websites; Evaluation results; Usability problems; Personal computers; Desktop-to-mobile adaptation; Mobile environments; Touch interaction; desktop-to-mobile adaptation; touch interaction,7,B,UX/usability issue detection,user interaction usability problem encounter,User interaction analysis,none,metrics + rule-based,User interactions,yes,"automated usability problem detection; participant's log data analysis; user interactions, task results; existing automated website testing tools usage; nan; approach to adapting desktop-based web applications to the mobile environment. Our approach is supported by a task-based usability evaluation tool called MOBILICS, which is able to evaluate desktop-based web applications used in mobile devices; desktop, mobile"
grano2018,Exploring the integration of user feedback in automated testing of Android applications,"Grano, Giovanni; Ciurumelea, Adelina; Panichella, Sebastiano; Palomba, Fabio; Gall, Harald C.",2018,conferencePaper,2018-March,,72 – 83,"25th IEEE International Conference on Software Analysis, Evolution and Reengineering, SANER 2018 - Proceedings",SANER,IEEE,10.1109/SANER.2018.8330198,https://doi.org/10.1109/SANER.2018.8330198,"The intense competition characterizing mobile application's marketplaces forces developers to create and maintain high-quality mobile apps in order to ensure their commercial success and acquire new users. This motivated the research community to propose solutions that automate the testing process of mobile apps. However, the main problem of current testing tools is that they generate redundant and random inputs that are insufficient to properly simulate the human behavior, thus leaving feature and crash bugs undetected until they are encountered by users. To cope with this problem, we conjecture that information available in user reviews - that previous work showed as effective for maintenance and evolution problems - can be successfully exploited to identify the main issues users experience while using mobile applications, e.g., GUI problems and crashes. In this paper we provide initial insights into this direction, investigating (i) what type of user feedback can be actually exploited for testing purposes, (ii) how complementary user feedback and automated testing tools are, when detecting crash bugs or errors and (iii) whether an automated system able to monitor crash-related information reported in user feedback is sufficiently accurate. Results of our study, involving 11,296 reviews of 8 mobile applications, show that user feedback can be exploited to provide contextual details about errors or exceptions detected by automated testing tools. Moreover, they also help detecting bugs that would remain uncovered when rely on testing tools only. Finally, the accuracy of the proposed automated monitoring system demonstrates the feasibility of our vision, i.e., integrate user feedback into testing process. © 2018 IEEE.",Android (operating system); Android applications; Application programs; Automated monitoring systems; Automated software testing; Automated systems; Automated testing tools; Automation; Behavioral research; Computer aided software engineering; Integration testing; Mobile applications; Mobile computing; Monitoring; Program debugging; Reengineering; Research communities; User reviews,67,A,UX/usability issue detection,topic classification,Text analysis,machine learning,,User feedback/reviews/text,yes,"automated usability problem detection; text analysis; user reviews, reported issues, stack traces; web scraping; review topic clasification; information available in user reviews—that previous work showed as effective for maintenance and evolution problems—can be successfully exploited to identify the main issues users experience while using mobile application; mobile"
grigera2017,Kobold: Web usability as a service,"Grigera, Julian; Garrido, Alejandra; Rossi, Gustavo",2017,conferencePaper,,,990 – 995,2017 32nd IEEE/ACM International Conference on Automated Software Engineering,ASE,IEEE,10.1109/ASE.2017.8115717,https://doi.org/10.1109/ASE.2017.8115717,"While Web applications have become pervasive in today's business, social interaction and information exchange, their usability is often deficient, even being a key factor for a website success. Usability problems repeat across websites, and many of them have been catalogued, but usability evaluation and repair still remains expensive. There are efforts from both the academy and industry to automate usability testing or to provide automatic statistics, but they rarely offer concrete solutions. These solutions appear as guidelines or patterns that developers can follow manually. This paper presents Kobold, a tool that detects usability problems from real user interaction (UI) events and repairs them automatically when possible, at least suggesting concrete solutions. By using the refactoring technique and its associated concept of bad smell, Kobold mines UI events to detect usability smells and applies usability refactorings on the client to correct them. The purpose of Kobold is to deliver usability advice and solutions as a service (SaaS) for developers, allowing them to respond to feedback of the real use of their applications and improve usability incrementally, even when there are no usability experts on the team. Kobold is available at: http://autorefactoring.lifia.info.unlp.edu.ar. A screencast is available at https://youtu.be/c-myYPMUh0Q. © 2017 IEEE.",Usability; Usability engineering; Software engineering; Tools; Automation; Servers; Business; Usability testing; Usability evaluation; Concrete; Web Design; Concretes; HTTP; Websites; Usability problems; User interaction; Web usability; Information exchanges; Refactorings; Software as a service (SaaS); Concrete testing; Social interactions; Software as a service; Software as a Service; Usability Refactoring; Web Usability,23,A,UX/usability issue detection,user interaction usability problem encounter,"Source code analysis (web/app crawling), User interaction analysis",none,metrics + rule-based,"Source code, User interactions",yes,"automated usability problem detection and fixing; participant's log data analysis, DOM analysis; user interactions, DOM; none; nan; tool that detects usability problems from real user interaction (UI) events and repairs them automatically when possible, at least suggesting concrete solution; not mentioned"
grigera2014,A tool for detecting bad usability smells in an automatic way,"Grigera, Julián; Garrido, Alejandra; Rivero, José Matías",2014,conferencePaper,8541,,490 – 493,Web Engineering: 14th International Conferenc,ICWE,Springer Nature,10.1007/978-3-319-08245-5_34,https://doi.org/10.1007/978-3-319-08245-5_34,"The refactoring technique helps developers to improve not only source code quality, but also other aspects like usability. The problems refactoring helps to solve in the specific field of web usability are considered to be issues that make common tasks complicated for end users. Finding such problems, known in the jargon as bad smells, is often challenging for developers, especially for those who do not have experience in usability. In an attempt to leverage this task, we introduce a tool that automatically finds bad usability smells in web applications. Since bad smells are catalogued in the literature together with their suggested refactorings, it is easier for developers to find appropriate solutions. © Springer International Publishing Switzerland 2014.",Bad smells; End users; Odors; Refactorings; Social networking (online); Source code qualities; Usability engineering; WEB application; Web usability,30,B,UX/usability issue detection,user interaction usability problem encounter,"Source code analysis (web/app crawling), User interaction analysis",none,metrics + rule-based,"Source code, User interactions",yes,"automated usability smell detection; participant's log data analysis; user interactions, user reports, DOM; none; nan; tool that automatically finds bad usability smells in web application; desktop"
grigera2017a,Automatic detection of usability smells in web applications,"Grigera, Julián; Garrido, Alejandra; Rivero, José Matías; Rossi, Gustavo",2017,journalArticle,97,,129 – 148,International Journal of Human-Computer Studies,,Elsevier,10.1016/j.ijhcs.2016.09.009,https://doi.org/10.1016/j.ijhcs.2016.09.009,"Usability assessment of web applications continues to be an expensive and often neglected practice. While large companies are able to spare resources for studying and improving usability in their products, smaller businesses often divert theirs in other aspects. To help these cases, researches have devised automatic approaches for user interaction analysis, and there are commercial services that offer automated usability statistics at relatively low fees. However, most existing approaches still fall short in specifying the usability problems concretely enough to identify and suggest solutions. In this work we describe usability smells of user interaction, i.e., hints of usability problems on running web applications, and the process in which they can be identified by analyzing user interaction events. We also describe USF, the tool that implements the process in a fully automated way with minimum setup effort. USF analyses user interaction events on-the-fly, discovers usability smells and reports them together with a concrete solution in terms of a usability refactoring, providing usability advice for deployed web applications. © 2016 Elsevier Ltd",Usability engineering; Usability testing; Concretes; Usability assessment; Automatic Detection; Refactorings; Automatic approaches; Web based; Odors; World Wide Web; Refactoring; Log analysis; Web-based interaction; Commercial services,108,Q2,UX/usability issue detection,user interaction usability problem encounter,User interaction analysis,none,metrics + rule-based,User interactions,yes,"automated usability smell detection; participant's log data analysis; user interactions; none; nan; we describe usability smells of user interaction, i.e., hints of usability problems on running web applications, and the process in which they can be identified by analyzing user interaction events; desktop"
gupta2023,AI-augmented usability evaluation framework for software requirements specification in cyber physical human systems,"Gupta, Sandeep; Epiphaniou, Gregory; Maple, Carsten",2023,journalArticle,23,,,Internet of Things,,Elsevier,10.1016/j.iot.2023.100841,https://doi.org/10.1016/j.iot.2023.100841,"A number of research in Information and Communication Technology (ICT) have shown that usability is an important goal for cyber-physical human systems for wider acceptance by their end-users. To evaluate the usability of a system under design, usability evaluation methods predominantly rely on subject matter experts or testers' assessments. However, integrating Artificial Intelligence (AI) technology and existing usability evaluation methods' processes can aid to evolve more effective user-centred designs. In this paper, we conceptualize an AI-augmented usability evaluation framework (AIUEF) that aims at replacing ‘end-users’ with ‘personas’ to evaluate requirements involving human-computer interaction (HCI) for a given Software Requirements Specification (SRS). We present a blueprint of AIUEF establishing a theoretical basis for an automated usability evaluation of HCI requirements for a given SRS with an aim to improve the usability of a system under design. © 2023 Elsevier B.V.",Artificial intelligence; Cyber physical human systems; Human-computer interaction; Software requirements specification; Usability evaluation,4,Q1,UX/usability attributes evaluation,"usability metrics (efficiency,...)",Synthetic participants,"NLP, deep learning",,none,no (synthetic),automated software requirement evaluation; persona generation; software requirements; none; generating personas to evaluate requirements; AI-augmented usability evaluation framework (AIUEF) that aims at replacing ‘end-users’ with ‘personas’; -
harms2019,Automated Usability Evaluation of Virtual Reality Applications,"Harms, Patrick",2019,journalArticle,26,3,,ACM Transactions on Computer-Human Interaction,,ACM,10.1145/3301423,https://doi.org/10.1145/3301423,"Virtual reality (VR) and VR applications have reached the end-user and, hence, the demands on usability, also for novel applications, have increased. This situation requires VR usability evaluation methods that can be applied quickly, even after a first release of an application. In this article, we describe such an approach, which is fully automated and does not ask users to perform predefined tasks in a fixed test setting. Instead, it works on recordings of the actual usage of a VR application from which it generates task trees. Afterwards, it analyzes these task trees to search for usability smells, i.e., user behavior indicating usability issues. Our approach provides detailed descriptions of the usability issues that have been found and how they can be solved. We performed a large case study to evaluate our approach and show that it is capable of correctly identifying usability issues. Although our approach is applicable for different VR interaction modalities, such as gaze, controller, or hand interaction, it also has limitations. For example, it can detect diverse issues related to user efficiency, but specific misunderstandings of users cannot be uncovered.",Usability engineering; Automation; Virtual reality; Forestry; virtual reality; Automated usability evaluation; task tree generation; usability smell detection; Behavioral research; Novel applications; User behaviors; Fully automated; Task tree; Usability evaluation methods; User efficiencies; VR applications,47,Q2,UX/usability issue detection,user interaction usability problem encounter,User interaction analysis,none,behavior patterns,User interactions,yes,"usasbilty issue identification; task tree generation (from events) and looking for usability smells in them; user interactions; usabiltiy test; nan; fully automated and does not ask users to perform predefined tasks in a fixed test setting. Instead, it works on recordings of the actual usage of a VR application from which it generates task trees; VR"
hasanmansur2023,AidUI: Toward Automated Recognition of Dark Patterns in User Interfaces,"Hasan Mansur, S M; Salma, Sabiha; Awofisayo, Damilola; Moran, Kevin",2023,conferencePaper,,,1958-1970,2023 IEEE/ACM 45th International Conference on Software Engineering,ICSE,IEEE,10.1109/ICSE48619.2023.00166,https://doi.org/10.1109/ICSE48619.2023.00166,"Past studies have illustrated the prevalence of UI dark patterns, or user interfaces that can lead end-users toward (unknowingly) taking actions that they may not have intended. Such deceptive UI designs can be either intentional (to benefit an online service) or unintentional (through complicit design practices) and can result in adverse effects on end users, such as oversharing personal information or financial loss. While significant research progress has been made toward the development of dark pattern taxonomies across different software domains, developers and users currently lack guidance to help recognize, avoid, and navigate these often subtle design motifs. However, automated recognition of dark patterns is a challenging task, as the instantiation of a single type of pattern can take many forms, leading to significant variability. In this paper, we take the first step toward understanding the extent to which common UI dark patterns can be automatically recognized in modern software applications. To do this, we introduce AidUI, a novel automated approach that uses computer vision and natural language processing techniques to recognize a set of visual and textual cues in application screenshots that signify the presence of ten unique UI dark patterns, allowing for their detection, classification, and localization. To evaluate our approach, we have constructed ContextDP, the current largest dataset of fully-localized UI dark patterns that spans 175 mobile and 83 web UI screenshots containing 301 dark pattern instances. The results of our evaluation illustrate that AidUI achieves an overall precision of 0.66, recall of 0.67, F1-score of 0.65 in detecting dark pattern instances, reports few false positives, and is able to localize detected patterns with an IoU score of 0.84. Furthermore, a significant subset of our studied dark patterns can be detected quite reliably (F1 score of over 0.82), and future research directions may allow for improved detection of additional patterns. This work demonstrates the plausibility of developing tools to aid developers in recognizing and appropriately rectifying deceptive UI patterns.",Dark Pattern; UI Analysis; UI Design,9,A,UX/usability issue detection,audio/visual usability problem encounter,"Screenshot metrics, Source code analysis (web/app crawling), Computer vision","NLP, deep learning",,"Screenshots/images, Source code",no,"automated UI smell detection; computer vision, NLP; UI screenshots; model training to evaluate the approach; nan; AidUI, a novel automated approach that uses computer vision and natural language processing techniques to recognize a set of visual and textual cues in application screenshots that signify the presence of ten unique UI dark patterns, allowing for their detection, classification, and localization; mobile, desktop"
hämäläinen2022,Neural Language Models as What If? -Engines for HCI Research,"Hämäläinen, Perttu; Tavast, Mikke; Kunnari, Anton",2022,conferencePaper,,,77 – 80,"International Conference on Intelligent User Interfaces, Proceedings IUI",IUI,ACM,10.1145/3490100.3516458,https://doi.org/10.1145/3490100.3516458,"Collecting data is one of the bottlenecks of Human-Computer Interaction (HCI) and user experience (UX) research. In this poster paper, we explore and critically evaluate the potential of large-scale neural language models like GPT-3 in generating synthetic research data such as participant responses to interview questions. We observe that in the best case, GPT-3 can create plausible reflections of video game experiences and emotions, and adapt its responses to given demographic information. Compared to real participants, such synthetic data can be obtained faster and at a lower cost. On the other hand, the quality of generated data has high variance, and future work is needed to rigorously quantify the human-likeness, limitations, and biases of the models in the HCI domain. © 2022 Owner/Author.",Computational linguistics; Computer users; GPT-3; Human computer interaction; Human-computer interaction researches; Interaction experiences; Language model; Large-scales; Research data; User interfaces; User Modelling; Users' experiences; Video-games,7,A,UX/usability attributes evaluation,synthetic interview,Synthetic participants,New LMs,LLM,none,no (synthetic),synthetic participants questionnaire responses; synthetic interviews; none; GPT responses labeling and validation; interviews with GPT; potential of large-scale neural language models like GPT-3 in generating synthetic research data such as participant responses to interview questions; -
jeong2020,Detecting usability problems in mobile applications on the basis of dissimilarity in user behavior,"Jeong, JongWook; Kim, NeungHoe; In, Hoh Peter",2020,journalArticle,139,,,International Journal of Human-Computer Studies,,Elsevier,10.1016/j.ijhcs.2019.10.001,https://doi.org/10.1016/j.ijhcs.2019.10.001,"Usability is one of the critical success factors for mobile applications. However, usability is not easy to improve. Many usability testing studies have been conducted to detect usability problems easily, however they usually carry a substantial cost and require specific expertise. In this paper, we present methods and tools to detect usability problems effectively via usability testing in mobile application. We expect users to interact with mobile applications in dissimilar ways when encountering usability problems. Based on this approach, we propose tracing, modeling and comparing methods for measuring the similarity of user behaviors. We evaluate the similarity of user behavior and use it in detecting problems. In addition, we developed tools for our methods to run in automatic way to save usability testing costs. An experiment was conducted using two mobile applications to confirm that our proposed method is useful. We tested eight tasks and found lower similarity when users encountered a usability problem. The experimental results show that our methods and tools can help find usability problems in mobile applications.",Usability; Usability engineering; Mobile applications; Mobile computing; Usability testing; Behavioral research; User behaviors; Usability problems; Behavior modeling; Critical success factor; Behavior model,34,Q2,UX/usability issue detection,user interaction usability problem encounter,User interaction analysis,none,behavior patterns,User interactions,yes,"automated usability problem detection; participant's log data analysis; user interactions, GUI data; usability test; nan; we propose tracing, modeling and comparing methods for measuring the similarity of user behaviors. We evaluate the similarity of user behavior and use it in detecting problems. In addition, we developed tools for our methods to run in automatic way to save usability testing costs; mobile"
kang2014,Analyzing repetitive action in game based on sequence pattern matching,"Kang, Shin Jin; Kim, Young Bin; Kim, Soo Kyun",2014,journalArticle,9,3,523 – 530,Journal of Real-Time Image Processing,,Springer Nature,10.1007/s11554-013-0347-0,https://doi.org/10.1007/s11554-013-0347-0,"As games become more popular, procedures which can support the analysis and understanding of players' behaviors are necessary for success of commercial games. This paper presents a log-based usability evaluation system to analyze user behavior in a gaming environment. We explore the potential of input log data for automated usability evaluation and visualization of player behavior in a game. We traced the keyboard input value and mouse movement of users using a sequence data mining technique in a gaming environment. And we also constructed 3D body meshes for the behavior analysis using Kinect interface. We visualized the data obtained by tracing and automatically searched repetitive patterns in the game and analyzed them. The result obtained from the analysis can be used for user interface optimization, fun evaluation, and the bot-detection field. © 2013 Springer-Verlag Berlin Heidelberg.",User interfaces; Data visualization; Automated usability evaluation; Usability evaluation; Behavioral research; Pattern matching; Behavior analysis; Mouse movements; Player behavior; Repetitive pattern; Sequence patterns; Sequence-data mining; Log data analysis; Mouse tracking,8,Q2,UX/usability attributes evaluation,user engagement,User interaction analysis,none,behavior patterns,"User interactions, User movements",yes,"user behavior analysis; participant's log data analysis; user interactions, motion data; data logging; nan; We explore the potential of input log data for automated usability evaluation and visualization of player behavior in a game.; desktop"
kaur2016,An Empirical Performance Evaluation of Universities Website,"Kaur, Sukhpuneet; Kaur, Kulwant; Kaur, Parminder",2016,journalArticle,146,,10-16,International Journal of Computer Applications,,Taylor & Francis,10.5120/ijca2016910922,,,Usability; Web Service; testing; speed; load time; Pingdom tool; GTMetrix tool; Site Speed Checker tool; Website Grader tool,101,Q4,UX/usability attributes evaluation,user effort/time,Source code analysis (web/app crawling),none,descriptive metrics/visualizations,Source code,no,"automated website qualities evaluation; web crawling data analysis; static web metrics; existing automated website testing tools usage; nan; evaluate and compare the automated testing tools to determine their performance, speed, number of requests, load time, page size, SEO (Search Engine Optimization), mobile and security; not mentioned"
khalajzadeh2023,Supporting Developers in Addressing Human-Centric Issues in Mobile Apps,"Khalajzadeh, Hourieh; Shahin, Mojtaba; Obie, Humphrey O.; Agrawal, Pragya; Grundy, John",2023,journalArticle,49,4,2149 – 2168,IEEE Transactions on Software Engineering,,IEEE,10.1109/TSE.2022.3212329,https://doi.org/10.1109/TSE.2022.3212329,"Failure to consider the characteristics, limitations, and abilities of diverse end-users during mobile app development may lead to problems for end-users, such as accessibility and usability issues. We refer to this class of problems as human-centric issues. Despite their importance, there is a limited understanding of the types of human-centric issues that are encountered by end-users and taken into account by the developers of mobile apps. In this paper, we examine what human-centric issues end-users report through Google App Store reviews, what human-centric issues are a topic of discussion for developers on GitHub, and whether end-users and developers discuss the same human-centric issues. We then investigate whether an automated tool might help detect such human-centric issues and whether developers would find such a tool useful. To do this, we conducted an empirical study by extracting and manually analysing a random sample of 1,200 app reviews and 1,200 issue comments from 12 diverse projects that exist on both Google App Store and GitHub. Our analysis led to a taxonomy of human-centric issues that characterises human-centric issues into three-high level categories: App Usage, Inclusiveness, and User Reaction. We then developed machine learning and deep learning models that are promising in automatically identifying and classifying human-centric issues from app reviews and developer discussions. A survey of mobile app developers shows that the automated detection of human-centric issues has practical applications. Guided by our findings, we highlight some implications and possible future work to further understand and better incorporate addressing human-centric issues into mobile app development. © 1976-2012 IEEE.",Application programs; Australia; Deep learning; E-learning; Github repository; Google play store; Google plays; Human aspects; Human-centric; Human-centric issue; Machine-learning; Mobile applications; Security; Software design; Software development management; Software-systems; Taxonomies,19,Q1,UX/usability issue detection,topic classification,Text analysis,machine learning,,User feedback/reviews/text,yes,automatic human-centric issue detection; NLP; user reviews; model training on existing dataset; issue classification from user reviews; human-centric issues end-users report through Google App Store reviews - We then investigate whether an automated tool might help detect such human-centric issues and whether developers would find such a tool useful; -
kiseleva2016,Predicting user satisfaction with intelligent assistants,"Kiseleva, Julia; Crook, Aidan C.; Williams, Kyle; Zitouni, Imed; Awadallah, Ahmed Hassan; Anastasakos, Tasos",2016,conferencePaper,,,45 – 54,Proceedings of the 39th International ACM SIGIR conference on Research and Development in Information Retrieval,SIGIR,ACM,10.1145/2911451.2911521,https://doi.org/10.1145/2911451.2911521,"There is a rapid growth in the use of voice-controlled intelligent personal assistants on mobile devices, such as Microsoft's Cortana, Google Now, and Apple's Siri. They significantly change the way users interact with search systems, not only because of the voice control use and touch gestures, but also due to the dialogue-style nature of the interactions and their ability to preserve context across different queries. Predicting success and failure of such search dialogues is a new problem, and an important one for evaluating and further improving intelligent assistants. While clicks in web search have been extensively used to infer user satisfaction, their significance in search dialogues is lower due to the partial replacement of clicks with voice control, direct and voice answers, and touch gestures. In this paper, we propose an automatic method to predict user satisfaction with intelligent assistants that exploits all the interaction signals, including voice commands and physical touch gestures on the device. First, we conduct an extensive user study to measure user satisfaction with intelligent assistants, and simultaneously record all user interactions. Second, we show that the dialogue style of interaction makes it necessary to evaluate the user experience at the overall task level as opposed to the query level. Third, we train a model to predict user satisfaction, and find that interaction signals that capture the user reading patterns have a high impact: when including all available interaction signals, we are able to improve the prediction accuracy of user satisfaction from 71% to 81% over a baseline that utilizes only click and query features. © 2016 ACM.",Forecasting; Information retrieval; Intelligent assistants; Mobile devices; Mobile search; Mobile telecommunication systems; Speech processing; Spoken dialogue system; User experience; User interfaces; User satisfaction; User study; World Wide Web,165,A,UX/usability attributes evaluation,user satisfaction,"User interaction analysis, Text analysis",machine learning,,"User interactions, User feedback/reviews/text",yes,"user satisfaction prediction; text analysis, participant logs analysis; user interactions, text, mobile taps; usability test; nan; automatic method to predict user satisfaction with intelligent assistants that exploits all the interaction signals, including voice commands and physical touch gestures on the device; mobile"
koch2016,Computational layout perception using Gestalt laws,"Koch, Janin; Oulasvirta, Antti",2016,conferencePaper,07-12-May-2016,,1423 – 1429,CHI Conference on Human Factors in Computing Systems,CHI,ACM,10.1145/2851581.2892537,https://doi.org/10.1145/2851581.2892537,"We present preliminary results on computational perception of interactive layouts. Our goal is to algorithmically estimate how users perceive a layout. Potential applications range from automated usability evaluation to computer-generated and adaptive interfaces. Layout perception is challenging, however, because of diverse features, combinatorial complexity, and absence of approaches. We have explored Gestalt laws as parsing heuristics. Our approach finds a parametrization that optimally resolves conflicts among competing interpretations of a layout. The output is a hierarchical grouping of main elements. The results are promising: an implementation of just four Gestalt laws enables hierarchical grouping that presents promising results in 90% of our (realistic) test cases. © 2016 Authors.",Adaptive interface; Application range; Automated usability evaluation; Combinatorial complexity; Computational perception; Computer generated; Diverse features; Gestalt law; Hierarchical groupings; Human computer interaction; Human engineering; User interface layouts; User interfaces,24,A,Saliency/visual importance,,Source code analysis (web/app crawling),none,metrics + rule-based,Source code,no,UI layout perception estimation; web crawling data analysis (UI elements metrics); UI elements; web crawling; nan; algorithmically estimate how users perceive a layout; desktop
koonsanit2021,Predicting Final User Satisfaction Using Momentary UX Data and Machine Learning Techniques,"Koonsanit, Kitti; Nishiuchi, Nobuyuki",2021,journalArticle,16,7,3136-3156,Journal of Theoretical and Applied Electronic Commerce Research,,MDPI,10.3390/jtaer16070171,https://doi.org/10.3390/jtaer16070171,"User experience (UX) evaluation investigates how people feel about using products or services and is considered an important factor in the design process. However, there is no comprehensive UX evaluation method for time-continuous situations during the use of products or services. Because user experience changes over time, it is difficult to discern the relationship between momentary UX and episodic or cumulative UX, which is related to final user satisfaction. This research aimed to predict final user satisfaction by using momentary UX data and machine learning techniques. The participants were 50 and 25 university students who were asked to evaluate a service (Experiment I) or a product (Experiment II), respectively, during usage by answering a satisfaction survey. Responses were used to draw a customized UX curve. Participants were also asked to complete a final satisfaction questionnaire about the product or service. Momentary UX data and participant satisfaction scores were used to build machine learning models, and the experimental results were compared with those obtained using seven built machine learning models. This study shows that participants’ momentary UX can be understood using a support vector machine (SVM) with a polynomial kernel and that momentary UX can be used to make more accurate predictions about final user satisfaction regarding product and service usage.",user experience; UX; UX evaluation; satisfaction; prediction; machine learning,19,Q2,UX/usability attributes evaluation,user satisfaction,Questionnaire analysis,machine learning,,User feedback/reviews/text,yes,user satisfaction prediction; questionnnaire data analysis; questionnaire data; product and service evaluation; satisfaction scores from questionnaire data; predict final user satisfaction by using momentary UX data and machine learning techniques; desktop
krause2017,Critique Style Guide: Improving Crowdsourced Design Feedback with a Natural Language Model,"Krause, Markus; Garncarz, Tom; Song, JiaoJiao; Gerber, Elizabeth M.; Bailey, Brian P.; Dow, Steven P.",2017,conferencePaper,,,4627–4639,Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems,CHI,ACM,10.1145/3025453.3025883,https://doi.org/10.1145/3025453.3025883,"Designers are increasingly leveraging online crowds; yet, online contributors may lack the expertise, context, and sensitivity to provide effective critique. Rubrics help feedback providers but require domain experts to write them and may not generalize across design domains. This paper introduces and tests a novel semi-automated method to support feedback providers by analyzing feedback language. In our first study, 52 students from two design courses created design solutions and received feedback from 176 online providers. Instructors, students, and crowd contributors rated the helpfulness of each feedback response. From this data, an algorithm extracted a set of natural language features (e.g., specificity, sentiment etc.) that correlated with the ratings. The features accurately predicted the ratings and remained stable across different raters and design solutions. Based on these features, we produced a critique style guide with feedback examples - automatically selected for each feature - to help providers revise their feedback through self-assessment. In a second study, we tested the validity of the guide through a between-subjects experiment (n=50). Providers wrote feedback on design solutions with or without the guide. Providers generated feedback with higher perceived helpfulness when using our style-based guidance.",Design; critique; feedback; crowdsourcing; expertise; rubrics,57,A,UX/usability attributes evaluation,feedback helpfulness,Text analysis,NLP,,User feedback/reviews/text,yes,feedback helpfulness prediction; NLP; feedback data (user reviews and questionnaire data); feedback giving (questionnaire); nan; semi-automated method to support feedback providers by analysing feedback language; -
kuang2024,Enhancing UX Evaluation Through Collaboration with Conversational AI Assistants: Effects of Proactive Dialogue and Timing,"Kuang, Emily; Li, Minghao; Fan, Mingming; Shinohara, Kristen",2024,conferencePaper,,,,Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems,CHI,ACM,10.1145/3613904.3642168,https://doi.org/10.1145/3613904.3642168,"Usability testing is vital for enhancing the user experience (UX) of interactive systems. However, analyzing test videos is complex and resource-intensive. Recent AI advancements have spurred exploration into human-AI collaboration for UX analysis, particularly through natural language. Unlike user-initiated dialogue, our study investigated the potential of proactive conversational assistants to aid UX evaluators through automatic suggestions at three distinct times: before, in sync with, and after potential usability problems. We conducted a hybrid Wizard-of-Oz study involving 24 UX evaluators, using ChatGPT to generate automatic problem suggestions and a human actor to respond to impromptu questions. While timing did not significantly impact analytic performance, suggestions appearing after potential problems were preferred, enhancing trust and efficiency. Participants found the automatic suggestions useful, but they collectively identified more than twice as many problems, underscoring the irreplaceable role of human expertise. Our findings also offer insights into future human-AI collaborative tools for UX evaluation.",User experience; Natural languages; Usability testing; Human-AI collaboration; Timing circuits; Performance; Users' experiences; Interactive system; Human engineering; Wizard-of-oz studies; Usability problems; Human actor; Proactive conversational assistant; Proactive conversational assistants,1,A,UX/usability issue detection,audio/visual usability problem encounter,"Text analysis, Conversational assistant",New LMs,LLM,Audio/speech,yes,"coversational AI assistant usability issue detection; speech analysis, GPT transcirpt analysis; transcripts (speech data); usability test; suggestion generation from transcripts; proactive conversational assistants to aid UX evaluators through automatic suggestions at three distinct times: before, in sync with, and after potential usability problems; desktop"
kumar2023,E-Commerce Website Usability Analysis Using the Association Rule Mining and Machine Learning Algorithm,"Kumar, Biresh; Roy, Sharmistha; Sinha, Anurag; Iwendi, Celestine; Strazovska, Lubomira",2023,journalArticle,11,1,,Mathematics,,MDPI,10.3390/math11010025,https://doi.org/10.3390/math11010025,"The overall effectiveness of a website as an e-commerce platform is influenced by how usable it is. This study aimed to find out if advanced web metrics, derived from Google Analytics software, could be used to evaluate the overall usability of e-commerce sites and identify potential usability issues. It is simple to gather web indicators, but processing and interpretation take time. This data is produced through several digital channels, including mobile. Big data has proven to be very helpful in a variety of online platforms, including social networking and e-commerce websites, etc. The sheer amount of data that needs to be processed and assessed to be useful is one of the main issues with e-commerce today as a result of the digital revolution. Additionally, on social media a crucial growth strategy for e-commerce is the usage of BDA capabilities as a guideline to boost sales and draw clients for suppliers. In this paper, we have used the KMP algorithm-based multivariate pruning method for web-based web index searching and different web analytics algorithm with machine learning classifiers to achieve patterns from transactional data gathered from e-commerce websites. Moreover, through the use of log-based transactional data, the research presented in this paper suggests a new machine learning-based evaluation method for evaluating the usability of e-commerce websites. To identify the underlying relationship between the overall usability of the eLearning system and its predictor factors, three machine learning techniques and multiple linear regressions are used to create prediction models. This strategy will lead the e-commerce industry to an economically profitable stage. This capability can assist a vendor in keeping track of customers and items they have viewed, as well as categorizing how customers use their e-commerce emporium so the vendor can cater to their specific needs. It has been proposed that machine learning models, by offering trustworthy prognoses, can aid in excellent usability. Such models might be incorporated into an online prognostic calculator or tool to help with treatment selection and possibly increase visibility. However, none of these models have been recommended for use in reusability because of concerns about the deployment of machine learning in e-commerce and technical issues. One problem with machine learning science that needs to be solved is explainability. For instance, let us say B is 10 and all the people in our population are even. The hash function's behavior is not random since only buckets 0, 2, 4, 6, and 8 can be the value of h(x). However, if B = 11, we would find that 1/11th of the even integers is transmitted to each of the 11 buckets. The hash function would work well in this situation.",association rule; big data analytics; collaborative filtering; e-commerce usability; hashing; KMP algorithm; machine learning; web mining,29,Q1,UX/usability attributes evaluation,user engagement,User interaction analysis,machine learning,,User interactions,yes,"automated website usability assessment; participant's log data analysis; web stream mining attributes - web visits, visitors (user interactions); none; nan; This study aimed to find out if advanced web metrics, derived from Google Analytics software, could be used to evaluate the overall usability of e-commerce sites and identify potential usability issues. - new machine learning-based evaluation method for evaluating the usability of e-commerce website; not mentioned"
kwon2022,Analytical Framework for Facial Expression on Game Experience Test,"Kwon, Seungjin; Ahn, Jaehyun; Choi, Hyukgeun; Jeon, Jiho; Kim, Doyoung; Kim, Hoyeon; Kang, Shinjin",2022,journalArticle,10,,104486-104497,IEEE Access,,IEEE,10.1109/ACCESS.2022.3210712,https://doi.org/10.1109/ACCESS.2022.3210712,"Game experience testing, an essential process for developing and servicing high-quality games, aims to evaluate and improve the play experience from the user perspective. Previous game experience tests are usually held offline with recruited test participants. Participants play the game in a controlled environment, and their experiences are recorded and analyzed by domain experts. However, this process not only requires substantial resources in terms of time and cost but also misses momentary data that can infer the user’s experience directly and quantitatively, such as facial expressions or body gestures of the participants while playing the game. In this paper, we propose a framework that can automatically collect and analyze video data of participants in a remote environment. We designed various experiments to verify that the proposed framework can automatically collect and analyze facial data in a remote environment. The experimental results show that our framework can be applied in gaming experience tests for popular commercial games currently in service. This paper presents a pioneering work that applies the face analysis method to the commercial game development process, and its purpose is beyond that of a pure research work.",face detection; Face recognition; facial expression recognition; game experience test framework; Game user research; Games; head pose recognition; Pose estimation; Sensors; Streaming media; Testing; User experience; WebRTC,6,Q2,Emotion detection,,Computer vision,deep learning,,Video/facial expressions,yes,automated game experience testing; computer vision; facial expressions; game test; facial expression and engagement detection; framework that can automatically collect and analyze video data of participants in a remote environment - framework can be applied in gaming experience tests for popular commercial games currently in service; desktop
kyelem2023,Automatic Graphical User Interface aesthetic evaluation tool using the UIED segmentation algorithm,"Kyelem, Yacouba; Zo, Mamadou; Kabore, Kisito K.; Ouedraogo, Frédéric T.",2023,conferencePaper,,,20–26,Proceedings of the 2023 3rd International Conference on Human Machine Interaction,INTERACT,ACM,10.1145/3604383.3604387,https://doi.org/10.1145/3604383.3604387,"In this work, we deal with the issue of automating the evaluation of the aesthetics of the user's applications interfaces. The aesthetics of the user interface is a sub-factor of software quality. The aesthetics of the interface provides several advantages to the users, such as the attraction of the interface, an impact on the usability of the interface, a great pleasure of the interface's use, etc. That is why we deal with the evaluation of the aesthetics interfaces application. Our goal is to suggest a tool for quantitative evaluation of user interfaces using metrics. That is why we have developed a tool for evaluating the aesthetics of application interfaces that takes screenshots of mobile and desktop applications. This tool also generates metrics as output. For the segmentation of the different components of the interfaces we used the UIED algorithm which is an algorithm for the automatic segmentation of images. Our tool uses 10 of the 14 metrics of Ngo et al [1]. We generated the values with our tool by taking the images of a site. We used a total of 351 images. The validation of our tool was done by comparing the data calculated by our tool to those of QUESTIM. We used Pearson correlation test to perform the comparison and we obtained a correlation rate of 0.91. This means that our tool and QUESTIM evaluate the interfaces in almost the same way.",Aesthetic automatic evaluation; Aesthetic metric; Quantitative approach; User Interface Element Detection (UIED),0,B,UX/usability attributes evaluation,Visual aesthetics,Screenshot metrics,deep learning,,Screenshots/images,no,aesthetics evaluation; image metric analysis; UI screenshots; none; nan; tool for evaluating the aesthetics of application interfaces that takes screenshots of mobile and desktop applications. This tool also generates metrics as output; desktop
leiva2022,Describing UI Screenshots in Natural Language,"Leiva, Luis A.; Hota, Asutosh; Oulasvirta, Antti",2022,journalArticle,14,1,19:1–19:28,ACM Transactions on Intelligent Systems and Technology,,ACM,10.1145/3564702,https://doi.org/10.1145/3564702,"Being able to describe any user interface (UI) screenshot in natural language can promote understanding of the main purpose of the UI, yet currently it cannot be accomplished with state-of-the-art captioning systems. We introduce XUI, a novel method inspired by the global precedence effect to create informative descriptions of UIs, starting with an overview and then providing fine-grained descriptions about the most salient elements. XUI builds upon computational models for topic classification, visual saliency prediction, and natural language generation (NLG). XUI provides descriptions with up to three different granularity levels that, together, describe what is in the interface and what the user can do with it. We found that XUI descriptions are highly readable, are perceived to accurately describe the UI, and score similarly to human-generated UI descriptions. XUI is available as open-source software.",Captioning; visual saliency; deep learning; natural language processing,7,Q2,UI understanding/feedback,,Computer vision,deep learning,,Screenshots/images,no,"UI automated description, automated saliency prediction; NLG, computer vision; UI screenshots; model training; predicting salient elemnts from sherenshots; informative descriptions of UIs, starting with an overview and then providing fine-grained descriptions about the most salient elements.; mobile"
leiva2023,Modeling how different user groups perceive webpage aesthetics,"Leiva, Luis A.; Shiripour, Morteza; Oulasvirta, Antti",2023,journalArticle,22,4,1417-1424,Universal Access in the Information Society,,Springer Nature,10.1007/s10209-022-00910-x,https://doi.org/10.1007/s10209-022-00910-x,"Aesthetics is a central consideration in user interface design. It is known to affect end-user behavior and perception, in particular the first impression of graphical user interfaces. However, what users perceive as pleasant or good design is highly subjective. We contribute a computational model that estimates the visual appeal of a given webpage for several common cohorts, or user groups, including gender, age, and education level. Our model, a convolutional neural network trained on 418 webpage screenshots having 771k aesthetic scores (in a 1–9 Likert scale) from 32k users, achieves high accuracy and is always less than 1 point off from ground-truth ratings. Designers can use our model to anticipate how people would rate their webpage, offer personalized designs according to the visual preferences of their users, and support rapid evaluations of webpage design prototypes for specific cohorts.",Webpage Aesthetics; Visual Design; Computational Accessibility; Neural Networks,9,Q3,UX/usability attributes evaluation,Visual aesthetics,Computer vision,deep learning,,Screenshots/images,no,"visual appeal estimation; computer vision; UI screenshots; model training; predicting aesthetics from screenshots; computational model that estimates the visual appeal of a given webpage for several common cohorts, or user groups, including gender, age, and education level; desktop"
li2016,Webpage saliency prediction with multi-features fusion,"Li, Jian; Su, Li; Wu, Bo; Pang, Junbiao; Wang, Chunfeng; Wu, Zhe; Huang, Qingming",2016,conferencePaper,,,674-678,2016 IEEE International Conference on Image Processing (ICIP),ICIP,IEEE,10.1109/ICIP.2016.7532442,https://doi.org/10.1109/ICIP.2016.7532442,"We proposed a novel model to predict human's visual attention when free-viewing webpages. Compared with natural images, webpages are usually full of salient regions such as logos, text, and faces, while few of them attract human's attention in a short sight. Moreover, webpages perform distinct viewing patterns which are quite different from the natural images. In this paper, we introduced multi-features according to our observation on webpages characters and related eye-tracking data. Further, in order to achieve a flexible adaptation to various types of webpages, we employed a machine-learning framework based on our proposed features. Experimental results demonstrate that our model outperforms other state-of-the-art methods in webpage saliency prediction.",Webpages viewing; Saliency; Support vector machine; Multi-features,17,B,Saliency/visual importance,,Computer vision,machine learning,,Screenshots/images,no,saliency prediction on the web; computer vision; UI screenshots; model training; generating a saliency map from feeatures from UI image metrics; MULTI-FEATURES FUSION for web saliency prediction approach; desktop
li2022,Continuous Usability Requirements Evaluation based on Runtime User Behavior Mining,"Li, Tong; Zhang, Tianai",2022,conferencePaper,2022-December,,1036 – 1045,"2022 IEEE 22nd International Conference on Software Quality, Reliability and Security",QRS,IEEE,10.1109/QRS57517.2022.00107,https://doi.org/10.1109/QRS57517.2022.00107,"Usability requirements have been widely recognized as an essential quality requirement for systems that interact with people. However, evaluating the satisfaction of usability requirements usually involves user interactions, which is intrusive and time-consuming. In this paper, we propose a novel framework for systematically and automatically evaluating the satisfaction of usability requirements at runtime. Specifically, a behavior-centric conceptual model is proposed to comprehensively characterize user behaviors. An analysis process is then proposed based on the conceptual model, which systematically refines high-level usability requirements into observable and measurable user behaviors in order to automatically evaluate their satisfaction. Moreover, we investigate and mine patterns of user behaviors, which further explain the results of the satisfaction analysis. We systematically design and conduct a case study to evaluate our proposed framework, the results of which show that our approach is able to identify most usability issues and precisely assess the satisfaction of participants' usability requirements. Importantly, our approach enables continuous usability requirements evaluation without interfering with users, pragmatically contributing to trade-off analysis among quality requirements at runtime. © 2022 IEEE.",Usability engineering; Software quality; Behavioral sciences; Analytical models; Systematics; Software reliability; Planning; Behavioral research; Runtimes; Quality control; Economic and social effects; User behaviors; Requirements engineering; Usability requirements; Requirement analysis; Behavior pattern mining; Behaviour patterns; Conceptual model; Pattern mining; Runtime requirement analyse; Usability smell; Usability smells; Runtime; Conceptual modeling; Runtime requirements analysis,1,B,UX/usability attributes evaluation,user satisfaction,User interaction analysis,none,metrics + rule-based,User interactions,yes,"automated usability smell detection; real usage user interaction analysis; user interactions; none; nan; behavior-centric conceptual model is proposed to comprehensively characterize user behaviors. An analysis process is then proposed basedon the conceptual model, which systematically refines highlevel usability requirements into observable and measurable user behaviors in order to automatically evaluate their satisfaction; not mentioned"
li2017,Towards Measuring and Inferring User Interest from Gaze,"Li, Yixuan; Xu, Pingmei; Lagun, Dmitry; Navalpakkam, Vidhya",2017,conferencePaper,,,525–533,Proceedings of the 26th International Conference on World Wide Web Companion,,ACM,10.1145/3041021.3054182,https://doi.org/10.1145/3041021.3054182,"How can we reliably infer web users' interest and evaluate the content relevance when lacking active user interaction such as click behavior? In this paper, we investigate the relationship between mobile users' implicit interest inferred from attention metrics, such as eye gaze or viewport time, and explicit interest expressed by users. We present the first quantitative gaze tracking study using front-facing camera of mobile devices instead of specialized, expensive eye-tracking devices. We focus on multi-column digital media pages in Google Play Store that display 30+ items per page belonging to diverse categories. In such pages, we find significantly different distribution of gaze metrics on items that users rate as interesting vs. not. We leverage this insight by building a prediction model that is able to infer a user's interest ratings from the the non-click actions of the user. Our model is able to attain AUC of 90.32% in predicting user interest at an individual item level. In addition, our experiments on collection item re-ranking show how user gaze and viewport signals can be used to personalize item ranking on the collection page. Beyond understanding users' attention behavior in novel contexts such as multi-column digital media pages in Google Play Store, the findings in this study have implications for the design of a novel personalization and recommendation mechanism by (1) prioritizing items that are most likely of interest to users based on historical attention signals, and (2) prioritizing positions receiving significant portion of gaze attention.",mobile phones; user attention; eye-tracking; personalization; user interest; inference,49,?,UX/usability attributes evaluation,user interest,Eyetracking,machine learning,,gaze data,yes,"automated usability testing; eye-tracking; eye-tracking data; model training; user interest from gaze data;  In this paper, we investigate the relationship between mobile users' implicit interest inferred from attention metrics, such as eye gaze or viewport time, and explicit interest expressed by users. We present the first quantitative gaze tracking study using front-facing camera of mobile devices instead of specialized, expensive eye-tracking devices.; mobile"
liapis2021,Advancing stress detection methodology with deep learning techniques targeting ux evaluation in aal scenarios: Applying embeddings for categorical variables,"Liapis, Alexandros; Faliagka, Evanthia; Antonopoulos, Christos P.; Keramidas, Georgios; Voros, Nikolaos",2021,journalArticle,10,13,,Electronics,,MDPI,10.3390/electronics10131550,https://doi.org/10.3390/electronics10131550,"Physiological measurements have been widely used by researchers and practitioners in order to address the stress detection challenge. So far, various datasets for stress detection have been recorded and are available to the research community for testing and benchmarking. The majority of the stress-related available datasets have been recorded while users were exposed to intense stressors, such as songs, movie clips, major hardware/software failures, image datasets, and gaming scenarios. However, it remains an open research question if such datasets can be used for creating models that will effectively detect stress in different contexts. This paper investigates the performance of the publicly available physiological dataset named WESAD (wearable stress and affect detection) in the context of user experience (UX) evaluation. More specifically, electrodermal activity (EDA) and skin temperature (ST) signals from WESAD were used in order to train three traditional machine learning classifiers and a simple feed forward deep learning artificial neural network combining continues variables and entity embeddings. Regarding the binary classification problem (stress vs. no stress), high accuracy (up to 97.4%), for both training approaches (deep-learning, machine learning), was achieved. Regarding the stress detection effectiveness of the created models in another context, such as user experience (UX) evaluation, the results were quite impressive. More specifically, the deep-learning model achieved a rather high agreement when a user-annotated dataset was used for validation. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.",deep learning; Deep learning; UX evaluation; Stress detection; electrodermal activity; entity embeddings; stress detection; Electrodermal activity; Entity embeddings,9,Q3,Emotion detection,,Physiological signal analysis,"deep learning, machine learning",,physiological signals,yes,"stress detection; physiological signals analysis; physiological signals - skin conductivity; none (model training); emotion prediction from skin conductivity data; This paper investigates the performance of the publicly available physiological dataset named WESAD (wearable stress and affect detection) in the context of user experience (UX) evaluation. More specifically, electrodermal activity (EDA) and skin temperature (ST) signals from WESAD were used in order to train three traditional machine learning classifiers; desktop"
liapis2021a,Detection of Subtle Stress Episodes During UX Evaluation: Assessing the Performance of the WESAD Bio-Signals Dataset,"Liapis, Alexandros; Faliagka, Evanthia; Katsanos, Christos; Antonopoulos, Christos; Voros, Nikolaos",2021,conferencePaper,12934 LNCS,,238 – 247,Human-Computer Interaction–INTERACT 2021: 18th IFIP TC 13 International Conference,INTERACT,Springer Nature,10.1007/978-3-030-85613-7_17,https://doi.org/10.1007/978-3-030-85613-7_17,"Stress is a highly subjective condition and may largely vary in different contexts. Bio-signals have been widely used by researchers and practitioners to monitor stress levels. Consequently, various bio-signals datasets for stress recognition have been recorded. The most of publicly available physiological datasets have been emotionally annotated in a context where users have been exposed to intense stressors, such as movie clips, songs, major hardware/software failures, image datasets, and gaming. However, it remains unexplored how effectively such datasets can be used in different contexts. This paper investigates the performance of the publicly available dataset named WESAD (Wearable Stress and Affect Detection) in the context of UX evaluation. More specifically, skin conductance signal from WESAD was used to train four machine learning classifiers. Regarding the binary classification problem (stress vs. no stress), models’ accuracy was rather high (at least 91.1%). However, it was found that their effectiveness in assessing stress in the context of UX was rather poor when a new bio-signals dataset was used. © 2021, IFIP International Federation for Information Processing.",Affect detection; Binary classification problems; Hardware/software; Human computer interaction; Image datasets; Learning systems; Movie clips; Skin conductance; Stress levels; Stress recognition,18,B,Emotion detection,,Physiological signal analysis,"deep learning, machine learning",,physiological signals,yes,"stress detection; physiological signals analysis; physiological signals - skin conductivity; none (model training); emotion prediction from skin conductivity data; This paper investigates the performance of the publicly available physiological dataset named WESAD (wearable stress and affect detection) in the context of user experience (UX) evaluation. More specifically, electrodermal activity (EDA) and skin temperature (ST) signals from WESAD were used in order to train three traditional machine learning classifiers; desktop"
liu2023,Nighthawk: Fully Automated Localizing UI Display Issues via Visual Understanding,"Liu, Zhe; Chen, Chunyang; Wang, Junjie; Huang, Yuekai; Hu, Jun; Wang, Qing",2023,journalArticle,49,1,403-418,IEEE Transactions on Software Engineering,,IEEE,10.1109/TSE.2022.3150876,https://doi.org/10.1109/TSE.2022.3150876,"Graphical User Interface (GUI) provides a visual bridge between a software application and end users, through which they can interact with each other. With the upgrading of mobile devices and the development of aesthetics, the visual effects of the GUI are more and more attracting, and users pay more attention to the accessibility and usability of applications. However, such GUI complexity posts a great challenge to the GUI implementation. According to our pilot study of crowdtesting bug reports, display issues such as text overlap, component occlusion, missing image always occur during GUI rendering on different devices due to the software or hardware compatibility. They negatively influence the app usability, resulting in poor user experience. To detect these issues, we propose a fully automated approach, Nighthawk, based on deep learning for modelling visual information of the GUI screenshot. Nighthawk can detect GUIs with display issues and also locate the detailed region of the issue in the given GUI for guiding developers to fix the bug. At the same time, training the model needs a large amount of labeled buggy screenshots, which requires considerable manual effort to prepare them. We therefore propose a heuristic-based training data auto-generation method to automatically generate the labeled training data. The evaluation demonstrates that our Nighthawk can achieve average 0.84 precision and 0.84 recall in detecting UI display issues, average 0.59 AP and 0.60 AR in localizing these issues. We also evaluate Nighthawk with popular Android apps on Google Play and F-Droid, and successfully uncover 151 previously-undetected UI display issues with 75 of them being confirmed or fixed so far.",deep learning; Deep learning; Graphical user interfaces; Location awareness; Software; Internet; UI display; UI testing; Visual effects; Training data; mobile app; Computer bugs; object detection; Application programs; Program debugging; End-users; Display devices; Fully automated; Heuristic methods; Interface complexity; Screenshots; Software applications,28,Q1,UI issue detection,,Computer vision,deep learning,,Screenshots/images,no,display (UI) issue detection (GUI); computer vision; UI screenshot; none (model training); issue detection from UI screenshot; Nighthawk to automatically detect and localize UI display issues in the screenshots of the application under tes; mobile
macakoğlu2023,"Accessibility, usability, and security evaluation of universities’ prospective student web pages: a comparative study of Europe, North America, and Oceania","Macakoğlu, Şevval Seray; Peker, Serhat; Medeni, İhsan Tolga",2023,journalArticle,22,2,671 – 683,Universal Access in the Information Society,,Springer Nature,10.1007/s10209-022-00869-9,https://doi.org/10.1007/s10209-022-00869-9,"Universities' prospective student web pages aim to disseminate information about their academic and social opportunities to their stakeholders; therefore, they must be accessible, of high quality of use and reliable. This article presents the accessibility, usage performance, and security analysis of prospective student web pages of 330 universities from three continents, namely Europe, North America, and Oceania. For this purpose, university websites were selected based on the Webometrics ranking, and online automated test tools were used. The results showed that websites at universities in North America paid more attention to accessibility and quality of use on prospective student web pages, followed by Oceanian and European websites. Evaluated websites had low compliance levels according to the WCAG 2.0 guideline. No major problems were identified in terms of usability and security, but there were certain points for improvement. Moreover, we present and discuss recommendations to developers and administrators for websites to resolve accessibility, usability, and security breaches and provide information equally to all stakeholders. Hence, this analysis report provides feedback to web developers to improve accessibility, quality of use, and security issues of university websites and their prospective student web pages. © 2022, The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature.",Human computer interaction; Information dissemination; Performances evaluation; Prospectives; Quality control; Students; Usability and security; Usability engineering; Web accessibility; Web Design; Web information systems; Web performance; Web performance evaluation; WEB security; Web usability; Web-page,35,Q3,UX/usability attributes evaluation,user effort/time,Source code analysis (web/app crawling),none,descriptive metrics/visualizations,Source code,no,"automated usability testing; web crawling; static web data (DOM); online testing tool usage; nan; analysis of prospective student web pages of 330 universities from three continents, namely Europe, North America, and Oceania; not mentioned"
marenkov2016,A Framework for Improving Web Application User Interfaces Through Immediate Evaluation,"Marenkov, Jevgeni; Robal, Tarmo; Kalja, Ahto",2016,journalArticle,291,,283-296,Frontiers in Artificial Intelligence and Applications,,IOS Press,10.3233/978-1-61499-714-6-283,https://doi.org/10.3233/978-1-61499-714-6-283,"More and more web applications are being migrated from desktop platforms to platforms for smartphones and tablets. Usability and User experience are highly different on desktop and portable devices. Every minor modification of user interface (UI) could lead to violations of usability guidelines, e.g. changing the link color or font size could lead to lower usability for people with disabilities. Empirical usability evaluation methods are the approaches that help to verify the usability conformance to guidelines. Nevertheless, there are number of obstacles preventing from using such methods widely. The purpose of our study is to propose a conceptual model and corresponding framework with methodology including category specific metrics for immediate automatic usability evaluation of web application user interfaces during design and implementation phase. We address the gap between usability evaluation and development stage of UI by providing immediate feedback to developers.",User interfaces; Usability engineering; Smart phones; Usability evaluation; WEB application; Web applications; Users' experiences; Web usability; Portable device; Usability guidelines; Web user interface; Desktop devices; web usability; usability guidelines; web user interface,2,Q3,UX/usability issue detection,guidelines evaluation,Source code analysis (web/app crawling),none,descriptive metrics/visualizations,Source code,no,automated GUI testing (selenium) - automatic guidalines evaluation; web crawling; static web data (DOM); web crawling; nan; onceptual model and corresponding framework with methodology including category specific metrics for immediate automatic usability evaluation of web application user interfaces during design and implementation phase; desktop
marenkov2016a,A Study on Immediate Automatic Usability Evaluation of Web Application User Interfaces,"Marenkov, Jevgeni; Robal, Tarmo; Kalja, Ahto",2016,conferencePaper,615,,257-271,Databases and Information Systems: 12th International Baltic Conference,DB&IS,Springer Nature,10.1007/978-3-319-40180-5_18,https://doi.org/10.1007/978-3-319-40180-5_18,"More and more web applications are being migrated from desktop platforms to mobile platforms. User experience is extremely different on desktop and portable devices. Changes in user interfaces (UI) could lead to severe violations of usability rules, e.g. changing the text color could lead to decrease of accessibility for users with low vision or cognitive impairments. Manual usability inspection methods are the approaches that help to verify the usability conformance to guidelines. Nevertheless, there are number of difficulties why the aforementioned approaches could not be always applied. The purpose of our research is to develop a conceptual model and corresponding framework including category specific metrics with methodology for immediate automatic usability evaluation of web application user interfaces during design and implementation phase. We address the gap between usability evaluation and development stage of user interface by providing immediate feedback to UI developers.",User interfaces; Usability engineering; Information systems; Usability evaluation; Design and implementations; Web usability; Immediate feedbacks; Usability inspection; World Wide Web; Usability guidelines; Web user interface; Cognitive impairment,8,B,UX/usability issue detection,guidelines evaluation,Source code analysis (web/app crawling),none,descriptive metrics/visualizations,Source code,no,automated GUI testing (selenium) - automatic guidalines evaluation; web crawling; static web data (DOM); web crawling; nan; develop a conceptual model and corresponding framework including category specific metrics with methodology for immediate automatic usability evaluation of web application user interfaces during design and implementation phase; desktop
marenkov2017,A tool for design-time usability evaluation of web user interfaces,"Marenkov, Jevgeni; Robal, Tarmo; Kalja, Ahto",2017,conferencePaper,10509 LNCS,,394 – 407,Advances in Databases and Information Systems: 21st European Conference,ADBIS,Springer Nature,10.1007/978-3-319-66917-5_26,https://doi.org/10.1007/978-3-319-66917-5_26,"The diversity of smartphones and tablet computers has become intrinsic part of modern life. Following usability guidelines while designing web user interface (UI) is an essential requirement for each web application. Even a minor change in UI could lead to usability problems, e.g. changing background or foreground colour of buttons could cause usability problems especially for people with disabilities. Empirical evaluation methods such as questionnaires and Card Sorting are effective in finding such problems. Nevertheless, these methods cannot be used widely when time, money and evaluators are scarce. The purpose of our work is to deliver a tool for design-time automatic evaluation of UI conformance to category-specific usability guidelines. The main contribution of this solution is enabling immediate cost-efficient and automatic web UI evaluation that conforms to available and set standards. This approach is being integrated into the Estonian eGovernment authority in order to automate usability evaluation of web applications. © 2017, Springer International Publishing AG.",Automatic evaluation; Category specifics; Empirical evaluations; Information systems; People with disabilities; Surveys; Usability engineering; Usability evaluation; Usability guidelines; User interfaces; Web usability; Web user interface; Websites,5,B,UX/usability attributes evaluation,domain usability (ontology),Source code analysis (web/app crawling),none,test execution,Source code,no,"automated GUI testing (selenium), ontology testing - automatic guidalines evaluation; web crawling; static web data (DOM); web crawling; nan; tool for design-time automatic evaluation of UI conformance to category-specific usability guidelines; desktop"
mathur2018,Usability Evaluation Framework for Mobile Apps using Code Analysis,"Mathur, Neeraj; Karre, Sai Anirudh; Reddy, Y. Raghu",2018,conferencePaper,,,187–192,Proceedings of the 22nd International Conference on Evaluation and Assessment in Software Engineering 2018,EASE,ACM,10.1145/3210459.3210480,https://doi.org/10.1145/3210459.3210480,"The increasing usage of smart-phones has resulted in mobile applications replacing or supplementing traditional web-based applications. Given the limitations of the form factor in smartphones, usability can be considered as one of the important attributes that determine the success of a mobile application. The measures available for assessing the usability of mobile applications tend to focus more on human aspects and less on the functional aspects of usability. As part of this paper, we propose a usability evaluation framework to identify functional usability issues specific to mobile applications. This framework uses usability guidelines and code analysis to improve the usability of a mobile application. As a proof of concept, we have built an end-to-end system using the framework to validate and verify usability issues in Android mobile applications. We also generate code recommendations to implement failed usability guidelines.",Mobile Usability; Usability Guidelines; Code Analysis; Usability Evaluation; Automation; Mobile Apps,9,A,UI issue detection,,Source code analysis (web/app crawling),none,test execution,Source code,no,automatic guidalines evaluation (GUI testing); source code analysis; source code; guideline evaluation; nan; usability evaluation framework to identify functional usability issues specific to mobile applications; mobile
miniukovich2015,Computation of Interface Aesthetics,"Miniukovich, Aliaksei; De Angeli, Antonella",2015,conferencePaper,,,1163–1172,Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems,CHI,ACM,10.1145/2702123.2702575,https://doi.org/10.1145/2702123.2702575,"People prefer attractive interfaces. Designers strive to outmatch competitors, and create apps and websites that stand out. However, significant expenses on design are unaffordable to small companies; instead, they could adopt automatic tools of interface aesthetics evaluation, a cheaper strategy to good design. This paper describes an important step towards such a tool; it presents eight automatic metrics of graphical user interface (GUI) aesthetics. We tested the metrics in two exploratory studies -- on desktop webpages (N = 62) and on iPhone apps (N = 53) -- and found them to function on both GUI types and for both immediate (150ms exposure) and deliberate (4s exposure) aesthetics impressions. Our best-fit regression models explained up to 49% of variance in webpage aesthetics and up to 32% (if app genre is considered) of variance in iPhone app aesthetics. These results confirm past results and suggest the metrics are valid and reliable enough to be widely discussed, and possibly, to be embedded in our prospective GUI evaluation tool, tLight.",Automatic metrics; GUI evaluation; user study; immediate impression; deliberate impression; tLight.,100,A,UX/usability attributes evaluation,Visual aesthetics,Screenshot metrics,machine learning,,Screenshots/images,no,aesthetics evaluation; screenshot analysis; UI screenshots; aesthetics evaluation; nan; it presents eight automatic metrics of graphical user interface (GUI) aesthetics.; mobile
moran2018,Automated reporting of GUI design violations for mobile apps,"Moran, Kevin; Li, Boyang; Bernal-Cárdenas, Carlos; Jelf, Dan; Poshyvanyk, Denys",2018,conferencePaper,,,165–175,Proceedings of the 40th International Conference on Software Engineering,ICSE,ACM,10.1145/3180155.3180246,https://doi.org/10.1145/3180155.3180246,"The inception of a mobile app often takes form of a mock-up of the Graphical User Interface (GUI), represented as a static image delineating the proper layout and style of GUI widgets that satisfy requirements. Following this initial mock-up, the design artifacts are then handed off to developers whose goal is to accurately implement these GUIs and the desired functionality in code. Given the sizable abstraction gap between mock-ups and code, developers often introduce mistakes related to the GUI that can negatively impact an app's success in highly competitive marketplaces. Moreover, such mistakes are common in the evolutionary context of rapidly changing apps. This leads to the time-consuming and laborious task of design teams verifying that each screen of an app was implemented according to intended design specifications.This paper introduces a novel, automated approach for verifying whether the GUI of a mobile app was implemented according to its intended design. Our approach resolves GUI-related information from both implemented apps and mock-ups and uses computer vision techniques to identify common errors in the implementations of mobile GUIs. We implemented this approach for Android in a tool called Gvt and carried out both a controlled empirical evaluation with open-source apps as well as an industrial evaluation with designers and developers from Huawei. The results show that Gvt solves an important, difficult, and highly practical problem with remarkable efficiency and accuracy and is both useful and scalable from the point of view of industrial designers and developers. The tool is currently used by over one-thousand industrial designers and developers at Huawei to improve the quality of their mobile apps.",,117,A,UI issue detection,,Source code analysis (web/app crawling),none,metrics + rule-based,Source code,no,GUI design violation detection; source code analysis; soruce code; GUI testing; nan; automated approof of wheter GUI was implemented accorgin to design; mobile
namoun2021,A Review of Automated Website Usability Evaluation Tools: Research Issues and Challenges,"Namoun, Abdallah; Alrehaili, Ahmed; Tufail, Ali",2021,conferencePaper,12779 LNCS,,292 – 311,International Conference on Human-Computer Interaction,HCII,Springer Nature,10.1007/978-3-030-78221-4_20,https://doi.org/10.1007/978-3-030-78221-4_20,"Web usability is a critical factor for visitors' acceptance and satisfaction. The prevalence of various free and proprietary website testing tools enabled the fast evaluation of the usability of websites. However, their effectiveness in providing meaningful, consistent, and valid results remains questionable. In this study, we initially devised a usability framework that incorporates 19 usability dimensions and investigated the compliance of 10 popular web usability testing tools to this framework. Next, we applied the automated evaluation to nine websites spanning three major categories, namely e-commerce, vacation rentals, and education. On a positive note, the tools inspected a wide range of aspects, including performance, SEO, page size, accessibility, and security. However, our in-depth analysis revealed numerous critical issues. First, usability seems to be ignored or distorted by most of the tools. Second, the automated tools exhibited variable and contradictory scores concerning the analysis of the same websites. Third, the analysis reports were vague and complicated for non-technical users. Fourth, the tools identified and explained issues affecting the technical implementation of the websites, overlooking web usability flaws. Fifth, only four tools (i.e., SEOptimiser, Dareboost, Website Grader, and Sure Oak) gave recommendations to remedy performance bottlenecks and improve the quality of the websites. Lastly, the inner working of the tools does not seem to incorporate the theoretical foundations of usability, which calls for urgent collaboration between industry experts and HCI practitioners and researchers. © 2021, Springer Nature Switzerland AG.",Usability engineering; Automation; Usability testing; Usability evaluation; Automated web evaluation tools; Automatic web usability evaluation; Perceived usability; Web usability issues; Website design; Web Design; Evaluation tool; Automated web evaluation tool; Web evaluations; Web usability; Web usability issue,21,B,UX/usability attributes evaluation,"usability metrics (efficiency,...)",Source code analysis (web/app crawling),none,descriptive metrics/visualizations,Source code,no,autoamted usability testing; web crawling; source code (DOM); online testing tool usage; nan; usability framework that incorporates 19 usability dimensions and investigated the compliance of 10 popular web usability testing tools to this framework; not mentioned
parajuli2020,A comparative study of accessibility and usability of norwegian university websites for screen reader users based on user experience and automated assessment,"Parajuli, Prabin; Eika, Evelyn",2020,conferencePaper,12188 LNCS,,300 – 310,International Conference on Human-Computer Interaction,HCII,Springer Nature,10.1007/978-3-030-49282-3_21,https://doi.org/10.1007/978-3-030-49282-3_21,"Websites are essential for learners’ access to information. However, due to the lack of accessibility and usability of websites, students with disabilities who solely rely on screen readers face challenges accessing webpage contents. This study explores accessibility and usability issues frequently encountered by screen reader students while interacting with Norwegian university webpages. An evaluation using automated tools showed that none of the university websites met the minimum WCAG 2.1 guidelines. Sixteen visually impaired participants were recruited and assigned five usability tasks on four different university websites. The results show that participants encountered usability and accessibility issues on all four websites. Recommendations for increased accessibility are proposed based on the findings. © Springer Nature Switzerland AG 2020.",Automated assessment; Automated tools; Comparative studies; Human computer interaction; Norwegian University; Screen readers; Usability engineering; User experience; Visually impaired; Web Design; Websites,9,B,UX/usability attributes evaluation,"usability metrics (efficiency,...)",Source code analysis (web/app crawling),none,descriptive metrics/visualizations,Source code,no,autoamted usability testing; web crawling; source code (DOM); online testing tool usage; nan; study explores accessibility and usability issues frequently encountered by screen reader students while interacting with Norwegian university webpages; not mentioned
paternò2016,Timelines for mobile web usability evaluation,"Paternò, F.; Schiavone, A.G.; Pitardi, P.",2016,conferencePaper,07-10-June-2016,,88 – 91,Proceedings of the Workshop on Advanced Visual Interfaces AVI,AVI,ACM,10.1145/2909132.2909272,https://doi.org/10.1145/2909132.2909272,"In the field of Web usability evaluation, one potential effective approach is the usage of logging tools for remote usability analysis (i.e. tools capable of tracking and recording the users' activities while they interact with a Web site), and then presenting the recorded data to usability experts in such a way to support detection of possible usability problems. In the design of such automated tools, in addition to the problems related to user behavior recording, another important issue is the choice of meaningful visual representations in order to support the usability expert analysis. In this paper we present a timeline-based system for interactive events visualization, which has been exploited in a proxy-based mobile Web usability evaluation tool. We discuss how such visualizations can be exploited in finding usability issues and the type of problems that can be detected through them. We show various ways to compare timelines related to users sessions with ideal timelines representing optimal behavior. © 2016 Copyright held by the owner/author(s).",Behavioral research; Effective approaches; Expert analysis; Mobile web; Remote usability; Remote usability evaluations; Timelines; Usability engineering; Usability problems; Visual representations; Visualization; Well logging,22,B,UX/usability issue detection,user interaction usability problem encounter,User interaction analysis,none,descriptive metrics/visualizations,User interactions,yes,"aid to usability issue tdetection; log data analysis; user interactions; none; nan; timeline-based system for interactive events visualization, which has been exploited in a proxy-based mobile Web usability evaluation tool; mobile"
paternò2017,Customizable automatic detection of bad usability smells in mobile accessed web applications,"Paternò, Fabio; Schiavone, Antonio Giovanni; Conti, Antonio",2017,conferencePaper,,,,Proceedings of the 19th international conference on human-computer interaction with mobile devices and services,MOBILEHCI,ACM,10.1145/3098279.3098558,https://doi.org/10.1145/3098279.3098558,"Remote usability evaluation enables the possibility of analysing users' behaviour in their daily settings. We present a method and an associated tool able to identify potential usability issues through the analysis of client-side logs of mobile Web interactions. Such log analysis is based on the identification of specific usability smells. We describe an example set of bad usability smells, and how they are detected. The tool also allows evaluators to add new usability smells not included in the original set. We also report on the tool use in analysing the usability of a real, widely used application accessed by forty people through their smartphones whenever and wherever they wanted.",Usability engineering; Human computer interaction; Mobile devices; WEB application; Automatic Detection; Odors; remote usability evaluation; Remote usability evaluations; Remote Usability Evaluation; Bad smells; Log analysis; Associated tool; usability bad smells; web mobile application log analysis; Usability Bad Smells; Web Mobile application log analysis; Client sides; Customizable,52,B,UX/usability issue detection,user interaction usability problem encounter,User interaction analysis,none,behavior patterns,User interactions,yes,usasbilty issue identification; log data analysis; user interactions; usability test; nan; associated tool able to identify potential usability issues through the analysis of client-side logs of mobile Web interactions; mobile
paul2020,Accessibility and usability analysis of Indian e-government websites,"Paul, Surjit; Das, Saini",2020,journalArticle,19,4,949 – 957,Universal Access in the Information Society,,Springer Nature,10.1007/s10209-019-00704-8,https://doi.org/10.1007/s10209-019-00704-8,"E-government provides a platform to deliver government services to stakeholders, and many countries have adopted e-government websites for good governance. Successful e-government implementation and service delivery depend on how the underlying information present in government websites is made accessible and usable to every individual. This article investigates the accessibility and usability of e-government sites in India. Our sample consists of 65 Indian e-government websites. The analysis was carried out using automatic evaluation tools. The results of the accessibility tests highlight the existence of accessibility issues based on Web Content Accessibility Guidelines 1.0 (WCAG 1.0) and WCAG 2.0. Usability tests also reveal that e-government websites give low priority to such aspects during website design and development. Hence, there is a need to improve the overall accessibility and usability of Indian e-government websites in order to improve the quality and in turn the E-Government Development Index rank of India. © 2019, Springer-Verlag GmbH Germany, part of Springer Nature.",Automatic evaluation tool; Design and Development; e-government; E-government implementation; Government services; Government websites; Service delivery; Usability analysis; Usability engineering; Web content accessibility guidelines; Web Design; Websites,51,Q2,UX/usability attributes evaluation,"usability metrics (efficiency,...)",Source code analysis (web/app crawling),none,descriptive metrics/visualizations,Source code,no,autoamted usability testing; web crawling; source code (DOM); online testing tool usage; nan; accessibility and usability of e-government sites in India; not mentioned
peng2022,MUBot: Learning to Test Large-Scale Commercial Android Apps like a Human,"Peng, Chao; Zhang, Zhao; Lv, Zhengwei; Yang, Ping",2022,conferencePaper,,,543-552,2022 IEEE International Conference on Software Maintenance and Evolution (ICSME),ICSME,IEEE,10.1109/ICSME55016.2022.00074,https://doi.org/10.1109/ICSME55016.2022.00074,"Automated GUI testing has been playing a key role to uncover crashes to ensure the stability and robustness of Android apps. Recent research has proposed random, search-based and model-based testing techniques for GUI event generation. In industrial practices, different companies have developed various GUI exploration tools such as Facebook Sapienz, WeChat WeTest and ByteDance Fastbot to test their products. However, these tools are bound to their predefined GUI exploration strategies and lack of the ability to generate human-like actions to test meaningful scenarios. To address these challenges, Humanoid is the first Android testing tool that utilises deep learning to imitate human behaviours and achieves promising results over current model-based methods. However, we find some challenges when applying Humanoid to test our sophisticated commercial apps such as infinite loops and low test coverage. To this end, we performed the first case study on the performance of deep learning techniques using commercial apps to understand the underlying reason of the current weakness of this promising method. Based on our findings, we propose MUBot (Multi-modal User Bot) for human-like Android testing. Our empirical evaluation reveals that MUBot has better performance over Humanoid and Fastbot, our in-house testing tool on coverage achieved and bug-fixing rate on commercial apps.",android testing; graphical user interface; deep learning,12,A,UI issue detection,,"Source code analysis (web/app crawling), Computer vision",deep learning,,"Source code, Screenshots/images",no,"GUI testing; GUI tree (source code) analysis; GUI tree (source code), UI screenshots (greyscale maps); model training; GUI info from screenshots (greyscale maps); ndroid testing tool that utilises deep learning to imitate human behaviours and achieves promising results over current model-based methods.; mobile"
poivet2023,The influence of conversational agents’ role and communication style on user experience,"Poivet, Remi; Lopez Malet, Mélanie; Pelachaud, Catherine; Auvray, Malika",2023,journalArticle,14,,,Frontiers in Psychology,,Frontiers Media SA,10.3389/fpsyg.2023.1266186,https://doi.org/10.3389/fpsyg.2023.1266186,"Conversational Agents (CAs) are characterized by their roles within a narrative and the communication style they adopt during conversations. Within computer games, users’ evaluation of the narrative is influenced by their estimation of CAs’ intelligence and believability. However, the impact of CAs’ roles and communication styles on users’ experience remains unclear. This research investigates such influence of CAs’ roles and communication styles through a crime-solving textual game. Four different CAs were developed and each of them was assigned to a role of either witness or suspect and to a communication style than can be either aggressive or cooperative. Communication styles were simulated through a Wizard of Oz method. Users’ task was to interact, through real-time written exchanges, with the four CAs and then to identify the culprit, assess the certainty of their judgments, and rank the CAs based on their conversational preferences. In addition, users’ experience was evaluated using perceptual measures (perceived intelligence and believability scales) and behavioral measures (including analysis of users’ input length, input delay, and conversation length). The results revealed that users’ evaluation of CAs’ intelligence and believability was primarily influenced by CAs’ roles. On the other hand, users’ conversational behaviors were mainly influenced by CAs’ communication styles. CAs’ communication styles also significantly determined users’ choice of the culprit and conversational preferences. Copyright © 2023 Poivet, Lopez Malet, Pelachaud and Auvray.",believability; conversational agents; human behaviors; human-machine interaction; verbal interactions,3,Q2,UX/usability attributes evaluation,"usability metrics (efficiency,...)","User interaction analysis, Conversational assistant",none,descriptive metrics/visualizations,User interactions,yes,game UX evaluation; user interaction analysis; user interactions; game test; nan; impact of CAs’ roles and communication styles on users’ experience; desktop
ponce2018,Deep learning for automatic usability evaluations based on images: A case study of the usability heuristics of thermostats,"Ponce, Pedro; Balderas, David; Peffer, Therese; Molina, Arturo",2018,journalArticle,163,,111-120,Energy and Buildings,,Elsevier,10.1016/j.enbuild.2017.12.043,https://doi.org/10.1016/j.enbuild.2017.12.043,"Thermostats are designed for increasing requirements on indoor thermal comfort. Nevertheless, they are critical devices for saving energy in buildings and households. However, when thermostats do not accomplish the usability requirements, the end-users do not save energy. Then, when a thermostat is designed or validated, one of the leading problems that must be tackled is the usability evaluation. Generally, the evaluation is based on usability heuristics that are done by experts and designers and involve a very complicated cycling process in which usability experts need to be included in the complete usability evaluation. On the other hand, there are several proposals for generating an automatic usability analysis that can be used by designers or end-users. However, they are limited by the methodologies that are implemented in the evaluation because usability evaluations necessitate a large amount of data abstraction, and the amount of processed information is enormous; As an alternative, Artificial Intelligence can help to solve this problem, especially machine learning techniques with deep learning capabilities that can reach a high level of data abstraction with a significant amount of information and implement an automatic usability evaluation based on images. Convolutional networks that are included in deep learning can classify complex problems, attain highly accurate results. This paper proposes to train a convolutional network with standard usability heuristics for evaluating usability, which is an easy method for evaluating usability in thermostats, based on images. The proposed automatic method gives excellent results for evaluating usability heuristics in the heuristic assigned. This paper provides a complete methodology, using deep learning, for automatically evaluating the usability heuristics of thermostats. (C) 2017 Elsevier B.V. All rights reserved.",Deep learning; Usability engineering; Automatic usability evaluation; Images; Thermostats; Artificial intelligence; Learning systems; Convolution; Usability evaluation; Machine learning techniques; Heuristic methods; Usability requirements; Indoor thermal comfort; Abstracting; Amount of information; Convolutional networks; Learning capabilities; Problem solving,25,Q1,UX/usability attributes evaluation,"usability metrics (efficiency,...)",Computer vision,deep learning,,Screenshots/images,no,"usability heuristics application; computer vision; images (thermostat screenshots); model training; detectnig elements from screenshots (images); train a convolutional network with standard usability heuristics for evaluating usability, which is an easy method for evaluating usability in thermostats, based on imag; thermostats"
poore2017,Operationalizing Engagement with Multimedia as User Coherence with Context,"Poore, Joshua C.; Webb, Andrea K.; Cunha, Meredith G.; Mariano, Laura J.; Chappell, David T.; Coskren, Mikaela R.; Schwartz, Jana L.",2017,journalArticle,8,1,95-107,IEEE Transactions on Affective Computing,,IEEE,10.1109/TAFFC.2015.2512867,https://doi.org/10.1109/TAFFC.2015.2512867,"Traditional approaches for assessing user engagement within multimedia environments rely on methods that are removed from the human computer interaction itself, such as surveys, interviews and baselined physiology. We propose a context coherence approach that operationalizes engagement as the amount of independent user variation that covaries in time with multimedia contextual events during unscripted interactions. This can address questions about the features of multimedia users are most engaged and how engaged users are without the need for prescribed interactions or baselining. We assessed the validity of this approach in a psychophysiological study. Forty participants played interactive video games. Intake and post-stimulus questionnaires collected subjective engagement reports that provided convergent and divergent validity criteria to evaluate our approach. Estimates of coherence between physiological variation and in-game contextual events predicted subjective engagement and added information beyond physiological metrics computed from baselines taken outside of the multimedia context. Our coherence metric accounted for task-dependent engagement, independent of predispositions; this was not true of a baselined physiological approach that was used for comparison. Our findings show compelling evidence that a context-sensitive approach to measuring engagement overcomes shortcomings of traditional methods by making best use of contextual information sampled from multimedia in time-series analyses.",Coherence; Context; electromyography; Engagement; eye-tracking; Games; human computer interaction; Human computer interaction; immersion; Measurement; Multimedia communication; physiology; Physiology; user experience,4,Q1,UX/usability attributes evaluation,user engagement,"User interaction analysis, Physiological signal analysis, Eyetracking",machine learning,,"User interactions, gaze data, physiological signals",yes,"automated user engagement detection; analysis of interactions, physiological data analysis; Physiological Data (ECG,...), user interactions, eye-tracking data; game test; nan; We propose a context coherence approach that operationalizes engagement as the amount of independent user variation that covaries in time with multimedia contextual events during unscripted interactions. This can address questions about the features of multimedia users are most engaged and how engaged users are without the need for prescribed interactions or baselining.; desktop"
prezenski2017,Predictive Cognitive Modelling of Applications,"Prezenski, Sabine; Bruechner, Dominik; Russwinkel, Nele",2017,conferencePaper,3,,165-171,International Conference on Human Computer Interaction Theory and Applications,HUCAPP,SCITEPRESS,10.5220/0006273301650171,https://doi.org/10.5220/0006273301650171,"This paper argues that important usability aspects of mobile applications can be automatically evaluated using computational cognitive models based on the cognitive architecture ACT-R. A tool incorporating cognitive models for specific tasks, users, applications and usability aspects is proposed. Explanations provided by the tool for usability flaws are based on simulations of cognitive mechanisms. A use-case of the tool is introduced, which is based on an ACT-R model that simulates how users search and select a specific target in a hierarchical android application and predicts efficiency and learnability for average users. The model has been empirically validated in four studies with two different applications. To fully automate the usability evaluation of the use-case, two basic requirements need to be fulfilled. First, the application and the cognitive model have to be connected. A tool called ACT-Droid acts as an interface between the Android application and the cognitive model. Second, the models knowledge of the world, which is application specific, has to be provided automatically by using an automated user interface analysation approach. Therefore, the open-source tool AppCrawler was extended to allow the extraction of the required information.",Usability; User interfaces; Usability engineering; Tools; Computer vision; Computer graphics; Tool; Android (operating system); Usability tests; Computation theory; ACT-R; Cognitive modelling; Usability Criteria; Usability Test; Cognitive Modelling,5,B,UX/usability attributes evaluation,"usability metrics (efficiency,...)",Source code analysis (web/app crawling),none,metrics + rule-based,Source code,no,automated usability evaluation; app element crawling; UI elements info; none; nan; model that simulates how users search and select a specific target in a hierarchical android application and predicts efficiency and learnability for average user; mobile
quade2014,Predicting task execution times by deriving enhanced cognitive models from user interface development models,"Quade, Michael; Halbrügge, Marc; Engelbrecht, Klaus-Peter; Albayrak, Sahin; Möller, Sebastian",2014,conferencePaper,,,139 – 148,Proceedings of the 2014 acm sigchi symposium on engineering interactive computing systems,EICS,ACM,10.1145/2607023.2607033,https://doi.org/10.1145/2607023.2607033,"Adaptive user interfaces (UI) offer the opportunity to adapt to changes in the context, but this also poses the challenge of evaluating the usability of many different versions of the resulting UI. Consequently, usability evaluations tend to become very complex and time-consuming. We describe an approach that combines model-based usability evaluation with development models of adaptive UIs. In particular, we present how a cognitive user behavior model can be created automatically from UI development models and thus save time and costs when predicting task execution times. With the help of two usability studies, we show that the resulting predictions can be further improved by using information encoded in the UI development models. Copyright © 2014 ACM 978-1-4503-2725-1/14/06.",Adaptive user interface; Automated usability evaluation; Behavioral research; Computer simulation; Computer systems; Human computer interaction; Model based development; Model-based usability evaluation; Simulation; Usability engineering; Usability evaluation; User behavior modeling; User interface development; User interfaces,12,B,UX/usability attributes evaluation,user effort/time,"Source code analysis (web/app crawling), User interaction analysis",deep learning,,"Source code, User interactions",yes,"task execution prediction; UI element analysis, interaction analysis; UI model, user interactions; usability test; nan; approach that combines model-based usability evaluation with development models of adaptive UIs. In particular, we present how a cognitive user behavior model can be created automatically from UI development models and thus save time and costs when predicting task execution times; desktop"
razzaq2023,A Hybrid Multimodal Emotion Recognition Framework for UX Evaluation Using Generalized Mixture Functions,"Razzaq, Muhammad Asif; Hussain, Jamil; Bang, Jaehun; Hua, Cam-Hao; Satti, Fahad Ahmed; Rehman, Ubaid Ur; Bilal, Hafiz Syed Muhammad; Kim, Seong Tae; Lee, Sungyoung",2023,journalArticle,23,9,,Sensors,,MDPI,10.3390/s23094373,https://doi.org/10.3390/s23094373,"Multimodal emotion recognition has gained much traction in the field of affective computing, human-computer interaction (HCI), artificial intelligence (AI), and user experience (UX). There is growing demand to automate analysis of user emotion towards HCI, AI, and UX evaluation applications for providing affective services. Emotions are increasingly being used, obtained through the videos, audio, text or physiological signals. This has led to process emotions from multiple modalities, usually combined through ensemble-based systems with static weights. Due to numerous limitations like missing modality data, inter-class variations, and intra-class similarities, an effective weighting scheme is thus required to improve the aforementioned discrimination between modalities. This article takes into account the importance of difference between multiple modalities and assigns dynamic weights to them by adapting a more efficient combination process with the application of generalized mixture (GM) functions. Therefore, we present a hybrid multimodal emotion recognition (H-MMER) framework using multi-view learning approach for unimodal emotion recognition and introducing multimodal feature fusion level, and decision level fusion using GM functions. In an experimental study, we evaluated the ability of our proposed framework to model a set of four different emotional states (Happiness, Neutral, Sadness, and Anger) and found that most of them can be modeled well with significantly high accuracy using GM functions. The experiment shows that the proposed framework can model emotional states with an average accuracy of 98.19% and indicates significant gain in terms of performance in contrast to traditional approaches. The overall evaluation results indicate that we can identify emotional states with high accuracy and increase the robustness of an emotion classification system required for UX measurement.",user experience; User interfaces; electroencephalography; Electroencephalography; Artificial Intelligence; Human computer interaction; Psychology; artificial intelligence; Speech recognition; procedures; Emotions; Learning; Algorithms; learning; algorithm; Emotion Recognition; human; Humans; Function evaluation; Users' experiences; emotion; physiology; recognition; Recognition; Emotion recognition; Audio-based; Audio-based emotion recognition; Decision fusioning; Feature fusioning; Generalized mixture function; Generalized mixtures; Multimodal emotion recognition; emotion recognition; audio-based emotion recognition; decision fusioning; feature fusioning; generalized mixture function,6,Q2,Emotion detection,,"Text analysis, Computer vision, Audio analysis, User interaction analysis",deep learning,,"Audio/speech, Video/facial expressions, User movements",yes,"emotion recognition; sentiment classification, computer vision; facial images, speech, body movements; video/speech/body data analysis; emotion detection from video and audio and body movemetns; hybrid multimodal emotion recognition (H-MMER) framework using multi-view learning approach for unimodal emotion recognition and introducing multimodal feature fusion level, and decision level fusion using GM functions; not mentioned"
rehse2024,User behavior mining: A research agenda,"Rehse, Jana-Rebecca; Abb, Luka; Berg, Gregor; Bormann, Carsten; Kampik, Timotheus; Warmuth, Christian",2024,journalArticle,,,,Business and Information Systems Engineering,,Springer Nature,10.1007/s12599-023-00848-1,https://doi.org/10.1007/s12599-023-00848-1,"Studying the behavior of users in software systems has become an essential task for software vendors who want to mitigate usability problems and identify automation potentials, or for researchers who want to test behavioral theories. One approach to studying user behavior in a data-driven way is through the analysis of so-called user interaction (UI) logs, which record the low-level activities that a user performs while executing a task. In the paper, the authors refer to the analysis of UI logs as User Behavior Mining (UBM) and position it as a research topic. UBM is conceptualized by means of a four-component framework that elaborates how UBM data can be captured, which technologies can be applied to analyze it, which objectives UBM can accomplish, and how theories can guide the analytical process. The applicability of the framework is demonstrated by three exemplary applications from an ongoing research project with a partner company. Finally, the paper discusses practical challenges to UBM and derives an agenda for potential future research directions.",Process mining; Robotic process automation; UI logs; User behavior mining,1,Q1,UX/usability issue detection,user interaction usability problem encounter,User interaction analysis,none,behavior patterns,User interactions,yes,"behavior mining; real usage user interaction analysis; user interactions (UI logs); none; nan; component framework that elaborates how UBM data can be captured, which technologies can be applied to analyze it, which objectives UBM can accomplish, and how theories can guide the analytical process; not mentioned"
ribeiro2019,Usability problems discovery based on the automatic detection of usability smells,"Ribeiro, Rafael Fontinele; de Meneses Campanhã Souza, Matheus; de Oliveira, Pedro Almir Martins; de Alcântara dos Santos Neto, Pedro",2019,conferencePaper,,,2328–2335,Proceedings of the 34th ACM/SIGAPP Symposium on Applied Computing,SAC,ACM,10.1145/3297280.3299747,https://doi.org/10.1145/3297280.3299747,"Web applications usage has gone through a big and fast growth in the past few years, becoming part of our daily lives. Large companies use these applications to provide their services. So, it is necessary to ensure the development of high-quality applications. One of the main attributes of a Web application that directly determines its success or failure is their usability, and several methods were developed to evaluate it. The most popular method of usability evaluation is the laboratory testing because it can directly evaluate users' reactions while interacting with the final application. However, performing this test entails high cost and complexity. So, aiming to simplify the execution of this evaluation and to ease the discovery of usability problems, this work proposes an approach to detect indicators of problems (usability smells), based on the capture of the user interaction in the production environment and automatic analysis of this interaction. A study carried out with the approach showed that smells indicated by it were able to aid the detection and correction of usability problems in a real application.",Usability; usability; Usability engineering; usability evaluation; refactoring; Usability evaluation; Laboratory testing; Usability problems; Automatic Detection; Refactorings; Odors; usability smells; Production environments; Usability smells; web; Web,6,B,UX/usability issue detection,user interaction usability problem encounter,User interaction analysis,none,metrics + rule-based,User interactions,yes,"automated usability testing; real usage logs analysis; user interactions; usabiolity test; nan; an approach to detect indicators of problems (usability smells), based on the capture of the user interaction in the production environment and automatic analysis of this interactio; not mentioned"
robal2017,Ontology Design for Automatic Evaluation of Web User Interface Usability,"Robal, Tarmo; Marenkov, Jevgeni; Kalja, Ahto",2017,conferencePaper,,,1-8,2017 Portland International Conference on Management of Engineering and Technology,PICMET,IEEE,10.23919/PICMET.2017.8125425,https://doi.org/10.23919/PICMET.2017.8125425,"The rapid development of the Internet and associated technologies for content delivery has led to a situation where the Web can be accessed on a multitude of different platforms - from desktop computers, laptops and tablets to smart-phones, which have become a crucial part of our lives, raising the question of usability on this plethora of devices. Testing and validating user experience (UX) throughout the whole development process is costly. However, some of this work done by humans could be executed automatically starting in early development. In this paper we address web user interface (UI) automatic evaluation and in particular discuss the ontology design for capturing knowledge of web usability domain for UI evaluation.",Usability; Testing; User interfaces; Tools; Guidelines; Ontologies; Automatic evaluation; Development process; Websites; Ontology; Interactive devices; Smartphones; Web usability; User experiences (ux); Personal computers; Web user interface; Content delivery; Ontology design; UI evaluations,14,A,UX/usability issue detection,guidelines evaluation,Source code analysis (web/app crawling),none,metrics + rule-based,Source code,no,"guidelines evaluation; UI inspection, web scraping; UI elements (DOM); none; nan; evaluate user interfaces’ cross-platform compatibility and usability during production-time with minimal cost and time; not mentioned"
salomón2023,Towards automatic evaluation of the Quality-in-Use in context-aware software systems,"Salomón, Sergio; Duque, Rafael; Montaña, José Luis; Tenés, Luis",2023,journalArticle,14,8,10321 – 10346,Journal of Ambient Intelligence and Humanized Computing,,Springer Nature,10.1007/s12652-021-03693-w,https://doi.org/10.1007/s12652-021-03693-w,"Context-aware systems adapt their services to the user’s intentions and environment to improve the user experience. However, how to evaluate the quality of these systems in terms of user perception and context recognition is still an open problem. Our goal in this work is to evaluate the Quality-in-Use (QinU) for context-aware software systems according to the ISO/IEC 25010 standard and in an automated manner. This evaluation is oriented to be model-based, with domain specification and log data as input, while quality metrics and representations of users’ behavior as output. In this process, we use probabilistic models to discover user patterns, heuristic metrics as QinU estimation, clustering techniques to obtain user profiles according to their QinU, and feature selection to identify relevant factors of context. We propose a framework for assessing the QinU in context-aware software systems called Framework for Assessing Quality-in-use of Software (FAQuiS). FAQuiS includes a set of models to represent all dimensions of context, a methodology to apply the quality analysis to any system, and a set of tools and metrics to support and automate the process. We seek to test the impact and ease of integration in the industry for this framework. A case study in a company allows us to validate the applicability in a real environment. We analyze the mechanisms that support the QinU evaluation in context-aware systems, the feasibility of the QinU quantification, and the suitability of the integration in companies. Compared to previous works, our proposal offers a novel data-driven approach with general-purpose and industrial viability. FAQuiS can be used as a solution to assess the QinU based on the ISO 25010 standard and the models of user behaviors in different contexts. This solution analyzes the context changes in the user interaction, can quantify the quality loss in these contexts, and does not require big efforts to be integrated into a software development process. © 2022, The Author(s).",Automatic evaluation; Behavioral research; Computer software selection and evaluation; Context models; Context-Aware; Context-aware systems; In contexts; Interaction analysis; ISO Standards; Quality assurance; Quality control; Quality in use; Software design; Software quality assurance; Software-systems; User behaviors; User profile,9,Q1,UX/usability attributes evaluation,"usability metrics (efficiency,...)",User interaction analysis,none,descriptive metrics/visualizations,User interactions,yes,quality of use evaluation; real usage logs analysis; user interactions; case study on real usage data; nan; a framework for assessing the QinU in context-aware software systems called Framework for Assessing Quality-in-use of Software (FAQuiS); mobile
sanchis-font2021,Cross-Domain Polarity Models to Evaluate User eXperience in E-learning,"Sanchis-Font, Rosario; Castro-Bleda, Maria Jose; Gonzalez, Jose-Angel; Plal, Ferran; Hurtado, Lluis-F",2021,journalArticle,53,5,3199-3215,Neural Processing Letters,,Springer Nature,10.1007/s11063-020-10260-5,https://doi.org/10.1007/s11063-020-10260-5,"Virtual learning environments are growing in importance as fast as e-learning is becoming highly demanded by universities and students all over the world. This paper investigates how to automatically evaluate User eXperience in this domain using sentiment analysis techniques. For this purpose, a corpus with the opinions given by a total of 583 users (107 English speakers and 476 Spanish speakers) about three learning management systems in different courses has been built. All the collected opinions were manually labeled with polarity information (positive, negative or neutral) by three human annotators, both at the whole opinion and sentence levels. We have applied our state-of-the-art sentiment analysis models, trained with a corpus of a different semantic domain (a Twitter corpus), to study the use of cross-domain models for this task. Cross-domain models based on deep neural networks (convolutional neural networks, transformer encoders and attentional BLSTM models) have been tested. In order to contrast our results, three commercial systems for the same task (MeaningCloud, Microsoft Text Analytics and Google Cloud) were also tested. The obtained results are very promising and they give an insight to keep going the research of applying sentiment analysis tools on User eXperience evaluation. This is a pioneering idea to provide a better and accurate understanding on human needs in the interaction with virtual learning environments and a step towards the development of automatic tools that capture the feed-back of user perception for designing virtual learning environments centered in user's emotions, beliefs, preferences, perceptions, responses, behaviors and accomplishments that occur before, during and after the interaction.",Deep learning; User experience; Machine learning; Semantics; Convolutional neural networks; Artificial neural networks; Learning systems; Sentiment analysis; Deep neural networks; E-learning; Behavioral research; State of the art; Computer aided instruction; User experience evaluations; User perceptions; Analysis techniques; Commercial systems; Cross-domain models; Learning management system; Virtual learning environments; Learning management systems,16,Q3,UX/usability attributes evaluation,sentiment,Text analysis,deep learning,,User feedback/reviews/text,yes,UX evaluation; text analysis; user reviews; none; sentiment analysis; automatically evaluate User eXperience in this domain using sentiment analysis techniques; -
sanchis-font2019,Applying Sentiment Analysis with Cross-Domain Models to Evaluate User eXperience in Virtual Learning Environments,"Sanchis-Font, Rosario; Castro-Bleda, Maria Jose; González, José-Ángel",2019,conferencePaper,11506 LNCS,,609 – 620,International Work-Conference on Artificial Neural Networks,IWANN,Springer Nature,10.1007/978-3-030-20521-8_50,https://doi.org/10.1007/978-3-030-20521-8_50,"Virtual Learning Environments are growing in importance as fast as e-learning is becoming highly demanded by universities and students all over the world. This paper investigates how to automatically evaluate User eXperience in this domain. Two Learning Management Systems have been evaluated, one system is an ad-hoc system called “Conecto” (in Spanish and English languages), and the other one is an open-source Moodle personalized system (in Spanish). We have applied machine learning tools to all the comments given by a total of 133 users (37 English speakers and 96 Spanish speakers) to obtain their polarity (positive, negative, or neutral) using cross-domain models trained with a corpus of a different domain (tweets for each language) and general models for the language. The obtained results are very promising and they give an insight to keep going the research of applying sentiment analysis tools on User eXperience evaluation. This is a pioneering idea to provide a better and accurate understanding on human needs in the interaction with Virtual Learning Environments. The ultimate goal is to develop further tools of automatic feed-back of user perception for designing Virtual Learning Environments centered in user’s emotions, beliefs, preferences, perceptions, responses, behaviors and accomplishments that occur before, during and after the interaction. © 2019, Springer Nature Switzerland AG.",User eXperience; User experience; Machine learning; Virtual reality; Neural networks; Learning systems; Data mining; Sentiment analysis; E-learning; Behavioral research; Computer aided instruction; Open systems; User experience evaluations; English languages; Cross-domain models; Learning management system; Virtual learning environments; Applied machine learning; Polarity; Learning Management Systems; Virtual Learning Environments,8,B,UX/usability attributes evaluation,difficulty/demand,Text analysis,deep learning,,User feedback/reviews/text,yes,UX evaluation; text analysis; user reviews; none; sentiment analysis; applying sentiment analysis tools on User eXperience evaluation; -
santos2022,A Framework to Semi-automated Usability Evaluations Processing Considering Users' Emotional Aspects,"Santos, Flavia de Souza; Treviso, Marcos Vinicius; Gama, Sandra Pereira; de Mattos Fortes, Renata Pontin",2022,conferencePaper,13302,,419-438,International Conference on Human-Computer Interaction,HCII,Springer Nature,10.1007/978-3-031-05311-5_29,https://doi.org/10.1007/978-3-031-05311-5_29,"The concern with providing a good experience for users increases simultaneously with technological evolution and dissemination. Different evaluation methods are presented in the literature to help developers evaluate and verify the interfaces they develop. However, for the application of evaluation methods, it is often necessary to have an expert, or users, which can make the assessment costly, both time-consumingly and monetarily. Consequently, developers may launch their products without carefully checking some critical aspects beforehand, causing anything from the non-acceptance of the technology to even its abandonment. Carrying out part of the evaluations with users, or totally with them, in an automated way, considering the diversity of data obtained and the support of analyzes processed by a computer, presents itself as a possible alternative to be investigated to support developers of interactive systems. At the same time, considering the emotional aspects during the interaction can provide the developer with valuable information, including the acceptance of the technology. In this way, considering users' emotions can improve the automatic evaluation results, capturing and processing the user experience in the system. Thus, we present a framework (EmotiUsing) composed of semi-automated usability evaluations, considering the emotional aspects of users. The framework aims to make the analytical evaluations less subjective and streamline the evaluation process, reducing costs and time for the evaluators.",User experience; User interfaces; Automation; Automated usability evaluation; Usability evaluation; Emotions; Semi-automated support; Interactive system; Evaluation methods; Automated support; Emotion; Emotional aspect; Technological evolution; User emotions,1,B,Emotion detection,,"User interaction analysis, Physiological signal analysis",machine learning,,"User interactions, physiological signals",yes,"emotion detection; user interactions analysis, physio data analysis; brain signals, user interactions, DOM data; udsability test; usability smell detection from interactions/signals; framework (EmotiUsing) composed of semi-automated usability evaluations, considering the emotional aspects of users; not mentioned"
schoop2022,Predicting and Explaining Mobile UI Tappability with Vision Modeling and Saliency Analysis,"Schoop, Eldon; Zhou, Xin; Li, Gang; Chen, Zhourong; Hartmann, Bjoern; Li, Yang",2022,conferencePaper,,,1–21,Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems,CHI,ACM,10.1145/3491102.3517497,https://doi.org/10.1145/3491102.3517497,"UI designers often correct false affordances and improve the discoverability of features when users have trouble determining if elements are tappable. We contribute a novel system that models the perceived tappability of mobile UI elements with a vision-based deep neural network and helps provide design insights with dataset-level and instance-level explanations of model predictions. Our system retrieves designs from similar mobile UI examples from our dataset using the latent space of our model. We also contribute a novel use of an interpretability algorithm, XRAI, to generate a heatmap of UI elements that contribute to a given tappability prediction. Through several examples, we show how our system can help automate elements of UI usability analysis and provide insights for designers to iterate their designs. In addition, we share findings from an exploratory evaluation with professional designers to learn how AI-based tools can aid UI design and evaluation for tappability issues.",Mobile UIs; Deep Learning; Explainable AI; Interpretability,22,A,Saliency/visual importance,,Computer vision,deep learning,,Screenshots/images,no,"tappability detection  - saliency; computer vision; UI screenshots; mdel training; perceived tappability from images; deep learning based approach to predict whether a selected element in a mobile UI screenshot will be perceived by users as tappable, based on pixels only instead of view hierarchies required by previous work ; mobile"
shen2015,Predicting Eye Fixations on Webpage With an Ensemble of Early Features and High-Level Representations from Deep Network,"Shen, Chengyao; Huang, Xun; Zhao, Qi",2015,journalArticle,17,11,2084-2093,IEEE Transactions on Multimedia,,IEEE,10.1109/TMM.2015.2483370,https://doi.org/10.1109/TMM.2015.2483370,"In recent decades, webpages are becoming an increasingly important visual information source. Compared with natural images, webpages are different in many ways. For example, webpages are usually rich in semantically meaningful visual media (text, pictures, logos, and animations), which make the direct application of some traditional low-level saliency models ineffective. Besides, distinct web-viewing patterns such as top-left bias and banner blindness suggest different ways for predicting attention deployment on a webpage. In this study, we utilize a new scheme of low-level feature extraction pipeline and combine it with high-level representations from deep neural networks. The proposed model is evaluated on a newly published webpage saliency dataset with three popular evaluation metrics. Results show that our model outperforms other existing saliency models by a large margin and both low- and high-level features play an important role in predicting fixations on webpage.",Web Viewing; Webpage Saliency; Visual Attention; Deep Learning,54,Q1,Saliency/visual importance,,Computer vision,deep learning,,Screenshots/images,no,saliency prediction; computer vision; UI screenshots; eye tracking experiment; predicting gaze points from screenshots; a new scheme of low-level feature extraction pipeline and combine it with high-level representations from deep neural networks - our model outperforms other existing saliency models by a large margin and both low- and high-level features play an important role in predicting fixations on webpage; desktop
soleimani2017,What Can Self-Reports and Acoustic Data Analyses on Emotions Tell Us?,"Soleimani, Samaneh; Law, Effie Lai-Chong",2017,conferencePaper,,,489–501,Proceedings of the 2017 Conference on Designing Interactive Systems,DIS,ACM,10.1145/3064663.3064770,https://doi.org/10.1145/3064663.3064770,"There are two approaches to measure people's experiences: memory-based and moment-based. Whereas the memory-based approaches are susceptible to the peak-end effect, the moment-based approaches appear to better reflect the experience of the present. In this paper, we propose that emotional assessment of think aloud verbalisations is a moment-based methodology for measuring UX. We conducted an empirical study with 46 participants in the domain of online shopping to evaluate their emotional experiences. Acoustic analysis of verbal data was used as the moment-based approach and self-report questionnaires as the retrospective or memory-based approach. Results of the study confirmed the previous finding that retrospective assessments did not reflect the actual experience. The results also suggested that retrospective evaluations of emotions were significantly correlated with the most frequently elicited emotion (i.e. modal emotion) during the interaction. In conclusion these results support the use of acoustic data analysis as an alternative approach to measuring UX.",User experience; evaluation; emotion; acoustic analysis,18,A,Emotion detection,,Audio analysis,machine learning,,Audio/speech,yes,emotion detection; speech analysis (emotions); voice; usability testing (lab); nan; emotional assessment of think aloud verbalisations is a moment-based methodology for measuring UX; mobile
soure2022,CoUX: Collaborative Visual Analysis of Think-Aloud Usability Test Videos for Digital Interfaces,"Soure, Ehsan Jahangirzadeh; Kuang, Emily; Fan, Mingming; Zhao, Jian",2022,journalArticle,28,1,643-653,IEEE Transactions on Visualization and Computer Graphics,,IEEE,10.1109/TVCG.2021.3114822,https://doi.org/10.1109/TVCG.2021.3114822,"Reviewing a think-aloud video is both time-consuming and demanding as it requires UX (user experience) professionals to attend to many behavioral signals of the user in the video. Moreover, challenges arise when multiple UX professionals need to collaborate to reduce bias and errors. We propose a collaborative visual analytics tool, CoUX, to facilitate UX evaluators collectively reviewing think-aloud usability test videos of digital interfaces. CoUX seamlessly supports usability problem identification, annotation, and discussion in an integrated environment. To ease the discovery of usability problems, CoUX visualizes a set of problem-indicators based on acoustic, textual, and visual features extracted from the video and audio of a think-aloud session with machine learning. CoUX further enables collaboration amongst UX evaluators for logging, commenting, and consolidating the discovered problems with a chatbox-like user interface. We designed CoUX based on a formative study with two UX experts and insights derived from the literature. We conducted a user study with six pairs of UX practitioners on collaborative think-aloud video analysis tasks. The results indicate that CoUX is useful and effective in facilitating both problem identification and collaborative teamwork. We provide insights into how different features of CoUX were used to support both independent analysis and collaboration. Furthermore, our work highlights opportunities to improve collaborative usability test video analysis.",Acoustics; collaboration; Collaboration; Feature extraction; machine learning; Machine learning; think-aloud; Tools; Usability; usability problems; User experience; video analysis; Videos; visual analytics,22,Q1,UX/usability issue detection,guidelines evaluation,"Audio analysis, Computer vision, Text analysis",NLP,,"Audio/speech, Video/facial expressions",yes,"thinkaloud deep analysis; video analysis, audio analysis; video recordings, audio recordings; usability testing; nan; collaborative visual analytics tool, CoUX, to facilitate UX evaluators collectively reviewing think-aloud usability test videos of digital interfaces; mobile"
souza2019,User Experience Evaluation Using Mouse Tracking and Artificial Intelligence,"Souza, Kennedy E. S.; Seruffo, Marcos C. R.; De Mello, Harold D.; Souza, Daniel Da S.; Vellasco, Marley M. B. R.",2019,journalArticle,7,,96506 – 96515,IEEE Access,,IEEE,10.1109/ACCESS.2019.2927860,https://doi.org/10.1109/ACCESS.2019.2927860,"Business platform models frequently require continuous adaptation and agility to allow new experiences to be created and delivered to customers. To understand user behavior in online systems, researchers have taken advantage of a combination of traditional and recently developed analysis techniques. Earlier studies have shown that user behavior monitoring data, as obtained by mouse tracking, can be utilized to improve user experience (UX). Many mouse-tracking solutions exist; however, the vast majority is proprietary, and open-source packages do not provide the resources and data needed to support UX research. Thus, this paper presents: 1) the development of an interaction monitoring application titled Artificial Intelligence and Mouse Tracking-based User eXperience Tool (AIMT-UXT); 2) the validation of the tool in a case study conducted on the Website of the Brazilian Federal Revenue Service (BFR); 3) the definition of a new relationship pattern of variables that determine user behavior; 4) the construction of a fuzzy inference system for measuring user performance using the defined variables and the data captured in the case study; and 5) the application of a clustering algorithm to complement the analysis. A comparison of the results of the applied quantitative methodologies indicates that the developed framework was able to infer UX scores similar to those reported by users in questionnaires. © 2013 IEEE.",Usability; User experience; User interfaces; Tools; Surveys; Computer science; Artificial intelligence; Monitoring; artificial intelligence; Clustering algorithms; ergonomics; Fuzzy logic; Ergonomics; Behavioral research; Mice; Online systems; Fuzzy inference; Fuzzy inference systems; User experience evaluations; Mammals; User experiences (ux); Analysis techniques; Inference engines; Interaction monitoring; Open source package; Quantitative methodology; Taxation; User behavior monitoring; computer science,42,Q1,UX/usability attributes evaluation,"usability metrics (efficiency,...)",User interaction analysis,none,metrics + rule-based,User interactions,yes,"usability evaluation; rule based system for user interaction analysis; user interactions, task results; usability testing; fuzzy score clustering to predict SUS scores; interaction monitoring application titled Artificial Intelligence and Mouse Tracking-based User eXperience Too; desktop"
souza2022,"An Evaluation Framework for User Experience Using Eye Tracking, Mouse Tracking, Keyboard Input, and Artificial Intelligence: A Case Study","Souza, Kennedy Edson Silva de; Aviz, Igor Leonardo de; Mello, Harold Dias de; Figueiredo, Karla; Vellasco, Marley Maria Bernardes Rebuzzi; Costa, Fernando Augusto Ribeiro; Seruffo, Marcos Cesar da Rocha",2022,journalArticle,38,7,646-660,International Journal of Human-Computer Interaction,,Taylor & Francis,10.1080/10447318.2021.1960092,https://doi.org/10.1080/10447318.2021.1960092,"User eXperience (UX) has been used to achieve improvements in digital information systems based on how people perceive them. In particular, this paper establishes a framework that employs methods for eye and mouse tracking, keyboard input, self-assessment questionnaire and artificial intelligence algorithms to evaluate user experience and categorize users in terms of performance profiles. The results obtained with this framework are artifacts that can be used to support customizations of the User Interface (UI) on the websites. Moreover, the established framework is generic and flexible and can be applied to any information system, such as the case study shown in the website of the Federal Revenue of Brazil (RFB). The main objectives of this paper are as follows: (i) to set out a powerful UX framework based on three tracking techniques - the AIT2-UX; (ii) to provide the T2-UXT to collect, collate, process and visualize data obtained from users' interactions (iii) to use and compare machine learning algorithms with the classification of user performance profiles; (iv) to use the artifacts generated by the framework to manually customize the UI with the website.",Eye tracking; User experience; Machine learning; User interfaces; Information systems; Learning algorithms; Information use; Websites; Evaluation framework; Mammals; User experiences (ux); Artificial intelligence algorithms; Digital information systems; Performance profile; Self assessment; Tracking techniques; User performance,18,Q2,UX/usability attributes evaluation,"usability metrics (efficiency,...)","User interaction analysis, Eyetracking","deep learning, machine learning",,"User interactions, gaze data",yes,"usability evaluation; user interaction analysis; user interactions, eyetracking data; usability testing; experience (knowledge) with a system classification from intearctions; a framework that employs methods for eye and mouse tracking, keyboard input, self-assessment questionnaire and artificial intelligence algo-rithms to evaluate user experience and categorize users in terms of performance profiles; desktop"
speicher2014,Ensuring Web Interface Quality through Usability-Based Split Testing,"Speicher, Maximilian; Both, Andreas; Gaedke, Martin",2014,conferencePaper,,,93-110,Web Engineering: 14th International Conference,ICWE,Springer Nature,10.1007/978-3-319-08245-5_6,https://doi.org/10.1007/978-3-319-08245-5_6,"Usability is a crucial quality aspect of web applications, as it guarantees customer satisfaction and loyalty. Yet, effective approaches to usability evaluation are only applied at very slow iteration cycles in today’s industry. In contrast, conversion-based split testing seems more attractive to e-commerce companies due to its more efficient and easy-to-deploy nature. We introduce Usability-based Split Testing as an alternative to the above approaches for ensuring web interface quality, along with a corresponding tool called WaPPU. By design, our novel method yields better effectiveness than using conversions at higher efficiency than traditional evaluation methods. To achieve this, we build upon the concept of split testing but leverage user interactions for deriving quantitative metrics of usability. From these interactions, we can also learn models for predicting usability in the absence of explicit user feedback. We have applied our approach in a split test of a real-world search engine interface. Results show that we are able to effectively detect even subtle differences in usability. Moreover, WaPPU can learn usability models of reasonable prediction quality, from which we also derived interaction-based heuristics that can be instantly applied to search engine results pages.",Usability; Metrics; Heuristics; Interaction Tracking; Search Engines; Interfaces; Context-Awareness,47,B,UX/usability attributes evaluation,"usability metrics (efficiency,...)",User interaction analysis,machine learning,,User interactions,yes,usability evaluation; user interaction analysis; user interactions (mouse); usability testing; nan; split testing but leverage user interactions for deriving quantitative metrics of usability; desktop
stefancova2018,Towards detection of usability issues by measuring emotions,"Stefancova, Elena; Moro, Robert; Bielikova, Maria",2018,conferencePaper,909,,63 – 70,New Trends in Databases and Information Systems,ADBIS,Springer Nature,10.1007/978-3-030-00063-9_8,https://doi.org/10.1007/978-3-030-00063-9_8,"User Experience is one of the most important criteria when designing and testing user interfaces with emotions as its essential element. To assess, how emotions could be used for automatic detection of usability issues, we carried out a user study with a website which included intentionally inserted usability issues. We classified valence of emotions, i.e., negative vs. positive ones based on data from electroencephalography (EEG) and facial expressions recognition. The study results confirmed that usability issues cause negative emotional response of the user and that presence of a negative emotion is a good predictor of a usability issue presence. When detecting negative and positive emotional states from the acquired dataset, we achieved the accuracy of 94% for samples with seconds granularity and 70% for the task granularity. © Springer Nature Switzerland AG 2018.",Usability; User interfaces; Usability engineering; Electroencephalography; Data analysis; Information systems; Emotions; Information use; Data reduction; Automatic Detection; Electrophysiology; Facial Expressions; EEG; Emotional response; Essential elements; Facial expressions recognition; Negative emotions; Facial expressions,6,B,Emotion detection,,"Physiological signal analysis, Computer vision",machine learning,,"Video/facial expressions, physiological signals",yes,"emotion detection; computer vision, physiological signal analysis; EEG data, facial expressions; usability testing; nan; classified valence of emotions, i.e., negative vs. positive ones based on data from electroencephalography (EEG) and facial expressions recognition; not mentioned"
su2021,OwlEyes-online: a fully automated platform for detecting and localizing UI display issues,"Su, Yuhui; Liu, Zhe; Chen, Chunyang; Wang, Junjie; Wang, Qing",2021,conferencePaper,,,1500–1504,Proceedings of the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering,ESEC/FSE,ACM,10.1145/3468264.3473109,https://doi.org/10.1145/3468264.3473109,"Graphical User Interface (GUI) provides visual bridges between software apps and end users. However, due to the compatibility of software or hardware, UI display issues such as text overlap, blurred screen, image missing always occur during GUI rendering on different devices. Because these UI display issues can be found directly by human eyes, in this paper, we implement an online UI display issue detection tool OwlEyes-Online, which provides a simple and easy-to-use platform for users to realize the automatic detection and localization of UI display issues. The OwlEyes-Online can automatically run the app and get its screenshots and XML files, and then detect the existence of issues by analyzing the screenshots. In addition, OwlEyes-Online can also find the detailed area of the issue in the given screenshots to further remind developers. Finally, OwlEyes-Online will automatically generate test reports with UI display issues detected in app screenshots and send them to users. The OwlEyes-Online was evaluated and proved to be able to accurately detect UI display issues. Tool Link: http://www.owleyes.online:7476 Github Link: https://github.com/franklinbill/owleyes Demo Video Link: https://youtu.be/002nHZBxtCY",UI display; Mobile app; UI testing; Deep learning; Issue detection,14,A,UI issue detection,,Computer vision,deep learning,,Screenshots/images,no,"UI display issue detection; computer vision; GUI screenshot; model training; detecting issues on screenshots; online UI display issue detection tool OwlEyes-Online, which provides a simple and easy-to-use platform for users to realize the automatic detection and localization of UI display issues; mobile"
tapia2022,A Process to Support the Remote Tree Testing Technique for Evaluating the Information Architecture of User Interfaces in Software Projects,"Tapia, Alejandro; Moquillaza, Arturo; Aguirre, Joel; Falconi, Fiorella; Lecaros, Adrian; Paz, Freddy",2022,conferencePaper,13321 LNCS,,75 – 92,International Conference on Human-Computer Interaction,HCII,Springer Nature,10.1007/978-3-031-05897-4_6,https://doi.org/10.1007/978-3-031-05897-4_6,"Nowadays, due to technological advancement, people are bound to use multiple digital tools and interact with them through a User Interface. For this reason, User Experience (UX) is one of the most important keys to success. UX includes the design and evaluation of an adequate Information Architecture. To design Information Architecture, the best-known technique is Card Sorting. For evaluation, there is the Tree Testing or Reverse Card Sorting technique. This technique can be applied in remote or non-remote ways. We identified three main issues: 1. The remote processes to apply Tree Testing are not standardized; 2. the tools were modeled and built after a process defined by the supplier of those tools, most of which are now discontinued. 3. In many development projects these techniques are left aside, the Information Architecture is assessed using other methods and techniques (i.e. User Testing) in later phases of the project. These result in Information Architecture errors and defects found late or not found at all. Hence, this study is focused on developing and validating a standardized remote Tree Testing process with an emphasis on automation. To meet the objectives, we conducted interviews and literature reviews to find out how Tree Testing is currently carried out and what tools are used to support the technique (AS-IS workflow). Subsequently, we proposed a process that can support the technique, considering the pain points found on the current processes, resulting in a TO-BE workflow. As a result, a proposal of this standardized process was modeled using the notation BPMN, and validated by the expert judgment of specialists in Human-Computer Interaction (HCI). Finally, it is important to mention that Tree Testing is useful, practical in its remote application, and should be applied in the early stages of software projects; a tool to support the whole process should be implemented. © 2022, The Author(s), under exclusive license to Springer Nature Switzerland AG.",Application programs; Architecture information; BPMN; Card-sorting; Digital devices; Forestry; Human computer interaction; Information architectures; Information management; Information retrieval; Software project; Software testing; Technological advancement; Testing; Testing technique; Tree testing; User interfaces; Users' experiences; Work-flows,6,B,UX/usability issue detection,user interaction usability problem encounter,User interaction analysis,none,descriptive metrics/visualizations,User interactions,yes,"tree testing process; user interaction analysis; task results (user interactions); tree testing (usability testing); nan; we proposed a process that can support the Tree testing technique, considering the pain points found on the current processes, resulting in a TO-BE workflow; not mentioned"
vigo2017,Real-time detection of navigation problems on the World `Wild' Web,"Vigo, Markel; Harper, Simon",2017,journalArticle,101,,1-9,International Journal of Human-Computer Studies,,Elsevier,10.1016/j.ijhcs.2016.12.002,https://doi.org/10.1016/j.ijhcs.2016.12.002,"We propose a set of algorithms to detect navigation problems in real-time. To do so, we operationalise some navigation strategies suggested by the literature and investigate the extent to which the exhibition of these strategies is an indicator of navigation problems. Our Firefox extension senses behaviour indicative of a user experiencing interaction problems. Once these problems are detected we can suggest changes to these sites, and eventually adapt the site in real time to better accommodate the user. A remote longitudinal study monitored real website user behaviour, analysing every application event on the client side both individually and in combination. The study was conducted with 34 participants over 400 days totalling 567 h of normal usage and with no task restriction. Our sensing algorithms detected 374 issues with a 85% precision for purposeful Web use, suggesting that, indeed, when users search for specific information the exhibition of these strategies indicates the presence of problems. This contribution is novel in that, as opposed to a post-hoc analysis of user interaction, real-time detection of navigation problems at the user end opens up new research avenues in the realm of adaptive interfaces and usability analysis.",Graphical user interfaces; Navigation; User interfaces; Signal detection; Usability testing; Behavioral research; Navigation strategies; Specific information; Interaction problems; Real-time detection; Sensing algorithms; Web interactions; Web usage; Automated usability testing; Web interaction,25,Q2,UX/usability issue detection,user interaction usability problem encounter,User interaction analysis,none,metrics + rule-based,User interactions,yes,web use problem detection; user interaction analysis; user interactions; usability test; nan; set of algorithms to detect navigation problems in real-time; desktop
villamane2024,Facilitating and automating usability testing of educational technologies,"Villamane, Mikel; Alvarez, Ainhoa",2024,journalArticle,32,3,,Computer Applications in Engineering Education,,Wiley,10.1002/cae.22725,https://doi.org/10.1002/cae.22725,"Usability evaluation is a key element to ensure a positive user experience with any software and it is especially important in educational software tools where there are many different actors involved (lecturers, students, administrators, etc.). However, evaluating usability is not an easy task for nonexpert evaluators. To facilitate this evaluation task, this article presents a Methodology for Usability Testing (MUT) and a system (CALMUT) that assists nonexpert evaluators in the application of the methodology by automatizing the calculations and facilitating their interpretation. This can be very useful for learning and instructional designers but also to people involved in the decision of introducing or not a new educational software. To develop the proposal, a literature review of different usability metrics, methods, and systems was carried out first, followed by a selection and adaptation for novice usability evaluators. This article also presents a case study where lecturers tested the usability of an educational software following the proposal and shows that using MUT and CALMUT helps people without previous experience detect the main usability problems of educational systems before deciding whether to use them or not.",Usability engineering; Software testing; Usability testing; Usability evaluation; usability testing; educational technologies; Case-studies; Users' experiences; Literature reviews; Key elements; Educational technology; Educational software; Instructional designer; Software-tools; Usability metrics,0,Q3,UX/usability attributes evaluation,"usability metrics (efficiency,...)",User interaction analysis,none,descriptive metrics/visualizations,User interactions,yes,"tool for automated usability testing; user interaction analysis; user interactions, task results; usability test; nan; Methodology for Usability Testing (MUT) and a system (CALMUT) that assists nonexpert evaluators in the application of the methodology by automatizing the calculations and facilitating their interpretation; desktop"
wang2020,ArguLens: Anatomy of Community Opinions on Usability Issues Using Argumentation Models,"Wang, Wenting; Arya, Deeksha; Novielli, Nicole; Cheng, Jinghui; Guo, Jin L.C.",2020,conferencePaper,,,,Conference on Human Factors in Computing Systems - Proceedings,CHI,ACM,10.1145/3313831.3376218,https://doi.org/10.1145/3313831.3376218,"In open-source software (OSS), the design of usability is often influenced by the discussions among community members on platforms such as issue tracking systems (ITSs). However, digesting the rich information embedded in issue discussions can be a major challenge due to the vast number and diversity of the comments. We propose and evaluate ArguLens, a conceptual framework and automated technique leveraging an argumentation model to support effective understanding and consolidation of community opinions in ITSs. Through content analysis, we anatomized highly discussed usability issues from a large, active OSS project, into their argumentation components and standpoints. We then experimented with supervised machine learning techniques for automated argument extraction. Finally, through a study with experienced ITS users, we show that the information provided by ArguLens supported the digestion of usability-related opinions and facilitated the review of lengthy issues. ArguLens provides the direction of designing valuable tools for high-level reasoning and effective discussion about usability. © 2020 ACM.",Argumentation model; Automated techniques; Conceptual frameworks; Content analysis; High-level reasoning; Human engineering; Issue Tracking; Learning systems; Open source software; Open systems; Supervised learning; Supervised machine learning; Usability engineering,35,A,UX/usability issue detection,topic classification,Text analysis,machine learning,,User feedback/reviews/text,yes,"UX issue detection; NLP; user reviews; text analysis, web scraping; nan; conceptual framework and automated technique leveraging an argumentation model to support effective understanding and consolidation of community opinions - information provided by ArguLens supported the digestion of usability-related opinions; -"
widodo2023,Enhancing Software User Interface Testing Through Few Shot Deep Learning: A Novel Approach for Automated Accuracy and Usability Evaluation,"Widodo, Aris Puji; Wibowo, Adi; Kurniawan, Kabul",2023,journalArticle,14,12,578 – 585,International Journal of Advanced Computer Science and Applications,,The Science and Information Organization,10.14569/IJACSA.2023.0141260,https://doi.org/10.14569/IJACSA.2023.0141260,"Traditional user interface (UI) testing methods in software development are time-consuming and prone to human error, requiring more efficient and accurate approaches. Moreover, deep learning requires extensive data training to develop accurate automated UI software testing. This paper proposes an efficient and accurate method for automating UI software testing using Deep learning with training data limitations. We propose a novel deep learning-based framework suitable for UI element analysis in data-scarce situations, focusing on Few-shot learning. Our framework initiates with several robust feature extraction modules that employ and compare sophisticated encoder models to be adept at capturing complex patterns from a sparse dataset. The methodology employs the Enrico and UI screen mistake datasets, overcoming training data limitations. Utilizing encoder models, including CNN, VGG-16, ResNet-50, MobileNet-V3, and EfficientNet-B1, the EfficientNet-B1 model excelled in the setting of Few-Shot learning with five-shot with an average accuracy of 76.05%. Our proposed model's accuracy was improved and compared to the state-of-the-art method. Our findings demonstrate the effectiveness of few-shot learning in UI screen classification, setting new benchmarks in software testing and usability evaluation, particularly in limited data scenarios. © (2023), (Science and Information Organization). All Rights Reserved.",Deep learning; User interfaces; Software testing; Training data; software testing; Learning systems; Usability evaluation; Software design; Interface testings; Software testings; Efficientnet; Few-shot; Screen classification; Signal encoding; User interface screen classification; User interface software; efficientnet; few-shot; UI screen classification,0,Q3,UI issue detection,,Computer vision,deep learning,,Screenshots/images,no,GUI testing (detecting UI issues); computer vision; UI screenshots; model training; detecting UI issues from screenshots; efficient and accurate method for automating UI software testing using Deep learning; mobile
wu2015,Modeling user psychological experience and case study in online e-learning,"Wu, Xiyuan; Liu, Min; Zheng, Qinghua; Zhang, Yunqiang; Li, Haifei",2015,journalArticle,10,6,53 – 61,International Journal of Emerging Technologies in Learning,,Kassel University Press GmbH,10.3991/ijet.v10i6.5114,https://doi.org/10.3991/ijet.v10i6.5114,"In the post WWW era, the research of e-learning focuses on facilitating intelligent and proactive services for learners. The quality of user experience determines whether e-learning services would be accepted by learners. However, many researchers traditionally focus on the effectiveness of computer systems or the accuracy of algorithms themselves rather than on user-centric psychological experience. How to model and evaluate user experience taking into account user psychological and cognitive properties are challenging research topics. There are some traditional methods typically proposed to evaluate users' psychological experience, such as interview, questionnaire etc. They are qualitative and easy to conduct but need more time and resource. And they are liable to subjective views. Based on user web log data, the current paper presents a quantitative approach of modeling user psychological experience in the context of intelligent e-learning. The properties and elements, which affect user experience, are analyzed and quantified. The holistic user experience is quantified through the fusion of analytic hierarchy process (AHP) and Delphi methods. A case study, at a university in China, is conducted for diagnosing whether the result of the proposed approach can be uniform with user subjective experience, and indicates that the proposed approach is effective and complements existing user experience research in intelligent e-learning.",Analytic hierarchy process; Analytic hierarchy process (ahp); Blogs; Cognitive properties; E-learning; Intelligent e-learning; Learning systems; Quantitative approach; Subjective experiences; Surveys; User experience; User experience research; User-centric evaluations; Web log analysis,5,Q3,UX/usability attributes evaluation,"usability metrics (efficiency,...)",User interaction analysis,none,metrics + rule-based,User interactions,yes,"UX evaluation; user interaction analysis; user interactions, task results; real usage logging; nan; Based on user web log data, the current paper presents a quantitative approach of modeling user psychological experience in the context of intelligent e-learning; desktop"
xiang2024,SimUser: Generating Usability Feedback by Simulating Various Users Interacting with Mobile Applications,"Xiang, Wei; Zhu, Hanfei; Lou, Suqi; Chen, Xinli; Pan, Zhenghua; Jin, Yuping; Chen, Shi; Sun, Lingyun",2024,conferencePaper,,,,Conference on Human Factors in Computing Systems - Proceedings,CHI,ACM,10.1145/3613904.3642481,https://doi.org/10.1145/3613904.3642481,"The confict between the rapid iteration demand of prototyping and the time-consuming nature of user tests has led researchers to adopt AI methods to identify usability issues. However, these AI-driven methods concentrate on evaluating the feasibility of a system, while often overlooking the infuence of specifed user characteristics and usage contexts. Our work proposes a tool named SimUser based on large language models (LLMs) with the Chain-of-Thought structure and user modeling method. It generates usability feedback by simulating the interaction between users and applications, which is infuenced by user characteristics and contextual factors. The empirical study (48 human users and 21 designers) validated that in the context of a simple smartwatch interface, SimUser could generate heuristic usability feedback with the similarity varying from 35.7% to 100% according to the user groups and usability category. Our work provides insights into simulating users by LLM to improve future design activities. © 2024 Copyright held by the owner/author(s)",Computational linguistics; Infuence; Iterative methods; Language model; Large language model; Mobile applications; Modeling languages; Thought structures; Usability feedback; Usage context; User characteristics; User interfaces; User simulation; User tests,0,A,UI understanding/feedback,,"Source code analysis (web/app crawling), Computer vision","New LMs, deep learning",LLM,"Source code, Screenshots/images",no (synthetic),"user feedback generation; computer vision, UI element analysis; UI screenshot, UI elements info; usability test; provide feedback for watch interface; tool named SimUser based on large language models (LLMs) with the Chain-of-Thought structure and user modeling method. It generates usability feedback by simulating the interaction between users and applications, which is influenced by user characteristics and contextual factors; watch"
xing2021,Computational model for predicting user aesthetic preference for GUI using DCNNs,"Xing, Baixi; Si, Huahao; Chen, Junbin; Ye, Minchao; Shi, Lei",2021,journalArticle,3,2,147-169,CCF Transactions on Pervasive Computing and Interaction,,Springer Nature,10.1007/s42486-021-00064-4,https://doi.org/10.1007/s42486-021-00064-4,"Visual aesthetics is vital in determining the usability of the graphical user interface (GUI). It can strengthen the competitiveness of interactive online applications. Human aesthetic preferences for GUI are implicit and linked to various aspects of perception. In this study, an aesthetic GUI image database was constructed with 38,423 design works collected from Huaban.com, a popular social network website for art and design sharing, collection, and exhibition in China. The numbers of user collection and likes of each design work were used as the annotation to represent user preference levels. Deep convolutional neural networks were applied to evaluate the aesthetic preferences of GUIs, based on a large dataset of user interface design images with the ground-truth annotations. The experimental result indicated the feasibility of the proposed method, with a mean squared error (MSE) of 0.0222 for user collection prediction and an MSE of 0.0644 for user likes prediction in the best model performance of Squeeze-and-Excitation-VGG19 networks (SE-VGG19). This study aims to build a large aesthetic image database, and to explore a practical and objective evaluation model of GUI aesthetics.",Computational aesthetics; GUI aesthetic assessment; DCNNs,6,Q3,UX/usability attributes evaluation,Visual aesthetics,Computer vision,deep learning,,Screenshots/images,no,aesthetics evaluation; computer vision; UI screenshot; model training; aesthetics prediction from GUI images; esthetic GUI image database was constructed; any
xu2015,TurkerGaze: Crowdsourcing Saliency with Webcam based Eye Tracking,"Xu, Pingmei; Ehinger, Krista A.; Zhang, Yinda; Finkelstein, Adam; Kulkarni, Sanjeev R.; Xiao, Jianxiong",2015,preprint,,,,,,arXiv,10.48550/arXiv.1504.06755,http://arxiv.org/abs/1504.06755,"Traditional eye tracking requires specialized hardware, which means collecting gaze data from many observers is expensive, tedious and slow. Therefore, existing saliency prediction datasets are order-of-magnitudes smaller than typical datasets for other vision recognition tasks. The small size of these datasets limits the potential for training data intensive algorithms, and causes overfitting in benchmark evaluation. To address this deficiency, this paper introduces a webcam-based gaze tracking system that supports large-scale, crowdsourced eye tracking deployed on Amazon Mechanical Turk (AMTurk). By a combination of careful algorithm and gaming protocol design, our system obtains eye tracking data for saliency prediction comparable to data gathered in a traditional lab setting, with relatively lower cost and less effort on the part of the researchers. Using this tool, we build a saliency dataset for a large number of natural images. We will open-source our tool and provide a web server where researchers can upload their images to get eye tracking results from AMTurk.",,383,-,Saliency/visual importance,,Computer vision,deep learning,,Video/facial expressions,yes,"webcam eyetracking (gaze prediction); computer vision aproach; eye images; model training; estimate gaze patterns; webcam-based gaze tracking system that supports large-scale, crowdsourced eye tracking; not mentioned"
xu2016,Spatio-Temporal Modeling and Prediction of Visual Attention in Graphical User Interfaces,"Xu, Pingmei; Sugano, Yusuke; Bulling, Andreas",2016,conferencePaper,,,3299–3310,Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems,CHI,ACM,10.1145/2858036.2858479,https://doi.org/10.1145/2858036.2858479,"We present a computational model to predict users' spatio-temporal visual attention on WIMP-style (windows, icons, menus, pointer) graphical user interfaces. Like existing models of bottom-up visual attention in computer vision, our model does not require any eye tracking equipment. Instead, it predicts attention solely using information available to the interface, specifically users' mouse and keyboard input as well as the UI components they interact with. To study our model in a principled way, we further introduce a method to synthesize user interface layouts that are functionally equivalent to real-world interfaces, such as from Gmail, Facebook, or GitHub. We first quantitatively analyze attention allocation and its correlation with user input and UI components using ground-truth gaze, mouse, and keyboard data of 18 participants performing a text editing task. We then show that our model predicts attention maps more accurately than state-of-the-art methods. Our results underline the significant potential of spatio-temporal attention modeling for user interface evaluation, optimization, or even simulation.",Visual attention; saliency; interactive environment; graphical user interfaces; physical action; spatio-temporal modeling,54,A,Saliency/visual importance,,"User interaction analysis, Eyetracking",machine learning,,"User interactions, gaze data",yes,"visual attention prediction; user interaction analysis; UI components, user interactions; usability test; nan; computational model to predict users' spatio-temporal visual attention on WIMP-style (windows, icons, menus, pointer) graphical user interfaces - data-driven approach to analyze and predict visual attention in the GUI space; desktop"
yang2020,Measuring and Improving User Experience Through Artificial Intelligence-Aided Design,"Yang, Bin; Wei, Long; Pu, Zihan",2020,journalArticle,11,,,Frontiers in Psychology,,Frontiers Media SA,10.3389/fpsyg.2020.595374,https://doi.org/10.3389/fpsyg.2020.595374,"This paper aims to propose a methodology for measuring user experience (UX) by using artificial intelligence-aided design (AIAD) technology in mobile application design. Unlike the traditional assistance design tools, AIAD focuses on the rational use of artificial intelligence (AI) technology to measure and improve UX since conventional data collection methods (such as user interview and user observation) for user behavior data are inefficient and time-consuming. We propose to obtain user behavior data from logs of mobile application. In order to protect the privacy of users, only a few dimensions of information is used in the process of browsing and operating mobile application. The goal of the proposed methodology is to make the deep neural network model simulate the user’s experience in the process of operating a mobile application as much as possible. We design and use projected pages of application to train neural networks for specific tasks. These projected pages consist of the click information of all users in the process of completing a certain task. Thus, features of user behavior can be aggregated and mapped in the connection layers and the hidden layers. Finally, the optimized design is executed on the social communication application to verify the efficiency of the proposed methodology.",user experience; artificial intelligence aided design; human computer interaction; mobile application design; deep neural network; usability evaluation,41,Q2,UX/usability attributes evaluation,"usability metrics (efficiency,...)",User interaction analysis,deep learning,,User interactions,yes,automated UX evaluation; user interaction data analysis; user interactions (clicks); model training; UX score from user clicks; methodology for measuring user experience (UX) by using artificial intelligence-aided design (AIAD) technology in mobile application design; mobile
yang2021,Don’t Do That! Hunting Down Visual Design Smells in Complex UIs Against Design Guidelines,"Yang, Bo; Xing, Zhenchang; Xia, Xin; Chen, Chunyang; Ye, Deheng; Li, Shanping",2021,conferencePaper,,,761-772,2021 IEEE/ACM 43rd International Conference on Software Engineering (ICSE),ICSE,IEEE,10.1109/ICSE43902.2021.00075,https://doi.org/10.1109/ICSE43902.2021.00075,"Just like code smells in source code, UI design has visual design smells. We study 93 don't-do-that guidelines in the Material Design, a complex design system created by Google. We find that these don't-guidelines go far beyond UI aesthetics, and involve seven general design dimensions (layout, typography, iconography, navigation, communication, color, and shape) and four component design aspects (anatomy, placement, behavior, and usage). Violating these guidelines results in visual design smells in UIs (or UI design smells). In a study of 60,756 UIs of 9,286 Android apps, we find that 7,497 UIs of 2,587 apps have at least one violation of some Material Design guidelines. This reveals the lack of developer training and tool support to avoid UI design smells. To fill this gap, we design an automated UI design smell detector (UIS-Hunter) that extracts and validates multi-modal UI information (component metadata, typography, iconography, color, and edge) for detecting the violation of diverse don't-guidelines in Material Design. The detection accuracy of UIS-Hunter is high (precision=0.81, recall=0.90) on the 60,756 UIs of 9,286 apps. We build a guideline gallery with real-world UI design smells that UIS-Hunter detects for developers to learn the best Material Design practices. Our user studies show that UIS-Hunter is more effective than manual detection of UI design smells, and the UI design smells that are detected by UIS-Hunter have severely negative impacts on app users.",GUI testing; UI design smell; Violation detection; Material design,50,A,UI issue detection,,Computer vision,deep learning,,Screenshots/images,no,"GUI design violation issue detection; computer vision; UI screenshot; GUI testing; nan; automated UI design smell detector (UIS-Hunter) that extracts and validates multi-modal UI information (component metadata, typography, iconography, color, and edge) for detecting the violation of diverse don't-guidelines in Material Design; mobile"
zhao2020,Seenomaly: vision-based linting of GUI animation effects against design-don't guidelines,"Zhao, Dehai; Xing, Zhenchang; Chen, Chunyang; Xu, Xiwei; Zhu, Liming; Li, Guoqiang; Wang, Jinshui",2020,conferencePaper,,,1286–1297,Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering,ICSE,ACM,10.1145/3377811.3380411,https://doi.org/10.1145/3377811.3380411,"GUI animations, such as card movement, menu slide in/out, snackbar display, provide appealing user experience and enhance the usability of mobile applications. These GUI animations should not violate the platform's UI design guidelines (referred to as design-don't guideline in this work) regarding component motion and interaction, content appearing and disappearing, and elevation and shadow changes. However, none of existing static code analysis, functional GUI testing and GUI image comparison techniques can ""see"" the GUI animations on the scree, and thus they cannot support the linting of GUI animations against design-don't guidelines. In this work, we formulate this GUI animation linting problem as a multi-class screencast classification task, but we do not have sufficient labeled GUI animations to train the classifier. Instead, we propose an unsupervised, computer-vision based adversarial autoencoder to solve this linting problem. Our autoencoder learns to group similar GUI animations by ""seeing"" lots of unlabeled real-application GUI animations and learning to generate them. As the first work of its kind, we build the datasets of synthetic and real-world GUI animations. Through experiments on these datasets, we systematically investigate the learning capability of our model and its effectiveness and practicality for linting GUI animations, and identify the challenges in this linting problem for future work.",GUI animation; Design guidelines; Lint; Unsupervised learning,60,A,UI issue detection,,Computer vision,"deep learning, machine learning",,Screenshots/images,no,"GUI design violation issue detection; computer vision; UI screenshot; GUI testing; feature extraction from screenshots, then classification with Knn; unsupervised, computer-vision based adversarial autoencoder to solve this linting problem - GUI animations should not violate the platform’s UI design guidelines; mobile"
zhou2020,Intelligent Exploration for User Interface Modules of Mobile App with Collective Learning,"Zhou, Jingbo; Tang, Zhenwei; Zhao, Min; Ge, Xiang; Zhuang, Fuzhen; Zhou, Meng; Zou, Liming; Yang, Chenglei; Xiong, Hui",2020,conferencePaper,,,3346–3355,Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery; Data Mining,KDD,ACM,10.1145/3394486.3403387,https://doi.org/10.1145/3394486.3403387,"A mobile app interface usually consists of a set of user interface modules. How to properly design these user interface modules is vital to achieving user satisfaction for a mobile app. However, there are few methods to determine design variables for user interface modules except for relying on the judgment of designers. Usually, a laborious post-processing step is necessary to verify the key change of each design variable. Therefore, there is only a very limited amount of design solutions that can be tested. It is time-consuming and almost impossible to figure out the best design solutions as there are many modules. To this end, we introduce FEELER, a framework to fast and intelligently explore design solutions of user interface modules with a collective machine learning approach. FEELER can help designers quantitatively measure the preference score of different design solutions, aiming to facilitate the designers to conveniently and quickly adjust user interface module. We conducted extensive experimental evaluations on two real-life datasets to demonstrate its applicability in real-life cases of user interface module design in the Baidu App, which is one of the most popular mobile apps in China.",user interface design; collective learning; gaussian process; preference learning; user interface exploration,10,A,UX/usability attributes evaluation,design preference,Source code analysis (web/app crawling),machine learning,,Source code,no,"design preference evaluation; UI element analysis; UI element info; model training; selecting the best design (preference scroe) from UI element information - labeling by users for training; FEELER can help designers quantitatively measure the preference score of different design solutions, aiming to facilitate the designers to conveniently and quickly adjust user interface module; mobile"
bakaev2018,Auto-Extraction and Integration of Metrics for Web User Interfaces,"Bakaev, Maxim; Heil, Sebastian; Khvorostov, Vladimir; Gaedke, Martin",2018,journalArticle,17,6–7,561-590,Journal of Web Engineering,,River Publishers,10.13052/jwe1540-9589.17676,https://ieeexplore.ieee.org/abstract/document/10251855,"Metric-based assessment of web user interface (WUI) quality attributes is shifting from code (HTML/CSS) analysis to mining webpages' visual representations based on image recognition techniques. In our paper, we describe a visual analysis tool which takes a WUI screenshot and produces structured and machine-readable representation (JSON) of the interface elements' spatial allocation. The implementation is based on OpenCV (image recognition functions), dlib (trained detector for the elements' classification), and Tesseract (label and content text recognition). The JSON representation is used to automatically calculate several metrics related to visual complexity, which is known to have major effect on user experience with UIs. We further describe a WUI measurement platform that allows integration of the currently dispersed sets of metrics from different providers and demonstrate the platform's use with several remote services. We perform statistical analysis of the collected metrics in relation to complexity-related subjective evaluations obtained from 63 human subjects of various nationalities. Finally, we build predictive models for visual complexity and show that their accuracy can be improved by integrating the metrics from different sets. Regressions with the single index of visual complexity metric that we proposed had R2=0.460, while the best joint model with 4 metrics had R2=0.647.",Automated metrics; HCI Vision; web design mining; visual complexity,38,,UX/usability attributes evaluation,Visual complexity,Screenshot metrics,deep learning,,Screenshots/images,no,visual complextiy; image metric analysis; UI screenshot; user evaluations; nan; visual analysis tool which takes a WUI screenshot and produces structured and machine-readable representation; desktop
bakaev2018a,HCI Vision for Automated Analysis and Mining of Web User Interfaces,"Bakaev, Maxim; Heil, Sebastian; Khvorostov, Vladimir; Gaedke, Martin",2018,conferencePaper,,,136-144,Web Engineering,ICWE,Springer Nature,10.1007/978-3-319-91662-0_10,https://doi.org/10.1007/978-3-319-91662-0_10,"Most techniques for webpage structure and design mining are based on code analysis and are detached from a human user’s perception of the web user interface (WUI). Our paper is dedicated to approaches that instead focus on analysis of webpage’s visual representation – the way it is rendered in different browsers and environments and delivered to the end user. Specifically, we describe the software tool that we built, which takes a WUI screenshot and produces structured and machine-readable representation (JSON) of interface elements as made out by a human user. The implementation is based on OpenCV (image recognition functions), dlib (trained detector for the elements’ classification), and Tesseract (label and content text recognition). To demonstrate feasibility of the approach, we describe application of our analyzer tool to auto-calculate certain measures for a WUI and to predict users’ subjective impressions. Particularly, we assess UI visual complexity, which is known to significantly influence both cognitive and affective aspects of interaction. The results suggest the analyzer’s output is mostly characteristic of the users’ visual perception and can be useful for auto-assessing and comparing WUIs.",Web design mining; HCI vision; Image recognition; Human factors; Visual complexity,28,B,UX/usability attributes evaluation,Visual complexity,Screenshot metrics,deep learning,,Screenshots/images,no,"visual complextiy; UI screenshot analysis from web scraping; UI screenshot; web scraping; nan; software tool that we built, which takes a WUI screenshot and produces structured and machine-readable representation (JSON) of interface elements as made out by a human user; desktop"
boychuk2019,Entropy and Compression Based Analysis of Web User Interfaces,"Boychuk, Egor; Bakaev, Maxim",2019,conferencePaper,,,253-261,Web Engineering,ICWE,Springer Nature,10.1007/978-3-030-19274-7_19,https://doi.org/10.1007/978-3-030-19274-7_19,"In our paper we explore whether user visual perception of web interfaces (WUI) can be predicted by certain quantitative characteristics of WUI screenshots. The considered metrics are JPEG file size, PNG file size, and information entropy value calculated with frequency-based MATLAB’s entropy(I) function. We ran survey with 70 subjects who provided subjective evaluations of complexity, aesthetics and orderliness for 497 website homepages. The results suggest that all the three metrics were significant, and the proposed regression models were considerably better than the respective baseline models that only used the popular JPEG-based metric. Remarkably, the entropy metric had significant positive correlations with aesthetic and orderliness evaluations, but not with the size of the image. We believe our findings might be used in development of automated WUI analysis tools to aid web engineers in their work.",Information entropy; Web interfaces; Cognitive models,15,B,UX/usability attributes evaluation,Visual complexity,Screenshot metrics,none,descriptive metrics/visualizations,Screenshots/images,no,aesthetics and complexity; UI screenshot analysis from web scraping; UI screenshot; web scraping; nan; whether user visual perception of web interfaces (WUI) can be predicted by certain quantitative characteristics of WUI screenshots; desktop
li2023,Spotlight: Mobile UI Understanding using Vision-Language Models with a Focus,"Li, Gang; Li, Yang",2023,preprint,,,,,,arXiv,10.48550/arXiv.2209.14927,http://arxiv.org/abs/2209.14927,"Mobile UI understanding is important for enabling various interaction tasks such as UI automation and accessibility. Previous mobile UI modeling often depends on the view hierarchy information of a screen, which directly provides the structural data of the UI, with the hope to bypass challenging tasks of visual modeling from screen pixels. However, view hierarchies are not always available, and are often corrupted with missing object descriptions or misaligned structure information. As a result, despite the use of view hierarchies could offer short-term gains, it may ultimately hinder the applicability and performance of the model. In this paper, we propose Spotlight, a vision-only approach for mobile UI understanding. Specifically, we enhance a vision-language model that only takes the screenshot of the UI and a region of interest on the screen -- the focus -- as the input. This general architecture of Spotlight is easily scalable and capable of performing a range of UI modeling tasks. Our experiments show that our model establishes SoTA results on several representative UI tasks and outperforms previous methods that use both screenshots and view hierarchies as inputs. Furthermore, we explore multi-task learning and few-shot prompting capacities of the proposed models, demonstrating promising results in the multi-task learning direction.",,29,-,UI understanding/feedback,,Computer vision,New LMs,VLM,Screenshots/images,no,"tappability prediction and screen annotation (summarization); VLM design annotation (cv); UI screenshot, source code; model training; annotating elements and tappability points from source code and with text from screenshots; approach for mobile UI understanding - vision-language model that only takes the screenshot of the UI and a region of interest on the screen -- the focus -- as the input; mobile"
miniukovich2018,Visual complexity of graphical user interfaces,"Miniukovich, Aliaksei; Sulpizio, Simone; De Angeli, Antonella",2018,conferencePaper,,,1–9,Proceedings of the 2018 International Conference on Advanced Visual Interfaces,AVI,ACM,10.1145/3206505.3206549,https://doi.org/10.1145/3206505.3206549,"Graphical User Interfaces (GUIs) of low visual complexity tend to have higher aesthetics, usability and accessibility, and result in higher user satisfaction. Despite a few authors recently used or studied visual complexity, the concept of visual complexity still needs to be better defined for the use in HCI research and GUI design, with its underlying aspects systematized and operationalized, and different measures validated. This paper reviews the aspects of GUI visual complexity and operationalizes four aspects with nine computation-based measures in total. Two user studies validated the measures on two types of stimuli - webpages (study 1, n = 55) and book pages (study 2, n = 150) - with two user groups, dyslexics (people with reading difficulties) and typical readers. The same complexity aspects could be expected to determine complexity perception for both GUI types, whereas different complexity aspects could be expected to determine complexity perception for dyslexics, relative to typical readers. However, the studies showed little to no difference between dyslexics and average readers, whereas web pages did differ from book pages in what aspects made them seem complex. It was not the intergroup differences, but the stimulus type that defined criteria to judge visual complexity. Future research and visual design could rely on the visual complexity aspects outlined in this paper.",Facets of Visual Complexity; Computation of Visual Complexity; Dyslexia; eBooks; Webpages; GUI Research and Design,31,B,UX/usability attributes evaluation,Visual complexity,Screenshot metrics,none,descriptive metrics/visualizations,Screenshots/images,no,visual complexity; UI screenshot metrics analysis; UI screenshots; usability test; nan; aspects of GUI visual complexity and operationalizes four aspects with nine computation-based measures in tot; desktop
oulasvirta2018,Aalto Interface Metrics (AIM): A Service and Codebase for Computational GUI Evaluation,"Oulasvirta, Antti; De Pascale, Samuli; Koch, Janin; Langerak, Thomas; Jokinen, Jussi; Todi, Kashyap; Laine, Markku; Kristhombuge, Manoj; Zhu, Yuxi; Miniukovich, Aliaksei; Palmas, Gregorio; Weinkauf, Tino",2018,conferencePaper,,,16–19,Adjunct Proceedings of the 31st Annual ACM Symposium on User Interface Software and Technology,UIST,ACM,10.1145/3266037.3266087,https://doi.org/10.1145/3266037.3266087,"Aalto Interface Metrics (AIM) pools several empirically validated models and metrics of user perception and attention into an easy-to-use online service for the evaluation of graphical user interface (GUI) designs. Users input a GUI design via URL, and select from a list of 17 different metrics covering aspects ranging from visual clutter to visual learnability. AIM presents detailed breakdowns, visualizations, and statistical comparisons, enabling designers and practitioners to detect shortcomings and possible improvements. The web service and code repository are available at interfacemetrics.aalto.fi.",,58,A,UX/usability attributes evaluation,Visual complexity,Screenshot metrics,none,descriptive metrics/visualizations,Screenshots/images,no,"visual complexity; UI screenshot metrics analysis, web scraping; UI screenshots; none; nan; everal empirically validated models and metrics of user perception and attention into an easy-to-use online service for the evaluation of graphical user interface (GUI) designs; desktop"
soui2020,Assessing the quality of mobile graphical user interfaces using multi-objective optimization,"Soui, Makram; Chouchane, Mabrouka; Mkaouer, Mohamed Wiem; Kessentini, Marouane; Ghedira, Khaled",2020,journalArticle,24,10,7685-7714,Soft Computing,,Springer Nature,10.1007/s00500-019-04391-8,https://doi.org/10.1007/s00500-019-04391-8,"Aesthetic defects are a violation of quality attributes that are symptoms of bad interface design programming decisions. They lead to deteriorating the perceived usability of mobile user interfaces and negatively impact the User’s eXperience (UX) with the mobile app. Most existing studies relied on a subjective evaluation of aesthetic defects depending on end-users feedback, which makes the manual evaluation of mobile user interfaces human-centric, time-consuming, and error-prone. Therefore, recent studies have dedicated their effort to focus on the definition of mathematical formulas that each targets a specific structural quality of the interface. As the UX is tightly dependent on the user profile, the combination and calibration of quality attributes, formulas, and user’s characteristics, when defining a defect, are not straightforward. In this context, we propose a fully automated framework which combines literature quality attributes with the user’s profile to identify aesthetic defects of MUI. More precisely, we consider the mobile user interface evaluation as a multi-objective optimization problem where the goal is to maximize the number of detected violations while minimizing the detection complexity of detection rules and enhancing the interfaces overall quality in means of guidance and coherence coverage. We conducted a comparative study of several evolutionary algorithms in terms of accurately identifying aesthetic defects. We reported their performance in solving the proposed search-based multi-objective optimization problem. The results confirm the efficiency of the indicator-based evolutionary algorithm in terms of assessing the developers in detecting typical defects and also in generating the most accurate detection rules.",Mobile user interface; Aesthetic quality; Evaluation,39,Q2,UI issue detection,,Source code analysis (web/app crawling),none,metrics + rule-based,Source code,no,Aesthetic defects detection; source code metrics analysis; GUI source code; existing tool comparision; nan; fully automated framework which combines literature quality attributes with the user’s profile to identify aesthetic defects of MUI; mobile
hong2024,CogAgent: A Visual Language Model for GUI Agents,"Hong, Wenyi; Wang, Weihan; Lv, Qingsong; Xu, Jiazheng; Yu, Wenmeng; Ji, Junhui; Wang, Yan; Wang, Zihan; Dong, Yuxiao; Ding, Ming; Tang, Jie",2024,conferencePaper,,,,Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,CVPR,IEEE,,https://arxiv.org/abs/2312.08914,"People are spending an enormous amount of time on digital devices through graphical user interfaces (GUIs), e.g., computer or smartphone screens. Large language models (LLMs) such as ChatGPT can assist people in tasks like writing emails, but struggle to understand and interact with GUIs, thus limiting their potential to increase automation levels. In this paper, we introduce CogAgent, an 18-billion-parameter visual language model (VLM) specializing in GUI understanding and navigation. By utilizing both low-resolution and high-resolution image encoders, CogAgent supports input at a resolution of 1120*1120, enabling it to recognize tiny page elements and text. As a generalist visual language model, CogAgent achieves the state of the art on five text-rich and four general VQA benchmarks, including VQAv2, OK-VQA, Text-VQA, ST-VQA, ChartQA, infoVQA, DocVQA, MM-Vet, and POPE. CogAgent, using only screenshots as input, outperforms LLM-based methods that consume extracted HTML text on both PC and Android GUI navigation tasks -- Mind2Web and AITW, advancing the state of the art.",,77,A,UI understanding/feedback,,Conversational assistant,New LMs,VLM,Screenshots/images,no,"GUI understanding; computer vision; UI screenshot; model training; annotating screenshots - possible actions; CogAgent an 18-billion-parameter visual language model (VLM) specializing in GUI understanding and navigation; desktop, mobile"
shojaeizadeh2019,Detecting task demand via an eye tracking machine learning system,"Shojaeizadeh, Mina; Djamasbi, Soussan; Paffenroth, Randy C.; Trapp, Andrew C.",2019,journalArticle,116,,,Decision Support Systems,,Elsevier,10.1016/j.dss.2018.10.012,https://doi.org/10.1016/j.dss.2018.10.012,"Computerized systems play a significant role in today's fast-paced digital economy. Because task demand is a major factor that influences how computerized systems are used to make decisions, identifying task demand automatically provides an opportunity for designing advanced decision support systems that can respond to user needs at a personalized level. A first step for designing such advanced decision tools is to investigate possibilities for developing automatic task load detectors. Grounded in decision making, eye tracking, and machine learning literature, we argue that task demand can be detected automatically, reliably, and unobtrusively using eye movements only. To investigate this possibility, we developed an eye tracking task load detection system and tested its effectiveness. Our results revealed that our task load detection system reliably predicted increased task demand from users' eye movement data. These results and their implications for research and practice are discussed.",Eye tracking; Machine learning; Human computer interaction; Adaptive decision making; Cognitive effort; Task demand,84,Q1,UX/usability attributes evaluation,difficulty/demand,Eyetracking,machine learning,,gaze data,yes,"task demand detection; eye-tracking; eye-tracking data; model training, usability test; task load perdiction from eye-tracking data; eye tracking task load detection system; desktop"
gu2021,Predicting webpage aesthetics with heatmap entropy,"Gu, Zhenyu; Jin, Chenhao; Chang, Danny; Zhang, Liqun",2021,journalArticle,40,7,,Behaviour & Information Technology,,Taylor & Francis,10.1080/0144929X.2020.1717626,https://doi.org/10.1080/0144929X.2020.1717626,"This paper introduces a descriptive global index for eye-tracking data called heatmap entropy, or visual attention entropy (VAE), and discerns its predictive value for webpage aesthetics. VAE represents the chaos, or uncertainty, in the allocation of visual attention. In the experiment, we tracked and recorded 30 observers' initial landings on 40 web pages displayed for 3 seconds each. The results show that the VAE and aesthetic ratings of the web pages are negatively correlated (r=−0.54, P<0.001). A calibrated form of VAE, known as relative VAE (rVAE), has a more significant correlation with the aesthetic ratings (r=−0.65, P<0.00001). On its own, the rVAE can differentiate between good- and bad-looking pages to a certain degree of accuracy (two-class ANOVA with F=26.84, P<0.00001). Further investigation reveals that the performances of both VAE and rVAE improve steadily after the first second, and could be better, if the tracking duration was longer than 3 seconds or if more observers were recruited.",eye tracking; visual attention; Entropy; aesthetics; web page,30,Q2,UX/usability attributes evaluation,Visual aesthetics,Eyetracking,none,descriptive metrics/visualizations,gaze data,yes,"web aesthetics prediction; eye-tracking data analysis; UI screenshots; model training, eye-tracking experiment; nan; descriptive global index for eye-tracking data called heatmap entropy, or visual attention entropy (VAE), and discerns its predictive value for webpage aesthetics; desktop"
katerina2018,Mouse behavioral patterns and keystroke dynamics in End-User Development: What can they tell us about users’ behavioral attributes?,"Katerina, Tzafilkou; Nicolaos, Protogeros",2018,journalArticle,83,,,Computers in Human Behavior,,Elsevier,10.1016/j.chb.2018.02.012,https://doi.org/10.1016/j.chb.2018.02.012,"Studying human behavior is of particular interest within the field of Human-Computer Interaction (HCI) as it can provide insight into human performance. Prior HCI research suggests that mouse and keyboard monitoring may provide a more complete picture of user behavior under high cognitive loads like decision making and developing tasks. In this exploratory study we investigate the potential correlation between mouse behavioral patterns or keystroke dynamics and a set of End-User Development (EUD) behavioral attributes. We conduct a field test on 30 end-users interacting with a modern web-based EUD tool for the construction of simple web forms. Our findings reveal the existence of several significant correlations between end-users’ behavioral attributes and mouse pattern metrics or keystroke dynamics during the development process. Mouse pattern metrics like random and straight movements, mouse hovers, etc., can be associated with perceived ease use, perceived usefulness, self-efficacy, willingness to learn or risk-perception. Similarly, some keystroke dynamics like key press speed and down-to-down time can be associated with perceived ease of use or self-efficacy. The findings of this work show a new interesting research direction and may motivate the EUD research community to study further the end-users’ mouse and keyboard behavior in today's web-based EUD systems.",End-user behavior; End-User Development (EUD); Keystroke dynamics; Mouse behavioral patterns; Mouse tracking,58,Q1,UX/usability attributes evaluation,"usability metrics (efficiency,...)",User interaction analysis,none,descriptive metrics/visualizations,User interactions,yes,UX attributes evaluation; user interactions data analysis; user interactions (key and mouse); usability test; nan; potential correlation between mouse behavioral patterns or keystroke dynamics and a set of End-User Development (EUD) behavioral attributes; desktop
arapakis2016,Predicting User Engagement with Direct Displays Using Mouse Cursor Information,"Arapakis, Ioannis; Leiva, Luis A.",2016,conferencePaper,,,,Proceedings of the 39th International ACM SIGIR conference on Research and Development in Information Retrieval,SIGIR,ACM,10.1145/2911451.2911505,https://doi.org/10.1145/2911451.2911505,"Predicting user engagement with direct displays (DD) is of paramount importance to commercial search engines, as well as to search performance evaluation. However, understanding within-content engagement on a web page is not a trivial task mainly because of two reasons: (1) engagement is subjective and different users may exhibit different behavioural patterns; (2) existing proxies of user engagement (e.g., clicks, dwell time) suffer from certain caveats, such as the well-known position bias, and are not as effective in discriminating between useful and non-useful components. In this paper, we conduct a crowdsourcing study and examine how users engage with a prominent web search engine component such as the knowledge module (KM) display. To this end, we collect and analyse more than 115k mouse cursor positions from 300 users, who perform a series of search tasks. Furthermore, we engineer a large number of meta-features which we use to predict different proxies of user engagement, including attention and usefulness. In our experiments, we demonstrate that our approach is able to predict more accurately different levels of user engagement and outperform existing baselines.",web search; knowledge module; direct displays; mouse cursor tracking; user engagement,50,A,UX/usability attributes evaluation,user engagement,User interaction analysis,machine learning,,User interactions,yes,"Predicting User Engagement; user interactions data analysis; user interactions (mouse); model training, usability test; engagement prediction from mouse data; predict different proxies of user engagement, including attention and usefulness from mouse cursor positions; desktop"
yamauchi2018,Reading Emotion From Mouse Cursor Motions: Affective Computing Approach,"Yamauchi, Takashi; Xiao, Kunchen",2018,journalArticle,42,3,,Cognitive Science,,Wiley,10.1111/cogs.12557,https://onlinelibrary.wiley.com/doi/abs/10.1111/cogs.12557,"Affective computing research has advanced emotion recognition systems using facial expressions, voices, gaits, and physiological signals, yet these methods are often impractical. This study integrates mouse cursor motion analysis into affective computing and investigates the idea that movements of the computer cursor can provide information about emotion of the computer user. We extracted 16–26 trajectory features during a choice-reaching task and examined the link between emotion and cursor motions. Participants were induced for positive or negative emotions by music, film clips, or emotional pictures, and they indicated their emotions with questionnaires. Our 10-fold cross-validation analysis shows that statistical models formed from “known” participants (training data) could predict nearly 10%–20% of the variance of positive affect and attentiveness ratings of “unknown” participants, suggesting that cursor movement patterns such as the area under curve and direction change help infer emotions of computer users.",Affective computing; Choice reaching trajectory; Emotion and motor control; Mouse cursor motion analysis,73,Q2,Emotion detection,,User interaction analysis,machine learning,,User interactions,yes,emotion detection; user interactions data analysis; user interactions (mouse); user mouse moving tasks; predicting anxiety from mouse data; study integrates mouse cursor motion analysis into affective computing and investigates the idea that movements of the computer cursor can provide information about emotion of the computer user; desktop
frey2016,Framework for Electroencephalography-based Evaluation of User Experience,"Frey, Jérémy; Daniel, Maxime; Castet, Julien; Hachet, Martin; Lotte, Fabien",2016,conferencePaper,,,,Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems,CHI,ACM,10.1145/2858036.2858525,https://doi.org/10.1145/2858036.2858525,"Measuring brain activity with electroencephalography (EEG) is mature enough to assess mental states. Combined with existing methods, such tool can be used to strengthen the understanding of user experience. We contribute a set of methods to estimate continuously the user's mental workload, attention and recognition of interaction errors during different interaction tasks. We validate these measures on a controlled virtual environment and show how they can be used to compare different interaction techniques or devices, by comparing here a keyboard and a touch-based interface. Thanks to such a framework, EEG becomes a promising method to improve the overall usability of complex computer systems.",EEG; HCI Evaluation; Workload; Attention; Interaction errors; Neuroergonomy,85,A,UX/usability attributes evaluation,difficulty/demand,"User interaction analysis, Physiological signal analysis",machine learning,,"User interactions, physiological signals",yes,"UX evaluation (some attributes); user interactions analysis, physio data analysis; EEG data, user interactions; game test; nan; framework, EEG becomes a promising method to improve the overall usability of complex computer systems; desktop"
liu2024,Conversation-based hybrid UI for the repertory grid technique: A lab experiment into automation of qualitative surveys,"Liu, Yunxing; Martens, Jean-Bernard ",2024,journalArticle,184,,103227,International Journal of Human-Computer Studies,,Elsevier,10.1016/j.ijhcs.2024.103227,https://doi.org/10.1016/j.ijhcs.2024.103227,"A frequent use of conversational user interfaces (CUIs) today is improving the users’ experience with online quantitative surveys. In this paper, we explore the use of CUIs in qualitative surveys. As a concrete use case, we adopt a specific, well-structured, qualitative research method called the repertory grid technique (RGT). We developed a hybrid user interface (HUI) that combines a graphical user interface (GUI) with a CUI to automate the distinct stages in a RGT survey. A pilot study was used to verify the feasibility of the approach and to fine-tune interface aspects of an initial prototype. In this paper, we report the results of a within-subject lab experiment with 24 participants that aimed to establish the performance and UX in a realistic context of a more advanced prototype. We observed a small decrease in UX in some hedonistic aspects, but also confirmed that the HUI performs similarly to a human agent in most pragmatic aspects. These results provide support for our hypothesis that automating qualitative surveys is possible with proper interface design. We hope that our work can inspire other researchers to design additional tools for qualitative survey automation, especially now that generative AI systems, such as ChatGPT, open up interesting new ways for computer systems to interact with users in natural language.",Chatbot; Qualitative survey automation; Hybrid UI; Repertory grid technique,3,Q1,UX/usability attributes evaluation,"usability metrics (efficiency,...)",Conversational assistant,New LMs,NLU,"Screenshots/images, User feedback/reviews/text, User interactions",yes,"UX evaluation (some attributes); LLM conversation, user interactions analysis; questionnaire data, user interactions, LLM feedback; usability test; chatbot interview; use of CUIs in qualitative surveys - survey parts automation; desktop"